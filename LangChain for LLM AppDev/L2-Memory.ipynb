{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8daf996d-13e5-4146-a353-ece6a16001c4",
   "metadata": {},
   "source": [
    "# LangChain: Memory\n",
    "When we interact with LLM, usualy they dont remember the past history and its problem with chatbots building. So we will build, how can we do conversation utilizing memory.\n",
    "\n",
    "## Outline\n",
    "* ConversationBufferMemory\n",
    "    - This memory allows for storing of messages and then extract the messages in a variable\n",
    "* ConversationBufferWindowMemory\n",
    "    - This memory keeps a list of the interations of the conversation over time. it onlybuses the last $k$ intearations\n",
    "    \n",
    "* ConversationTokenBufferMemory\n",
    "    - This memory keeps a buffer of recent interations in memory and usues tokens length rather than number of interations to determing when to flush interations\n",
    "* ConversationSummaryMemory\n",
    "    - This memory creates a suummary of the covnersation over time.\n",
    "\n",
    "## Additional memory Types\n",
    "* Vector data memory\n",
    "    - Stores text (from conversations or elsewhere) in a vector database and retrieves the most relevant blocks of text\n",
    "* Entity memory\n",
    "    - Using an LLM, it rememebers details about specific entities (people)\n",
    " \n",
    "You can also use multiple memories at a time. E.g. Conversation memory + Entity memory to recall endividuals\n",
    "\n",
    "You can also store the conversation in a conventional database (such as key -value store or SQL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082d3364-5699-44b6-8931-c7e327d260da",
   "metadata": {},
   "source": [
    "## ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98efc0ff-c9e5-4e42-b3f4-49ee631d9299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# import openAI key\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16a9173-cd3d-4108-ba41-92b63fc0b468",
   "metadata": {},
   "source": [
    "Note: LLM's do not always produce the same results. When executing the code in your notebook, you may get slightly different answers that those in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ce2eeb-a3ac-4f07-9dd6-92af1d7028a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# account for deprecation of LLM model\n",
    "import datetime\n",
    "# Get the current date\n",
    "current_date = datetime.datetime.now().date()\n",
    "\n",
    "# Define the date after which the model should be set to \"gpt-3.5-turbo\"\n",
    "target_date = datetime.date(2024, 6, 12)\n",
    "\n",
    "# Set the model variable based on the current date\n",
    "if current_date > target_date:\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005bab5b-45e3-4c78-967e-89834a1e46fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some other tools from langchain that we will need\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bfd6f4-6f44-4dfe-bad0-730c030e3341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A good example of using memory is to use langchain to manage a chat or\n",
    "# a chatbot conversation.\n",
    "\n",
    "# lets setup an LLM as a chat interface with temperature 0\n",
    "llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "# we are using memory as conversation buffer memory\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# build a conversation chain. We will see later what is chain.\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True # you can choose `false` to not show waht Lanchain is doing \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2873e8d7-fc2b-457e-b264-d327da54921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start conversation\n",
    "conversation.predict(input=\"Hi, my name is Abhishek\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ee1523-b17e-4b81-9ab3-a8e817305c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask another conversation\n",
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46309696-d660-4f07-9bce-d48b35163c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now cehck if llm can remember my name. When you set verbose=True you will \n",
    "# see that llm is storing the conversation. History of conversation \n",
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e8e9e3-a0e2-47f8-98ab-7345da1320b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# at the start we defined memory variable to store the hoistory\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94e9426-ad98-4899-9552-3477498b4e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {} is empty dioctionary. we can use other features as input. this is what langchain is remebered. \n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119dac3c-d3d7-4048-bc05-f8089b3de22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The way langchain is storing history is uing `ConversationBufferMemory`\n",
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d6eda9-c801-49b7-acaa-6b4ab4c310f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add additonal things in the input\n",
    "memory.save_context({\"input\": \"Hi\"}, \n",
    "                    {\"output\": \"What's up\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2babc5-5695-44ad-bde3-5831d3ab1d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c67ccd-6434-4835-b6b4-d8005b384e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d99fb4b-1730-4634-9f37-7038b465c553",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Not much, just hanging\"}, \n",
    "                    {\"output\": \"Cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7fc490-0fa7-4795-95ae-427dc616d5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bdabe5-01e7-4223-b383-fad195e8fb64",
   "metadata": {},
   "source": [
    "## ConversationBufferWindowMemory\n",
    "when using LLM for conversation. \n",
    "\n",
    "- The LLM is actually stateless. \n",
    "    - Each transaction is independent\n",
    "    - chatbots  appear to have memory by providing the full conversation as 'context'\n",
    "    \n",
    "So memory story all the conversation and it is used as an input to LLm as a context. So that it can generate an output.\n",
    "\n",
    "As the memory storage become lomng its getting expensive to send tokens to the LLM. Langchain provides differnt kinds of memory and accumulate the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e6f55d-6c08-4b0c-80f6-8bcbab57e0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets look other type of memory. It only contains window of a memory\n",
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bd7ecb-a073-4109-8a00-a5fdafa161cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k=1 means I want to remeber only one conversational exchange.\n",
    "memory = ConversationBufferWindowMemory(k=1)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6b5ec0-e9ef-4f64-80e0-73fb0bda5217",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi\"},\n",
    "                    {\"output\": \"What's up\"})\n",
    "\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a236b9f-cb51-4077-89c2-8325d3e7c29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It only rememebr most recent conversation.\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842e328e-0660-4f9c-8abe-ea6897a16a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerun the conversation that we have\n",
    "llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=False # change this to true and see what LLm is doing and how memory change.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5c1137-1ad0-42cb-85a6-d60bf506435a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Hi, my name is Abhishek\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0443c071-17e2-4e4f-aee8-66c2715b1dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1d390e-0e4a-4df1-b1c2-6e21c9faea41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# because k=1 , we only remeber laterst conversation\n",
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ad4bec-1459-4156-abc7-a573d597ce90",
   "metadata": {},
   "source": [
    "Exercise: change this to true and see what LLm is doing and how memory change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17fcb93-cf43-46c6-abba-21acd654b797",
   "metadata": {},
   "source": [
    "## ConversationTokenBufferMemory\n",
    "With `ConversationTokenBufferMemory` the memory will limit the number of tokens saved, and because a lot of LLM pricing is based on tokens. That maps directly to the cost of the cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7053d384-e444-44c4-b97e-3ff0856f74be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39581707-4016-49e5-b401-72f99d59d9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.llms import OpenAI\n",
    "llm = ChatOpenAI(temperature=0.0, model=llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ff78b3-03e2-49a4-96c1-72ee7e560897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit the tokens to 50 `max_token_limit=50`\n",
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)\n",
    "\n",
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Beautiful!\"})\n",
    "\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, \n",
    "                    {\"output\": \"Charming!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1882f41f-9a97-4cf8-9bc6-7599cc1cbacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c71c6c1-ded8-4076-83d5-448d1eadb859",
   "metadata": {},
   "source": [
    "Exercise: Change the token size and check how much it is stored. Differnt LLM has differnt way of counting token.\n",
    "\n",
    "Change the prompt and see if it change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051aed6d-c912-4e7a-8813-eb8ee99a2384",
   "metadata": {},
   "source": [
    "## ConversationSummaryMemory\n",
    "instead of limiting the memory to fix number of tokens, based on most recent utterance or a fixed number of conversational exchanges. \n",
    "\n",
    "Lets use an LLM to write a summary of the conversation so far, and let that be the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6231685a-b9ef-47d9-abf6-c6189abe7dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff02246-2a3f-4495-9c10-95d89f6c5935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a long string about an schedule\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0938542d-3289-43e8-acf1-560700076e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a conversational buffer memory\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=500)\n",
    "\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "\n",
    "memory.save_context({\"input\": \"What is on the schedule today?\"}, \n",
    "                    {\"output\": f\"{schedule}\"})\n",
    "\n",
    "# Now the memory has a lot of information, because the size of tokens are enough\n",
    "# but if we reduce it will generate only latest conversaiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0998f6f9-319a-4dcd-b0f2-69dcd51caf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1cc99d-5fe4-440f-9414-87694fb8ea69",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc7d5bb-e16f-4857-a030-6fd391813e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"What would be a good demo to show?\")\n",
    "# you can se system message but its not the openAI API system message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5017d5b4-2a2b-49af-8c40-1faf22540788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check what happend to the memory.\n",
    "memory.load_memory_variables({})\n",
    "\n",
    "#  It has encorporated the most recent conversation whereas the human \n",
    "# utterance is encorporated in the system message."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85b6cc1-ce5b-4115-b83c-abffbfebcbb3",
   "metadata": {},
   "source": [
    "With the conversation summary buffer memory, what it tries to do is keep the explicit storage of the mesages upto the number of tokens defined and anything beyond that we will see in the history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8091e24-2c93-42f1-a03c-330bffd06777",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b29111-fe5e-4b0b-933d-b7b108dd04fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0806a11-a38a-4feb-9f5d-067b6e181407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b7f125-3389-4f31-895a-021e83f63a70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
