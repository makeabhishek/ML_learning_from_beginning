Differentiation is a crucial step in nearly all machine learning and deep learning optimization algorithms. While the calculations for taking these derivatives are straightforward, working out the updates by hand can be a tedious task. We will get a conceptual understanding of 
how __autograd__ works to find the __gradient__ of multivariable functions. \
We will discuss some fundamentals on _derivatives_, _partial derivatives_, _gradients_, and _Jacobians_. We then discuss how to compute _gradients_ using ```requires_grad=True``` and the ```backward()``` method. 
Thus, we cover ```classes``` and ```functions``` implementing __automatic differentiation__ of arbitrary scalar-valued and non-scalar-valued functions. We also discuss the _Jacobian matrix_ in PyTorch. 


