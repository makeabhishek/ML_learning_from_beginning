{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec0c7fc8-12c0-4c35-8da1-9a610400e03b",
   "metadata": {},
   "source": [
    "### Key Changes for pytorch:\n",
    "- __Dataset Handling:__ Replaced `h5py` with `scipy.io.loadmat` to read `.mat` files into PyTorch tensors.\n",
    "- __Model Definition:__ Used `torch.nn.Conv1d`, `torch.nn.MaxPool1d`, and `torch.nn.Linear` to replicate Keras layers.\n",
    "- __Training & Validation:__ Used `torch.utils.data.Dataset` and `torch.utils.data.DataLoader` for data batching.\n",
    "- __Training Process:__ Implemented manual training loop with binary cross-entropy loss and metrics calculation.\n",
    "\n",
    "\n",
    "### Keras to PyTorch Conversion\n",
    "\n",
    "#### ✅ Data Handling:\n",
    "- Reads `.mat` file and loads it into PyTorch tensors.\n",
    "- Balances positive & negative samples.\n",
    "\n",
    "#### ✅ CNN Model Definition:\n",
    "- Uses `Conv1d`, `ReLU`, `BatchNorm1d`, and `MaxPool1d`.\n",
    "- Implements `Flatten` and `Linear` layer for binary classification.\n",
    "\n",
    "#### ✅ Training:\n",
    "- Uses `BCELoss` and `Adam` optimizer.\n",
    "- Implements batch training with `DataLoader`.\n",
    "\n",
    "#### ✅ Evaluation:\n",
    "- Tracks loss and accuracy across epochs.\n",
    "- Saves model and visualizes training performance.\n",
    "\n",
    "#### ✅ Filter Visualization:\n",
    "- Prints the shape of convolution filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2105ee-a49f-4ca9-8803-9758a4840bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import h5py\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from scipy.io import loadmat\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e62a285-a6b6-4217-b719-0081834e809c",
   "metadata": {},
   "source": [
    "# ------------------- Step 1: Load Data -------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed245e2-870c-4aee-81b4-13d5ce968eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tStart = time.time()\n",
    "numRun = 1  # For running multiple tests\n",
    "\n",
    "# for ModRun in range(numRun): # This is to run multiple files. But for now just consider single file\n",
    "# dataFileName = f'training_data_Rec_7to11_Din_random_1%_11012024_test{ModRun+1}.mat'\n",
    "# filePath = D:\\CNN_John\\keras_CNN\\TrainingData\\Random\\Random_training\\\n",
    "dataFileName = f'training_data_Rec_7to11_Din_random_10%_11012024_test8.mat'\n",
    "matlabDat_train = h5py.File(dataFileName, 'r')\n",
    "\n",
    "print(matlabDat_train.keys())\n",
    "\n",
    "x_train = np.transpose(matlabDat_train['dat'][:])\n",
    "y_train = np.transpose(matlabDat_train['labels'][:])\n",
    "Nframes = matlabDat_train['Nframes'][0][0]\n",
    "winLen = matlabDat_train['winLen'][0][0]\n",
    "shift = matlabDat_train['shift'][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812a0951-1bc0-42cd-9a8e-781c9077ea8a",
   "metadata": {},
   "source": [
    "# ------------------- Step 2: Data Preprocessing -------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6c661a-da6e-49c5-98c7-fce21d1f3a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.expand_dims(x_train, axis=2)  # Adding channel dimension\n",
    "y_train = (y_train > 0.25).astype(\"int\")  # Binarization\n",
    "y_train = np.expand_dims(y_train, axis=2)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "x_train = torch.tensor(x_train, dtype=torch.float32).permute(0, 2, 1)  # Shape: (batch, channels, seq_len)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "# Reduce the number of negative-label samples\n",
    "Nsamples_pos = torch.sum(y_train).item()\n",
    "Nsamples_neg_in = y_train.numel() - Nsamples_pos\n",
    "Nsamples_neg = min(int(10 * Nsamples_pos), Nsamples_neg_in)  # class_sample_ratio = 10\n",
    "\n",
    "NegInds = torch.where(y_train.squeeze(2) < 0.5)[0]  # Indices of negative samples\n",
    "prune_size = len(NegInds) - Nsamples_neg\n",
    "\n",
    "if prune_size > 0:\n",
    "    remove_inds = torch.randperm(len(NegInds))[:prune_size]\n",
    "    NegInds = NegInds[remove_inds]\n",
    "\n",
    "    x_train = torch.cat([x_train, x_train[NegInds]], dim=0)\n",
    "    y_train = torch.cat([y_train, y_train[NegInds]], dim=0)\n",
    "\n",
    "# Create DataLoader for batch processing\n",
    "dataset = TensorDataset(x_train, y_train)\n",
    "dataloader = DataLoader(dataset, batch_size=250, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118a1010-aada-4180-a782-5b908587a1cd",
   "metadata": {},
   "source": [
    "# ------------------- Step 3: Define CNN Model -------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0349bd-0756-4f9a-ae8b-734d17802397",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers=5\n",
    "num_filters=16\n",
    "kernel_size=50\n",
    "dropout=0.2\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, input_size, num_layers=num_layers, num_filters=num_filters, kernel_size=kernel_size, dropout=dropout):\n",
    "        super(CNNModel, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        in_channels = 1  # Input channels (single-channel signal)\n",
    "\n",
    "        for l in range(num_layers):\n",
    "            out_channels = num_filters * (2 ** l)\n",
    "            layers.append(nn.Conv1d(in_channels, out_channels, kernel_size, padding=\"same\"))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.BatchNorm1d(out_channels))\n",
    "            layers.append(nn.MaxPool1d(2))  # Pool size of 2\n",
    "            in_channels = out_channels\n",
    "\n",
    "        self.conv_layers = nn.Sequential(*layers)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(in_channels, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return self.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18465a1-5d0b-492c-b5d5-741c23a569ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "input_size = x_train.shape[2]\n",
    "model = CNNModel(input_size=input_size)\n",
    "\n",
    "# ------------------- Step 4: Train the Model -------------------\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 15\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "train_losses, val_losses, val_accs = [], [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    for batch_x, batch_y in dataloader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x).squeeze()\n",
    "\n",
    "        loss = criterion(outputs, batch_y.squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        correct += (predicted == batch_y.squeeze()).sum().item()\n",
    "        total += batch_y.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_acc = correct / total\n",
    "    train_losses.append(epoch_loss)\n",
    "    val_accs.append(epoch_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "tStop = time.time()\n",
    "print(f\"Training time: {tStop - tStart:.1f} s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70987ab4-317a-4879-aa24-05d1562f66ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4824e898-524f-45db-8ece-da4407afa02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A more detailed summary (like in Keras), you can use `torchinfo`:\n",
    "\n",
    "# !pip install torchinfo\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "# Print model summary (input shape: batch_size=1, channels=1, input_size=your_input_length)\n",
    "summary(model, input_size=(1, 1, input_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b43cfd-c2eb-4117-8de3-fe8cecd74543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- Step 5: Plot Training Metrics -------------------\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training Loss\")\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(val_accs, label=\"Validation Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Validation Accuracy\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"Training_{ModRun+1}.png\")\n",
    "plt.show()\n",
    "\n",
    "# ------------------- Step 6: Save Model -------------------\n",
    "torch.save(model.state_dict(), f\"model_{ModRun+1}.pth\")\n",
    "\n",
    "# ------------------- Step 7: Visualize Filters -------------------\n",
    "for name, param in model.named_parameters():\n",
    "    if \"conv\" in name and \"weight\" in name:\n",
    "        print(f\"Layer: {name}, Filter Shape: {param.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2950f754-8a6c-4033-bca4-0ac41ac6711e",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "- Run on GPU: If available, the script will automatically utilize CUDA.\n",
    "- Hyperparameter Tuning: Modify kernel size, dropout, or layers as needed.\n",
    "- Inference: Load the saved model using:\n",
    "\n",
    "```\n",
    "model.load_state_dict(torch.load('model_1.pth'))\n",
    "model.eval()\n",
    "```\n",
    "Further Enhancements:\n",
    "- Add validation/test dataset split.\n",
    "- Use learning rate scheduling.\n",
    "- Implement early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2544e9-1a52-498b-af63-020a182dacc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
