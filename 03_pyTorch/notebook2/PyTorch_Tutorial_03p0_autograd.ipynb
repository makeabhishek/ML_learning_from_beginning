{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae3d0511-de44-4821-98cb-1df7a171f54d",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-size: 32px; font-weight: bold;\">\n",
    "     Calculus Background\n",
    "</div>\n",
    "Derivatives\n",
    "$$\n",
    "f: \\mathbb{R}\\rightarrow \\mathbb{R} \\quad \\text{input and output are scalars}\n",
    "$$\n",
    "function derivative of $f$\n",
    "$$\n",
    "f: f'(x) = \\lim_{x \\to 0} \\frac{f(x+h)-f(x)}{h} \\text{instantaneous rate of change}\n",
    "$$\n",
    "\n",
    "### Example\n",
    "$ f(x) = 3x^2 -4x $ \\\n",
    "$f'(x)= 6x-4$ \\\n",
    "$ f'(1)=2$\n",
    "\n",
    "### Can we check our Solution in Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "094e133a-648b-40cf-b74b-23db440988e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h=0.100000, numerical limit = 2.300000\n",
      "h=0.010000, numerical limit = 2.030000\n",
      "h=0.001000, numerical limit = 2.003000\n",
      "h=0.000100, numerical limit = 2.000300\n",
      "h=0.000010, numerical limit = 2.000030\n",
      "h=0.000001, numerical limit = 2.000003\n",
      "h=0.000000, numerical limit = 2.000000\n",
      "h=0.000000, numerical limit = 2.000000\n",
      "h=0.000000, numerical limit = 2.000000\n",
      "h=0.000000, numerical limit = 2.000000\n"
     ]
    }
   ],
   "source": [
    "# Define the function\n",
    "def f(x):\n",
    "    return 3*x**2 - 4*x\n",
    "\n",
    "# Define the limit\n",
    "def numerical_lim(f, x, h):\n",
    "    return (f(x+h) -  f(x)) /h\n",
    "\n",
    "# Lets choose h\n",
    "h = 0.1\n",
    "for i in range(10):\n",
    "    x=1 # find value of numerical_lim at x = 1\n",
    "    print(f'h={h:.6f}, numerical limit = {numerical_lim(f, x, h):.6f}') \n",
    "    h*=0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263784b9-ee05-463d-84a9-f3bdf910d784",
   "metadata": {},
   "source": [
    "# Parital Derivatives of multi variable function\n",
    "$$\n",
    "f: \\mathbb{R^n}\\rightarrow \\mathbb{R} \\quad \\text{inputs: n variables /output: scalars}\n",
    "$$\n",
    "function $(f(x_1, x_2, x_3 ... x_{i-1}, x_{i}, x_{i+1}, ... x_n))$\n",
    "partial derivative $\\frac{\\partial f}{\\partial x_i}$\n",
    "\n",
    "### Example \n",
    "we have a function of variable $f(x_1 , x_2)$\n",
    "$f(x_1 , x_2) = x_1^2 + x_2^2$ \\\n",
    "$\\frac{\\partial f}{\\partial x_1} = 2x_1$ \\\n",
    "$\\frac{\\partial f}{\\partial x_2} = 2x_2$\n",
    "\n",
    "# Gradient\n",
    "Assume we have a function $f$ with $n$ input variables, written as \\\n",
    "The gradeint of $(f(x_1,  ..., x_n))$ is a vector of $n$ partial derivatives\n",
    "$$\n",
    "\\nabla f(x) =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial x_1} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial f}{\\partial x_n} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Lets assume  we have a function $f(\\vec{x})$, which is simply a Euclidian norm \n",
    "\n",
    "$$\n",
    "f(\\vec{x}) = \\|x\\|_{2}^{2} = \\vec{x}^T\\vec{x} = x_{1}^{2} + \\dots + x_{n}^{2}\n",
    "$$\n",
    "\n",
    "Note: Here it is __Gradeint $(\\nabla)$__ not __Jacobian $(J)$__, because of scalar valued function\n",
    "\n",
    "$$\n",
    "\\nabla f(x) =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial x_1} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial f}{\\partial x_n} \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "2x_1 \\\\\n",
    "\\vdots \\\\\n",
    "2x_n \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "2\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "\\vdots \\\\\n",
    "x_n \\\\\n",
    "\\end{bmatrix}\n",
    "= 2\\vec{x}\n",
    "$$\n",
    "\n",
    "To perfomr the these partial derivative we use `torch.autograd`\n",
    "\n",
    "## First look at `torch.autograd`\n",
    "This is pytorch automatic differentiation engine to obtain gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1fae9f-0402-4c4c-a76f-27432f6f47b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# we are telling PyTorch that at some point we want to find partial derivative w.r.t this variable. This is where those \n",
    "# gradient information will be saved `x.grad`. Remember this is a n attribute not a function. Therefore we donot use double paranthesis () in\n",
    "# front of the name\n",
    "x = torch.arange(4.0, requires_grad=True) \n",
    "print(x)\n",
    "\n",
    "print(x.grad) # This default value is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "338482fb-64e4-44ae-b757-6d139b27f75f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Define y to perfrom inner product or dot product between x and x. Same as in above example we did\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mdot(x,x)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# To find the partial derivatives we just need to use `.backward`\u001b[39;00m\n\u001b[0;32m      4\u001b[0m y\u001b[38;5;241m.\u001b[39mbackward() \u001b[38;5;66;03m#dy/dx\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# Define y to perfrom inner product or dot product between x and x. Same as in above example we did\n",
    "y = torch.dot(x,x)\n",
    "# To find the partial derivatives we just need to use `.backward`\n",
    "y.backward() #dy/dx\n",
    "print(x.grad)\n",
    "\n",
    "x.grad == 2*x # the gradeint is 2 times the vector `x` as shown in provious example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeb593a-13b2-4a06-8c9f-201ead7582a6",
   "metadata": {},
   "source": [
    "# Example\n",
    "$$f()=x_1 + x_2 + \\dots + x_n$$\n",
    "$$\n",
    "\\nabla f(x) =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial x_1} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial f}{\\partial x_n} \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1\\\\\n",
    "\\vdots \\\\\n",
    "1 \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989fc91d-0dd3-4563-b572-195e284df6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch accumulates the gradeint by default, we need to clear the previous\n",
    "x.grad.zero_()\n",
    "y = x.sum()\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2481a2d2-88e7-47d5-89ff-389c1a994231",
   "metadata": {},
   "source": [
    "## Backward for non-scalar variables\n",
    "We have input vector x that has three variables. Instead of output being scalar now we ahve vector output.\n",
    "$$\n",
    "\\vec{x} =\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3  \\\\\n",
    "\\end{bmatrix}\n",
    "\\rightarrow \\\n",
    "\\vec{y} =\n",
    "\\begin{bmatrix}\n",
    "x_1^2 \\\\\n",
    "x_2^2 \\\\\n",
    "x_3^2  \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "let $x_1^2 =y_1, x_2^2=y_2, x_3^2=y_3 $ \\\n",
    "$$\n",
    "(f: \\mathbb{R}^3\\rightarrow \\mathbb{R}^3)\n",
    "$$\n",
    "Here, we have vector elements. To find partial derivative we need to use Jacobian matrix , whcih is partial derivative of output w.r.t.  variables.\n",
    "\n",
    "$$\n",
    "J = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial \\vec{y}}{\\partial x_1} & \\frac{\\partial \\vec{y}}{\\partial x_2} & \\frac{\\partial \\vec{y}}{\\partial x_3} \\\\\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial y_1}{\\partial x_1} & \\frac{\\partial y_1}{\\partial x_2} & \\frac{\\partial y_1}{\\partial x_3} \\\\\n",
    "\\frac{\\partial y_2}{\\partial x_1} & \\frac{\\partial y_2}{\\partial x_2} & \\frac{\\partial y_2}{\\partial x_3} \\\\\n",
    "\\frac{\\partial y_3}{\\partial x_1} & \\frac{\\partial y_3}{\\partial x_2} & \\frac{\\partial y_3}{\\partial x_3} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Given any vector $vec{v}$, compute the product $J^T \\cdot vec{y}$. PyTorch compute transpose of Jacobain. We have to pass $vec{y}$ as an input argument to `torch.backward()`\n",
    "\n",
    "As we defiend earlier  $x_1^2 =y_1, x_2^2=y_2, x_3^2=y_3$. We pass vector $1$. This is called partial derviative w.r.t. self \\\n",
    "\n",
    "$$\n",
    "J =  \n",
    "\\begin{bmatrix}\n",
    "2x_1 & 0 & 0 \\\\\n",
    "0 & 2x_2 & 0 \\\\\n",
    "0 & 0 & 2x_3 \\\\\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "1\\\\\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "2x_1\\\\\n",
    "2x_2 \\\\\n",
    "2x_3 \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e585f6df-62b3-4bf9-9651-362ec2eb9465",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.arange(3.0, requires_grad=True)\n",
    "y= x*x # element wise operation\n",
    "y.backward(torch.tensor([1.0, 1.0, 1.0]))\n",
    "print(x.grad)\n",
    "\n",
    "\"\"\"\n",
    "# What if you pass vector [1.0, 1.0, 0.0])\n",
    "import torch\n",
    "x = torch.arange(3.0, requires_grad=True)\n",
    "y= x*x # element wise operation\n",
    "y.backward(torch.tensor([1.0, 1.0, 0.0]))\n",
    "print(x.grad)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3969c99-0b02-4cba-aaa5-89186b93d0e6",
   "metadata": {},
   "source": [
    "# Another Example\n",
    "$$\n",
    "Q = 3a^3 - b^2 \\\n",
    "$$\n",
    "Make it a bit more complex. $a$ and $b$ are vector valued\n",
    "\n",
    "$$\n",
    "a = \n",
    "\\begin{bmatrix}\n",
    "a_1\\\\\n",
    "a_2 \\\\\n",
    "\\end{bmatrix}\n",
    ", b = \n",
    "\\begin{bmatrix}\n",
    "b_1\\\\\n",
    "b_2 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Q = \n",
    "\\begin{bmatrix}\n",
    "Q_1\\\\\n",
    "Q_2 \\\\\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "3a_1^3 - b1^2\\\\\n",
    "3a_2^3 - b_2^2\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "J =  \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial Q_1}{\\partial a_1} & \\frac{\\partial Q_1}{\\partial b_1} & \\frac{\\partial q_1}{\\partial a_2} & \\frac{\\partial Q_1}{\\partial b_2}\\\\\n",
    "\\frac{\\partial Q_2}{\\partial a_1} & \\frac{\\partial Q_2}{\\partial b_1} & \\frac{\\partial q_2}{\\partial a_2} & \\frac{\\partial Q_2}{\\partial b_2}\\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "9a_1^2 & -2b_1 & 0 & 0 \\\\\n",
    "0 & 0 & 9a_2^2 & -2b_2 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "J^T \\cdot \\vec{v} = \n",
    "\\begin{bmatrix}\n",
    "9a_1^2 & 0 \\\\\n",
    "-2b_1 & 0\\\\\n",
    "0 & 9a_2^2 \\\\\n",
    "0 & -2b_2\\\\\n",
    "\\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "9a_1^2 \\\\\n",
    "-2b_1\\\\\n",
    "9a_2^2 \\\\\n",
    "-2b_2\\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3177d27-0ed7-4ac7-a29c-5d092379bb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.tenosr([2., 3.], requires_grad = True)\n",
    "b = torch.tenosr([6., 4.], requires_grad = True)\n",
    "\n",
    "Q = 3*a**3 - b**2\n",
    "\n",
    "external_grad = torch.tensor([1., 1.])\n",
    "Q.backward(gradient = external_grad)\n",
    "print(a.grad, b.grad)\n",
    "\n",
    "\"\"\"\n",
    "# Change the external_grad = torch.tensor([1., 0.])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fa2f4f-03c8-4c39-99e6-957ee0801199",
   "metadata": {},
   "source": [
    "# Detaching Computation:\n",
    "PyTorch tracks what type of operation you are doing on particular varaible in __Computational Graph__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7430df1-322a-4f3e-a8fa-7bd34c1adc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.tensor(4.0, requires_grad=True)\n",
    "y = x*x\n",
    "u = y\n",
    "z = u*x  # z = [x_1^3 x_2^3 x_3^3 x_4^3]. Partial derivative = 3x_i^3 \n",
    "\n",
    "z.sum().backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ec089c-6a03-4857-b423-52c4263b0174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.tensor(4.0, requires_grad=True)\n",
    "y = x*x\n",
    "u = y.detach() # $u$ doesnot have any dependencies on $x$\n",
    "z = u*x # z =[u_1x_1 u_2x_2 u_3x_3 u_4x_4] . partial derivative = u_i\n",
    "\n",
    "z.sum().backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf81c5e6-fa4e-4df8-a836-37059d176c96",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"text-align: center; font-size: 32px; font-weight: bold;\">\n",
    "    Gradient Calculation with Autograd\n",
    "</div>\n",
    "\n",
    "- Lets see the autograd package in pytorch and see how we can calculate gradeints from it. Gradients are important for model optimization\n",
    "\n",
    "- We want to calculate gradient of some function w.r.t. x, so we have to specify the argument ` requires_grad = True`\n",
    "\n",
    "- \n",
    "```\n",
    "# Create a tensor\n",
    "x = torch.randn(3, requires_grad = True)\n",
    "Print(x)\n",
    "# >>> tensor([-0.3172, 0.3467, 0.6648], requires_grad=True)\n",
    "```\n",
    "- Whenever we do operations with this, pytorch will create a computational graph for this. Let’s do an operation\n",
    "$$𝑦=𝑥+2$$\n",
    "- This will create a CG which looks like the below image. For each operation we have a node with inputs and outputs. Here we have operation is addition. We have input x and 2; output is y. Pytorch creae a computaitonal graphy using use backpropagation to calcaulte the gradients\n",
    "\n",
    "<center><img src='./images/CG_addition.PNG' width=350px></center> \n",
    "\n",
    "- With this graph a technique called __backpropagation__  is used to calculate the gradeints.\n",
    "- First we do forward pass, where we apply operation and calculate output $y$. Since we specified that it requires gradient. Pytroch will automatically create and store a function for us and this function is than used in backpropagation to get the gradients. \n",
    "- Here $y$ has an attribute `grad_fn`, which point to a gradient function. In this case it is called `Add Backward`. \n",
    "With this function we can calculate the gradients in the backward path i.e., $𝑑𝑦/𝑑𝑥$.\n",
    "\n",
    "---\n",
    "```\n",
    "y = x+ 2\n",
    "Print(y)\n",
    "# >>> tensor([-1.6828, 2.3467, 2.6648], grad_fn=<AddBackward0>)\n",
    "```\n",
    "- We can see the `AddBackward0` function when we print $y$, because our operation is additon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5b012b-5276-4a68-a68b-921bfc91d6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when we need to calculate the gradient, later for the optimization step.\n",
    "import torch\n",
    "x = torch.randn(3)\n",
    "print(x)\n",
    "\n",
    "# If we need to Calcualte gradients of some fucntion w.r.t. x, we give additonal arguemnts to tensor\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c328040b-249c-4e31-adf2-d59ba80ccfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# whenever we will do a computaiton with this tensor. Pytorch will create a computaional graph. \n",
    "# let's do some operation\n",
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2934488-b6c2-420f-a970-01182822c3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets do some more operations with our tensors.\n",
    "z = y*y*2 # also has grad funciton attributes\n",
    "print(z)\n",
    "# >>> tensor([7.2041, 0.3141, 7.1014], grad_fn=<MulBackward0>)\n",
    "\n",
    "z = z.mean # also has grad funciton attributes\n",
    "print(z)\n",
    "# >>> tensor([8.9153], grad_fn=<MeanBackward0>)\n",
    "\n",
    "# Now if we want to claculate the gradeints w.r.t. $x$. The only thing we have to do is to call `.backward`\n",
    "z.backward() # dz/dx\n",
    "# $x$ than has a gradeint with `.grad` attributes where the gradeints are stored.\n",
    "print(x.grad) # so we have the gradeint for the tensors\n",
    "# >>> tensor([0.0160, 3.3650, 4.5153]) \n",
    "\n",
    "# NOTE: It should be noted that we have to multiply $J \\cdot v$. So  if we have a scalar value, than we dont need to\n",
    "# put any vector argument. But if we have dont ahve a scalar, than we ahve to multiply vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98001e31-f585-4703-bc65-29cf88c96151",
   "metadata": {},
   "source": [
    "So in the backgraound it is createing a vector Jacobain product to get the gradeints, whcih looks like\n",
    "\n",
    "#### Vector Jacobian \n",
    "$$\n",
    "J\\cdot v = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial y_1}{\\partial x_1} & \\cdot \\cdot \\cdot & \\frac{\\partial y_m}{\\partial x_1}\\\\\n",
    "\\vdots & \\ddots & \\vdots\\\\\n",
    "\\frac{\\partial y_1}{\\partial x_n} & \\cdot \\cdot \\cdot & \\frac{\\partial y_m}{\\partial x_n}\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial l}{\\partial y_1} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial l}{\\partial y_m} \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial l}{\\partial x_1} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial l}{\\partial x_n} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- Here we have a Jacobain Matrix (first term on R.H.S), multiplied with gradeitn vector. Than we get the final gradients in wheihc we are interested. this is called __Chain Rule__.\n",
    "- It should be noted that we have to multiply $J \\cdot v$. So  if we have a scalar value, than we dont need to put any vector argument. But if we have dont ahve a scalar, than we ahve to multiply vector\n",
    "```\n",
    "z = y*y*2 # also has grad funciton attributes\n",
    "print(z)\n",
    "\n",
    "z.backward() # dz/dx\n",
    "```\n",
    "\n",
    "#### <span style=\"color: red;\">This will give an error</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370ffe8f-b1a8-4c0d-a3de-3be2647e968b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.ones(5, requires_grad=True)\n",
    "y = x + 2\n",
    "z = y*y*2 # also has grad funciton attributes\n",
    "print(z)\n",
    "z.backward() # dz/dx\n",
    "# ERROR: grad can be implicitly created for scalar outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f22488-6b34-4904-957b-9442d54bc022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we have to create a vector of the same size\n",
    "v = torch.tensor([0.1, 1.0, 0.001], dtpye = torch.float32)\n",
    "z.backward(v) # dz/dx\n",
    "print(x.grad)\n",
    "# So we should remeber that in backgrpund or theory it as a vector Jacobian product. Most of the times\n",
    "# last operations create a scalar value so we dont need vector and call without an argument. But if its a vector we have to pass vector argument"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d490e2a9-62d2-4a81-8f20-8f8577bfcd44",
   "metadata": {},
   "source": [
    "### try above if `x = torch.ones(5, requires_grad=True)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e4012b-c11b-4d51-bb8b-7afa7e5cc66d",
   "metadata": {},
   "source": [
    "### Prevent Pytorch from tracking gradients history and calculating `grad_fn` attributes\n",
    "- How to stop autograd from tracking history\n",
    "- How to zero (empty) gradients\n",
    "- Example: sometimes in our training loop when we want to update our trainign weights. When we update the weights, this operaion should not be part of gradeint computaiton. We can do it in three ways\n",
    "- Stop pytorch in creat `grad_fn`\n",
    "- There three ways to do that\n",
    "```\n",
    "x.requires_grad_(False)\n",
    "x.detach() # create new tensor whcih doent require gradient\n",
    "with torch.no_grad(): # wratp this with with statement. Than we can do our operations\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03727d56-58b3-4955-bc87-3fb135c25e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.ones(5, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85efdf68-8402-4b83-bc34-43255354cdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First option\n",
    "x.requires_grad_(False) # rememebr whenever we have fucntion with underscore at the end it will modify our varaible inplace.\n",
    "print(x) # this doent have `requires_grad` attribute anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b969d017-7c5d-47ec-b35a-b024f543f947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second Option: this will create a new tensor with the same new values but doent require the gradeints\n",
    "y = x.detach()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d37a48-6dbb-4290-a88b-1d31a0fa1299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third option: wrap in a `with` statement\n",
    "with torch.no_grad():\n",
    "    y = x + 2\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bc2743-3423-4470-93f2-767a3a5f4ba4",
   "metadata": {},
   "source": [
    "### NOTE: \n",
    "\n",
    "whenever we call the __backward fucntion__ than the grdient for the tensor is accumulated in the `.grad` attribut. So the values will be summed up. So we must be very carefull lets create soem dummy training example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64e9f12-7072-49f7-8f81-bd70b2093bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.ones(4, requires_grad=true)\n",
    "\n",
    "# training Loop\n",
    "# First lets do only for one epoch\n",
    "for epoch in range(1):\n",
    "    model_output = (weights*3).sum() # this is a dummy operation whcih will simulate some model output\n",
    "    # calculate the gradeints\n",
    "    model_output.backward()\n",
    "    # call .grad to print\n",
    "    print(weights.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f86e2be-ef47-46fc-b4ea-5c2bc589f5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we do it for another operation. We can see that the grad is accumulated.\n",
    "for epoch in range(2):\n",
    "    model_output = (weights*3).sum()\n",
    "    model_output.backward()\n",
    "    print(weights.grad)\n",
    "\n",
    "# Similarly do it for 3 epoch, we can see that the gredients are summed up\n",
    "for epoch in range(3):\n",
    "    model_output = (weights*3).sum()\n",
    "    model_output.backward()\n",
    "    print(weights.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec4e1dc-371f-4ab1-8383-4ee9c0d21b4f",
   "metadata": {},
   "source": [
    "### Empty the gradients\n",
    "We can see all the values are summed up and gradeints are not correct. So before we do the next iteration in optimization step we must __empty the gradients__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd625bcd-439e-40e8-a6ab-a7803655bd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(2):\n",
    "    model_output = (weights*3).sum()\n",
    "    model_output.backward()\n",
    "    \n",
    "    print(weights.grad)\n",
    "    \n",
    "    weights.grad.zero_() # This is important to make gradeints zero\n",
    "    \n",
    "    print(weights.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9cbcec-2e0d-4e35-9f54-7f4f907d34a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# later we will work on pytorch builtin optimizer. So we have to do the same.\n",
    "# from the torch optimization package  `toch.optim.`\n",
    "weights = torch.ones(4, requires_grad=true)\n",
    "optimizer = toch.optim.SGD(weights, lr=0.01)\n",
    "# With this optimizer we can do the optimization step\n",
    "optimizer.step()\n",
    "# before doing the next step empty the gradeints\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c003ce-deb3-426e-a46a-6626d64ed817",
   "metadata": {},
   "source": [
    "### Summary\n",
    "- So for now the things you should remeber is that whenever we want to caluate teh gradeints. We have to put attribute `requires_grad=true`.\n",
    "`weights = torch.ones(4, requires_grad=true)` \\\n",
    "- Than calcualte the gradeints by calling the backward fucntion. \\\n",
    "`z.backward()` \\\n",
    "- Before calling next iteration in our optimization step. we must empty the gradeint \\\n",
    "` weights.grad.zero_()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830b71e0-351d-450a-9afd-7205866531f4",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-size: 32px; font-weight: bold;\">\n",
    "    Backpropagation - Theory With Example\n",
    "</div>\n",
    "\n",
    "Back Propagation Algorithm to Calcualte Gradients\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04780ff3-b22b-42e2-a48a-15f87159b495",
   "metadata": {},
   "source": [
    "### Chain Rule\n",
    "<center><img src='./images/chainRule.PNG' width=350px></center> \n",
    "\n",
    "\n",
    "Lets say we have two operations or two functions. We have input $x$ and apply a fucntion $a(x)$ which give output $y$, whcih is further pass to fucntion $b(y)$ and give final output $z$. We want to minimize $z$, so we want to know the derivative of `z` w.r.t. $x$. $\\frac{dz}{dx} =?$ We can do this using Chain rule, so we first compute derivative locally\n",
    "$$\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial y} \\cdot \\frac{\\partial y}{\\partial x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85804fbc-4f6c-49c2-bfe6-2714275fb0c2",
   "metadata": {},
   "source": [
    "### COMPUTATIONAL Graph (CG):\n",
    "- Next thing is CG. So for every operations we do with tensors, PyTorch will create a CG. For each node we apply some operation/function with some inputs and we get an output.\n",
    "- \n",
    "#### Example\n",
    "- Here we did multiply operation of $x$ and $y$ so the node has multiply operator.\n",
    "- For these nodes we can calculate so called __local gradients__ and we can use them in the __chain rule__ to get final gradient.\n",
    "- Local gradients are easy because we know the fucntion at the nodes.\n",
    "\n",
    "<center><img src='./images/CG_explained.PNG' width=450px></center> \n",
    "\n",
    "#### Why do we need  local Gradients?\n",
    "- because the Graph has more operations. At the end we have to calculate the Loss function, which we want to minimize.\n",
    "- Lets assume if we know the loss at $z$ position i.e, $\\frac{\\partial loss}{\\partial z}$, and we can obtain the final gradeint we want with the chain rule. \n",
    "$$\\frac{\\partial loss}{\\partial x} =\\frac{\\partial loss}{\\partial z}.\\frac{\\partial z}{\\partial x}$$\n",
    "where $\\frac{\\partial z}{\\partial x}$ is the local gradient\n",
    "\n",
    "The whole concept consist of three steps \n",
    "1. Forward Pass: Apply all the functions and Compute Loss\n",
    "2. Compute Local Gradients: At each node\n",
    "3. Backward Pass: Compute the gradient of the loss w.r.t. weights $dLoss/dWeight$ using chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ee1806-b0ea-480b-81af-a0d57d6c5589",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "- In Regression we want to predict continuous walues. However in classification we want to predict discrete values like 0 or 1.\n",
    "- Approximation :\n",
    "$$\\hat{y} = wx + b$$\n",
    "\n",
    "where $w$ is the slope and $b$ is the intercept or shift on the $y-$axis for 2D case. We have to come up with an algorithm to find w and b. For that we have to define cost function. In linear regression it is mean squared Error. Cost function: \n",
    "\n",
    "$$MSE = J(w,b) = \\frac{1}{N} \\sum_{i=1}^{n} (y_i - (wx_i + b))^2$$\n",
    "\n",
    "- We want to minimise this error. To find the minimum we have to fins d the derivative or gradient. So we want to calculate the gradient w.r.t. $w$ and $b$ $J'(m,b) =$\n",
    "- We model our output with a linear combination of some weights and input so \\hat{y} = w.x. We formualte the loss ficntion. Lets assume is squared error.\n",
    "- $$Loss = predicted y - actual y)^2$$\n",
    "- $$Loss = (\\hat{y} - y)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be10cee2-a7e3-4f23-9c63-75afe8fba791",
   "metadata": {},
   "source": [
    "## Example: Linear Regression\n",
    "- We model our output with a linear combination of some weights and some inputs. \n",
    "- Approximation :\n",
    "$$\\hat{y} = wx$$\n",
    "where $w$ is the slope. We have to come up with an algorithm to find w and b. For that we have to define cost function. In linear regression it is mean squared Error. For simplicity just take sqared error. Else we will have another opeation to get the mean. \n",
    "\n",
    "- We formulate our loss function. Assume a squared error. It should be mean squared error. So the loss is the differentce between predicted and actual $Loss = predicted y - actual y)^2$ and we square it.\n",
    "$$ loss = $(\\hat{y} -y)^2 = (wx - y)^2$$$ \n",
    "\n",
    "- Now we want to minimize our loss function. To know the derivative of loss w.r.t. the weigths\n",
    "$$Mimize \\quad Loss \\rightarrow \\frac{\\partial {Loss}}{\\partial w}$$\n",
    "\n",
    "#### how to get derivative of loss w.r.t. the weigths?\n",
    "We apply three steps \n",
    "1. Forward Pass: Apply all the functions and Compute Loss\n",
    "2. Compute Local Gradients: At each node\n",
    "3. Backward Pass: Compute the gradient of the loss w.r.t. weights $dLoss/dWeight$ using chain rule.\n",
    "\n",
    "<center><img src='./images/linear_regression_example.PNG' width=700px></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e2c8fa-9368-4bda-b502-eea69dcfb356",
   "metadata": {},
   "source": [
    "Example: Let's x, and y are our training samples \\\n",
    "#### (1) Forward Pass, $x=1, y=2$\n",
    "- We initialize our weights, $w=1$\n",
    "- Do the Forward pass. At first node we multiply $x$ and $w$. $\\hat{y}=1\\cdot 1 = 1$\n",
    "- Next node we do a substraction So $S = \\hat{y} - 2 =  1-2=-1$\n",
    "- At very end we square the $S$. $Loss = 1^2 = 1$\n",
    "\n",
    "####  (2) Calculate local gradient\n",
    "- At last node $\\frac{\\partial loss}{\\partial S} = \\frac{\\partial S^2}{\\partial S} = 2S$\n",
    "- At second last node: $\\frac{\\partial S}{\\partial \\hat{y}}=\\frac{\\partial (\\hat{y}-y)}{\\partial \\hat{y}} = 1$\n",
    "- At the first node: $\\frac{\\partial \\hat{y}}{\\partial w}=\\frac{\\partial wx)}{\\partial w} = x$\n",
    "- We don't ned the derivative of $x$ and $y$. Becaiuse these are fix values. We are only interested in our parameters whcih we want to update.\n",
    "\n",
    "####  (3) Now Do backward Pass\n",
    "Now we use our local gradeints to claculate the final loss . We have three gradeints so compute loss w.r.t. each\n",
    "- $\\frac{\\partial loss}{\\partial \\hat{y}} =\\frac{\\partial loss}{\\partial S} \\cdot \\frac{\\partial S}{\\partial \\hat{y}} = 2\\cdot S \\cdot 1 = -2$\n",
    "- $\\frac{\\partial loss}{\\partial w} =\\frac{\\partial loss}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial \\hat{w}} = -2 \\cdot x = -2$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151a5abe-fa4c-433f-adfb-6c4cb1fd8983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To minimise the loss we apply three steps.\n",
    "import torch\n",
    "x = torch.tensor(1.0) y = torch.tensor(2.0)\n",
    "\n",
    "# This is the parameter we want to optimize -> requires_grad=True\n",
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "# forward pass to compute loss\n",
    "y_predicted = w * x\n",
    "loss = (y_predicted - y)**2\n",
    "print(loss)\n",
    "\n",
    "# First gradient after forwward and backward pass: backward pass to compute gradient d Loss/dw\n",
    "loss.backward()\n",
    "print(w.grad) # It should be -2 in the begining\n",
    "\n",
    "# update weights: Next forward and backward pass...\n",
    "\n",
    "# continue optimizing:\n",
    "# update weights, this operation should not be part of the computational graph\n",
    "with torch.no_grad():\n",
    "    w -= 0.01 * w.grad\n",
    "# don't forget to zero the gradients\n",
    "w.grad.zero_()\n",
    "\n",
    "# next forward and backward pass...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e69fbc-87b9-4861-b8ef-3833f61c24ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c576c869-733b-4b80-9bb9-065a2053c85c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7598d390-98da-482b-a21e-a11e364d89f0",
   "metadata": {},
   "source": [
    "## 🎉 **Thank You!** 🙌  \n",
    "### 🚀 Happy Coding & Keep Learning! 💡\n",
    "\n",
    "## <span style=\"color: yellow;\">We will see the Backpropagation with detailed theory and example in next notebook</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afdb8d6-f160-4e90-ac3c-2da37b2dd6fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
