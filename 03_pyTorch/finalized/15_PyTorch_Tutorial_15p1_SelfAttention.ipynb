{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5db93e06-d82c-4593-b266-62c61201ee4d",
   "metadata": {},
   "source": [
    "# Attention in Transformers: Concepts and Code in PyTorch\n",
    "Attention mecahnism is a key breakthorugh in transformer development. \n",
    "\n",
    "Ref: \n",
    "1. Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. \"Neural machine translation by jointly learning to align and translate.\" arXiv preprint arXiv:1409.0473 (2014).\n",
    "2. Luong, Minh-Thang, Hieu Pham, and Christopher D. Manning. \"Effective approaches to attention-based neural machine translation.\" arXiv preprint arXiv:1508.04025 (2015).\n",
    "3. Vaswani, Ashish, et al. \"Attention is all you need.\" Advances in neural information processing systems 30 (2017).\n",
    "\n",
    "Machine Translation:\n",
    "\n",
    "encoder decoder mechanism works better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2e0009-8ddd-4ca3-9333-c64a8b0025e5",
   "metadata": {},
   "source": [
    "# Attention in transformers: Concept and code in PyTorh\n",
    "## The main idea behind Transformers and attention\n",
    "\n",
    "lanuage models like chatgpt is based on transformers. Transformers can look complicated, but fundamanetally they require 3 main parts.\n",
    "1. word embedding: Converts inputs into numbers. Converts words, bits of words and symbols , collectively called __Tokens__, into numbers. We need this because Transformers are a type of Neural Networkd and NN only have numbers for input values. For example: if the input is \"\" Tell me about Pizza!\", than word embedding will convert it into numbers $[0.5 1.3 2.03 -0.45!]$\n",
    "2. Positional Encodeing: Helps keep track of word order. Squatch eats pizza or pizza eat Squatch.\n",
    "3. Attention: Helps establish realtionships among words. Transformer establishes realtionships among words with Attention. Example, \"The Pizza came out of the oven and it tasted good. The word 'it' can refer to pizza or oven\" Thats why transformer correctly associate the word with Pizza not with the oven. Transformers have something called Attention, which is a mecahnism to correclty assocaite the word 'it' witht he word 'pizza'. There are differnet types of attention,\n",
    "    - __Self Attention:__ self attention works by seeing how similar each word is to all of the words in the sentens, inclsing itself. Self attention calcualtes the similarities for every word in the sentense. Once the similarities are calcualted they are used to determine how the __Transformer__ encode each word.\n",
    "      $$ Attention(Q,K,V)=SoftMax \\bigg(\\frac{QK^T}{\\sqrt{d_k}}\\bigg)V$$\n",
    "    - Masked-Slef-attention: $$ Masked-Slef-attention (Q,K,V,M)=SoftMax \\bigg(\\frac{QK^T}{\\sqrt{d_k}}+ M\\bigg)V$$\n",
    "    - Multi-Head atention: when we have multi attetnion heads\n",
    "    - Encoder-decoder attention / cross-atetnion: example Seq2Seq model\n",
    "  \n",
    "## The matrix math for calcualting self-attention\n",
    "At first glance, the equation for calculating Self-Atention can be a little intimidating ... \n",
    "$$ Attention(Q,K,V)=SoftMax \\bigg(\\frac{QK^T}{\\sqrt{d_k}}\\bigg)V$$\n",
    "The term query, key and value comes from database terminoloigy. Example, lets look the database. if the database has lastname and room number of a hotel. If a search a name from the database than this serach anme is query and the stored values in the databaser is called key. In database terminology room numbers are values.\n",
    "- $Q$ stands for query: It is the things we use to search the database.\n",
    "- $K$ stands for Key. The computer calculates the similarity between query and all the keys and return the ranks\n",
    "- $V$ stands for Value. The values are what the database turns results of the search.\n",
    "- $d_k$ is the dimension of the key matrix and dimension refers to number of values we have for each token, \n",
    "### How we determine the Queries, Keys, and Values in the context of transformer.\n",
    "- Rememeber that self-attention alcualtes similarity between each word and itself and all of the other words. that means we need to calcualte query and key for each word and just like we saw in the database example, each key needs to return a value. Example. Lets assume we write in the prompt \"Write a poem\"\n",
    "    - First things transfromer do with these words . It woill convert into word embedding. Than transformer adds positional encodeing to the word embedding to get these numbers or encodings that represent each word in the prompt. NOTE: in this simple example , we're just going to use 2 numbers to reapresent each word in the prompt. However, it's much more common to use 512 or more numbers to represent each word.\n",
    "    Note that I have put the weights as transpose. This is because PyTorch prints out the weights in a way that requires them to be transposed before we van get the math to work out correctly.\n",
    "- dot products (unscaled) can be used as an unscalaed measure of similarity between two things, and this metric is cloasely realted to something called __Cosine Similarity__. The big difference is that the cosine similarity scales Dot Product to be between $-1$ and $1$. In contrast the dot prodcut similarity isn't scaled.\n",
    "- once we divide by $d_k$ , we get scaled dot-product similarity. Note: saling by just $d_k$ of the values per token doesn't scale dot product similarities in any kind of systematic way. Thats said, even with this limited scaling the original authors of the transformer said that it improved perfromance.\n",
    "\n",
    "\n",
    "In sumaary, the equation for self attention does is calclcualte the scaled dot-proidcut similarieties among all of the words, convert those scaled similarities into percentages with the `SoftMax()` fucniton and then use those percentages to sclae the values to becaome the self-attention scalere for each word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd9e03e-0f1b-4dc0-8818-b803f6543e72",
   "metadata": {},
   "source": [
    "---- \n",
    "\n",
    "In this tutorial, we will code **Self-Attention** in **[PyTorch](https://pytorch.org/)**. **Attention** is an essential component of neural network **Transformers**, which are driving the current excitement in **Large Language Models** and **AI**. Specifically, an **Enecoder-Only Transformer**, illustrated below, is the foundation for the popular model **BERT**. \n",
    "\n",
    "<img src=\"./images/encoder_only_1.png\" alt=\"an enecoder-only transformer neural network\" style=\"width: 800px;\">\n",
    "\n",
    "At the heart of **BERT** is **Self-Attention**, which allows it to establish relationships among the words, characters and symbols, that are used for input and collectively called **Tokens**. For example, in the illustration below, where the word **it** could potentially refer to either **pizza** or **oven**, **Attention** could help a **Transformer** establish the correctly relationship between the word **it** and **pizza**.\n",
    "\n",
    "<img src=\"./images/attention_ex_1.png\" alt=\"an illustration of how attention works\" style=\"width: 800px;\"/>\n",
    "\n",
    "In this tutorial, you will...\n",
    "\n",
    "- **[Code a Basic Self-Attention Class!!!](#selfAttention)** The basic self-attention class allows the transformer to establish relationships among words and tokens.\n",
    "\n",
    "- **[Calculate Self-Attention Values!!!](#calculate)** We'll then use the class that we created, SelfAttention, to calculate self-attention values for some sample data.\n",
    " \n",
    "- **[Verify The Calculations!!!](#validate)** Lastly, we'll validate the calculations made by the SelfAttention class..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115343f5-36c3-4359-9f34-bdbc000431ee",
   "metadata": {},
   "source": [
    "# Import the modules that will do all the work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73f2de42-7cd5-4476-a5ea-00c7ea09e01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch ## torch let's us create tensors and also provides helper functions\n",
    "import torch.nn as nn ## torch.nn gives us nn.module() and nn.Linear()\n",
    "import torch.nn.functional as F # This gives us the softmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee83d72-c0fc-47b7-abf5-e4c5d2eefdb2",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6ff; padding:15px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\"> ðŸ’» &nbsp; <b>Access <code>requirements.txt</code> file:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Open\"</em>. For more help, please see the <em>\"Appendix - Tips and Help\"</em> Lesson.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182e21b5-d29f-47fa-80f3-beb8111e0f3e",
   "metadata": {},
   "source": [
    "# Code Self-Attention\n",
    "<a id=\"selfAttention\"></a>\n",
    "\n",
    "We will use `d_model` to define the size of weigths matrices that we'll use to create the Querries, Keys and Values. If `d_model=2` that means we have two embedding numbers per token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ae83c6-6088-4c64-a107-82461d520ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module): \n",
    "                            \n",
    "    def __init__(self, d_model=2,  \n",
    "                 row_dim=0, \n",
    "                 col_dim=1):\n",
    "        ## d_model = the number of embedding values per token.\n",
    "        ##           Because we want to be able to do the math by hand, we've\n",
    "        ##           the default value for d_model=2.\n",
    "        ##           However, in \"Attention Is All You Need\" d_model=512\n",
    "        ##\n",
    "        ## row_dim, col_dim = the indices we should use to access rows or columns\n",
    "\n",
    "        \n",
    "        super().__init__() # call the parent's __init__ method otherwise there is no point in inheriti from a class to begin with.\n",
    "        \n",
    "        ## Initialize the Weights (W) that we'll use to create the\n",
    "        ## query (q), key (k) and value (v) for each token\n",
    "        ## NOTE: A lot of implementations include bias terms when\n",
    "        ##       creating the the queries, keys, and values, but\n",
    "        ##       the original manuscript that described Attention,\n",
    "        ##       \"Attention Is All You Need\" did not, so we won't either\n",
    "        self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False) # create weight matrix\n",
    "        self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        \n",
    "        self.row_dim = row_dim\n",
    "        self.col_dim = col_dim\n",
    "\n",
    "    # the forward methos where we actually calculate the self-attention values for each tokens.\n",
    "    def forward(self, token_encodings):\n",
    "        ## Create the query, key and values using the encoding numbers\n",
    "        ## associated with each token (token encodings)\n",
    "        q = self.W_q(token_encodings)\n",
    "        k = self.W_k(token_encodings)\n",
    "        v = self.W_v(token_encodings)\n",
    "\n",
    "        ## Compute similarities scores: (q * k^T)\n",
    "        sims = torch.matmul(q, k.transpose(dim0=self.row_dim, dim1=self.col_dim))\n",
    "\n",
    "        ## Scale the similarities by dividing by sqrt(k.col_dim)\n",
    "        scaled_sims = sims / torch.tensor(k.size(self.col_dim)**0.5)\n",
    "\n",
    "        ## Apply softmax to determine what percent of each tokens' value to\n",
    "        ## use in the final attention values.\n",
    "        attention_percents = F.softmax(scaled_sims, dim=self.col_dim)\n",
    "\n",
    "        ## Scale the values by their associated percentages and add them up.\n",
    "        attention_scores = torch.matmul(attention_percents, v)\n",
    "\n",
    "        return attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ced138-a328-4844-87c5-5e6ea463c308",
   "metadata": {},
   "source": [
    "# Calculate Self-Attention\n",
    "<a id=\"calculate\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba1d49e-c7ef-4693-95d9-972929657b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a matrix of token encodings...\n",
    "encodings_matrix = torch.tensor([[1.16, 0.23],\n",
    "                                 [0.57, 1.36],\n",
    "                                 [4.41, -2.16]])\n",
    "\n",
    "## set the seed for the random number generator\n",
    "torch.manual_seed(42)\n",
    "\n",
    "## create a basic self-attention ojbect\n",
    "selfAttention = SelfAttention(d_model=2,\n",
    "                               row_dim=0,\n",
    "                               col_dim=1)\n",
    "\n",
    "## calculate basic attention for the token encodings\n",
    "selfAttention(encodings_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d64c92f-2e43-45d8-ba33-df4a4d248998",
   "metadata": {},
   "source": [
    "# Print Out Weights and Verify Calculations\n",
    "<a id=\"validate\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00bb8a8-8656-44bc-b560-34edb5756ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## print out the weight matrix that creates the queries\n",
    "selfAttention.W_q.weight.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bff6c4-5832-4653-951f-31224077d394",
   "metadata": {},
   "outputs": [],
   "source": [
    "## print out the weight matrix that creates the keys\n",
    "selfAttention.W_k.weight.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e88a435-5514-4054-b4e1-b6c0a2ae4cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## print out the weight matrix that creates the values\n",
    "selfAttention.W_v.weight.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6c83e5-1975-466e-909a-dc0782a70f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate the queries\n",
    "selfAttention.W_q(encodings_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714089d0-d5c6-49e2-8d83-e671d0686aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate the keys\n",
    "selfAttention.W_k(encodings_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbd4522-63f4-48a7-b7b2-b8e37374ded6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate the values\n",
    "selfAttention.W_v(encodings_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f565e705-438f-412a-be66-1f5f03339c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = selfAttention.W_q(encodings_matrix)\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed14f92-af9f-4273-b74e-efed3d0e26e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = selfAttention.W_k(encodings_matrix)\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff373ee-eb5a-4476-a645-1fd65eced76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = torch.matmul(q, k.transpose(dim0=0, dim1=1))\n",
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce981341-b6ed-4033-a526-7226bdf7d728",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_sims = sims / (torch.tensor(2)**0.5)\n",
    "scaled_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988a1889-ba00-48ba-9d00-94658f46b3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_percents = F.softmax(scaled_sims, dim=1)\n",
    "attention_percents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2757ed41-6109-4001-9455-d7fe37674f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.matmul(attention_percents, selfAttention.W_v(encodings_matrix))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
