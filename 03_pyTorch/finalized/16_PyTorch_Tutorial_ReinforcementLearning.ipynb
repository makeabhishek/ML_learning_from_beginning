{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "704bebd8-c9ea-48a3-ab9b-c78c02a64b1a",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=w0OZ5iHsamk&list=PLM0a6Z788YAZdVDOpK2rAkvLTkcydhENy&index=6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb24389-fbd4-40d8-a2da-ebed234b99e6",
   "metadata": {},
   "source": [
    "# Reinforcement Learning (Reinforcement learning + deep learning)\n",
    "\n",
    "Instead of deep learning the models that we train in deep learning being shown fixed data sets that don't change over time and superimposing them teaching them based on these data sets we're now going to look into problems that involve the Deep learning model exploring and interacting with the data sets in a dynamic way so I can learn how to improve based on these scenarios and these environments that they're being placed in dynamically and evolve over time and the goal of course is to do this entirely without any human supervision so previously our data sets\n",
    "\n",
    "<center><img src='./images/reinforceMIT_1.PNG' width=550px></center> \n",
    "In Reinforcement learning, data not in the form of inputs and labels but you're going to be shown data in the form of __states__ and __actions__ and these are going to be paired pieces of data now _states are the observations_ of your agent and the actions are the decisions that that agent takes when it's sees itself in a certain state so the goal of reinforcement learning said very simply is to create an agent to create a model that can learn how to maximize the future rewards that it obtains right over many time steps into the future so this is again a completely new paradigm of learning problems that we've seen\n",
    "\n",
    "The __goal__ of reinforcement learning said very simply is to create an agent to create a model that can learn how to maximize the future rewards that it obtains right over many time steps into the future \n",
    "\n",
    "## Reinforcement learning\n",
    "<center><img src='./images/reinforceMIT_2.PNG' width=550px></center> \n",
    "\n",
    "- Agent : an agent is uh something that can take actions right so for example in uh in let's say autonomous delivery of packages a drone would be an agent okay in Super Mario games Super Mario would be the agent. the algorithm itself is the agent\n",
    "- the environment is simply the world in which the agent lives and it can take actions in and the agent can send commands to its environment in the form of these quote actions uh and we can also denote just for formality let's call a the set of all possible actions that this agent could possibly take so in a very simplified world we could say that this agent let's say it's in a two-dimensional World it can move\n",
    "- observations are simply how the environment interacts back to the agent so observations are states essentially that the environment s and shows to the agent and how the agent observes the world\n",
    "- reward so in addition to providing a state from environment to action the environment will also provide a reward so a reward is simply the feedback which we can measure or which the environment provides to measure the success or the failures or the penalties of the agent in that time step. Examples so in a video game when Mario touches a gold coin you know the points go up right so he gets a positive reward right\n",
    "    - so rewards can be both immediate in the sense of the gold coin Right Touch the gold coin you get an immediate reward but they can also be delayed and that's a very important concept so you may take some actions that result in a reward much later on but they were critical actions that you took at this time step and the reward was delayed and you don't recognize that reward until much later it's still a reward and that's a very important concept so we can now look at not only the reward at one time step which is rft but we can also look at the total reward\n",
    "which is simply you can think of the sum of all all rewards up until that time\n",
    "\n",
    "- Discount: on so often it's useful to not only consider the reward at time T right or the sum of all rewards up until that time but also to consider the discounted. What's called the discounted reward over time okay so what does that mean so the discount Factor which here is denoted as this gamma term okay so it's a gam it's a typically a fixed term so you have one discounting factor in your environment and your environment provides that typically you can think of the discounting factor as a factor that will\n",
    "\n",
    "Discounted Total Reward or return of the agent \n",
    "<center><img src='./images/discountFacor.PNG' width=550px></center> \n",
    "\n",
    "\n",
    "- discounting factor as a factor that will dampen the effects of a reward over time okay so why would you want to do this essentially a discounting factor is designed such that it will make immediate or let me say it will make future rewards much less uh worth much less than immediate rewards okay so what's a what's an example of kind of a uh an example where you have this enforcing of a short-term learning on an agent right so for example if I was to offer you a reward of \\$5 today or a reward of \\$5 in one year it's still a reward of $5 but you have an implicit discounting factor which allows you to prioritize that reward of \\$5 today over the reward of \\$5 in one yyear time\n",
    "- you multiply it by the future Awards as discovered by the agent in order to dampen all of the the future of rewards that the agent sees now again this is just meant to make it such that the future rewards are less worth less than any of the immediate rewards that it's\n",
    "\n",
    "## Q-function\n",
    "<center><img src='./images/q_function.PNG' width=550px></center>  \n",
    " the Q function now is a function that can take as input the state the current state that you're in and a possible action that you take from\n",
    "this state and it will try to return the expected total future reward or the the the return of that agent that can be received from that time point up until the Future Okay so let's think about that a little bit more just digested so given a state that you're in and an action that you take from that state the Q function will tell you what is the the expected amount of reward that you will get from that point on if you take that action right now if you change the action AF of T in the current state your Q function will say that okay this is actually going to retur\n",
    "\n",
    "<center><img src='./images/q_function_2.PNG' width=550px></center> \n",
    "-keep maximizing the Q function right so what actions would you take you would take the ones that would a maximize the Q function right so at every time Point ultimately what you have to recognize as the agent's point of view right is at every time step you want to create some policy right so what we're thinking of here is now this policy function which is slightly different than the Q function so the policy function will denote it with a pi it only takes us input s and it's going to say what is the optimal action that I should take in this state so we can try to evaluate valate the policy function using the Q function right and as as was stated this can be\n",
    "done just by trying to choose an action which maximizes the future return of\n",
    "\n",
    "so the Q function the sorry the optimal policy function Pi how\n",
    "to act in a current state s is simply going to be defined by taking the action\n",
    "which gives you the highest Q value right so you can evaluate all possible actions from your Q function and pick\n",
    "the action that gives you the highest return or the highest future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab2daad-d053-4ff8-9064-34d2ffc499ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
