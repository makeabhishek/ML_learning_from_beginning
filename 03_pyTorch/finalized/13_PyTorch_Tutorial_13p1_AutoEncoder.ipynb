{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58f7b040-7bc1-427c-8523-5b5109f9ede6",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=zp8clK9yCro\n",
    "\n",
    "The idea behind autoencoders are simple. W ehave an input image and we encode this image into low dimensional embedding of the image and than decode it to reconstruct original image as good as possible.\n",
    "\n",
    "<center><img src='./images/autoencoder.PNG' width=550px></center> \n",
    "\n",
    "An example for this is video compression where we want to send the images over the network from one end to other end. So instead of sending full image we can send encoded data only. And on the other side we than have the decoder which can decode the iamge. This will save a lot of cost and could be efficient.\n",
    "<center><img src='./images/autoencoder_1.PNG' width=550px></center> \n",
    "\n",
    "### Giw do we encode and decode image.\n",
    "For both operation we can simply use feed forward neural net or when we deal with images, we often use CNN.\n",
    "\n",
    "- When we speak about such a model it is called generative model. Because here instead of doing a classification at the end we want to generate images based on encoding.\n",
    "- In order to train our model we need loss funciton that we want to optimize . We want our reconstructed iamge as claose as original image. All pixel value will almost be same. therfore, our loss functon is simple an MSE, whcih calculate the error for each pixel.\n",
    "- There is a tric, whcih helps to understand whole process better. instead of thinking of all transformations as operations in one way. We can think it as a circular or forth and back operation. We encode in one direction and than go in other direction to decode it.\n",
    "- so for each transformation we apply in the encoder we want to apply the inverse of this operation in the decoder. For example if we apply a linear layer that reduces the size the reverse operation is also a linear layer that increases the size again\n",
    "- Now for cnns this is a little bit trickier here we apply convolutional layers so  the inverse is actually called a __transpose convolution__.\n",
    "\n",
    "### `nn.convTranspose2d`\n",
    "- PyTorch has this layer already included for us so we can simply use the `nn.convTranspose2d` layer the only tricky thing with this is to determine the correct input and output shapes \n",
    "-\n",
    "- <center><img src='./images/autoencoder_2.PNG' width=550px></center> \n",
    "- <center><img src='./images/autoencoder_3.PNG' width=550px></center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-canyon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-metallic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST data\n",
    "\n",
    "# normalization if the image values are not normalized.\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5), (0.5))\n",
    "    ])\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Download data\n",
    "mnist_data = datasets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            download=True, \n",
    "                            transform=transform)\n",
    "\n",
    "# load data\n",
    "data_loader = torch.utils.data.DataLoader(dataset=mnist_data,\n",
    "                                          batch_size=64,\n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-birth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets check the first batch\n",
    "dataiter = iter(data_loader)\n",
    "images, labels = dataiter.next() # call the iamges using dataiter.next()\n",
    "print(torch.min(images), torch.max(images))\n",
    "# >>> tensor(0.) tensor(1.) # we can see that values are between 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f516bf-030b-402f-b936-08fee3ad0e15",
   "metadata": {},
   "source": [
    "# Autoencoder with simple feedforwatrd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-charlotte",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple feed forward network with linear layer and repeatedly reduce the size\n",
    "class Autoencoder_Linear(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 128), # (N, 784) -> (N, 128) N=batch size, initial size of image is 28*28=784. image size is reduced from 784 to 128\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 12),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(12, 3) # -> N, 3 output = 3. So size of image is reduced from 784 to 3. We dont need activation fucniton in the last layer.\n",
    "        )\n",
    "\n",
    "        # In decoder we go in opposite direction\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(3, 12),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(12, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 28 * 28), #N,3 -> N,784\n",
    "            nn.Sigmoid() # we apply ativation funtion. In the starting we checked the value in image and we know that the value si in between 0 and 1.\n",
    "            # Now we need activation fucntion whcih puts value beween 0 and. So we use Sigmoid funciton.\n",
    "        )\n",
    "\n",
    "    # Apply encoder-decoer in forward pass.\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "NOTE: \n",
    "# Input image is in range of  [-1, +1] -> use `nn.Tanh` instead of sigmoid. to transform we  defined transform above.\n",
    "# transform = transforms.Compose(\n",
    "#     [transforms.ToTensor(),\n",
    "#      transforms.Normalize((0.5), (0.5))\n",
    "#     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b42823-6ff1-4383-ac6e-8d5010c811f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder_Linear()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), # optimize the parameters of our model.\n",
    "                             lr=1e-3, \n",
    "                             weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-ceremony",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([32, 3])\n",
      "Epoch:1, Loss:0.0484\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "# Point to training loop video\n",
    "num_epochs = 10\n",
    "outputs = [] # create alist to storre output\n",
    "for epoch in range(num_epochs): # iterate over epochs\n",
    "    for (img, _) in data_loader: # iterate over batch or dataloader.\n",
    "        img = img.reshape(-1, 28*28) # -> use for Autoencoder_Linear\n",
    "        recon = model(img)\n",
    "        loss = criterion(recon, img)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch:{epoch+1}, Loss:{loss.item():.4f}')\n",
    "    outputs.append((epoch, img, recon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerous-greensboro",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the iamges to check how good its reconstructued\n",
    "for k in range(0, num_epochs, 4):\n",
    "    plt.figure(figsize=(9, 2))\n",
    "    plt.gray()\n",
    "    imgs = outputs[k][1].detach().numpy() # as output is torch tensor, we convert to numpy using detach\n",
    "    recon = outputs[k][2].detach().numpy()\n",
    "    for i, item in enumerate(imgs):\n",
    "        if i >= 9: break\n",
    "        plt.subplot(2, 9, i+1)\n",
    "        item = item.reshape(-1, 28,28) # -> use for Autoencoder_Linear\n",
    "        # item: 1, 28, 28\n",
    "        plt.imshow(item[0])\n",
    "            \n",
    "    for i, item in enumerate(recon):\n",
    "        if i >= 9: break\n",
    "        plt.subplot(2, 9, 9+i+1) # row_length + i + 1\n",
    "        item = item.reshape(-1, 28,28) # -> use for Autoencoder_Linear\n",
    "        # item: 1, 28, 28\n",
    "        plt.imshow(item[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ac0a1c-2c92-463e-8287-05094b13d04d",
   "metadata": {},
   "source": [
    "# Autoencoder using conv2D\n",
    "Lets see if we can use convolutional neural network and improve the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swedish-robin",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()        \n",
    "        # N, 1, 28, 28\n",
    "        self.encoder = nn.Sequential(\n",
    "            # instead of linear layer we use conv2D layer\n",
    "            nn.Conv2d(1, 16, 3, stride=2, padding=1), #  (inputchannel, out_channel, kerenelsize, stride , padding)). # -> N, 16, 14, 14. We reduced the size of image by 50% from 28 to 14\n",
    "            nn.ReLU(),\n",
    "            # becasreful to get correct shape and size.\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1), # -> N, 32, 7, 7\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 7) # -> N, 64, 1, 1  16 channels and output image is is 1x1\n",
    "            \n",
    "        )\n",
    "\n",
    "        # Go in backward diirection using convTranspose2D layer.\n",
    "        # N , 64, 1, 1 We have this size from encoder.\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, 7), # -> N, 32, 7, 7\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1), # N, 16, 14, 14 (N,16,13,13 without output_padding)\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1), # N, 1, 28, 28  (N,1,27,27 without output_padding)\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "\n",
    "# Note: If you want to use pooling with CNN. nn.MaxPool2d -> use nn.MaxUnpool2d, or use different kernelsize, stride etc to compensate...\n",
    "# Input [-1, +1] -> use `nn.Tanh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-hours",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=1e-3, \n",
    "                             weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d96dea3-10d8-475a-a4d2-69a962b111f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "# Point to training loop video\n",
    "num_epochs = 10\n",
    "outputs = [] # create alist to storre output\n",
    "for epoch in range(num_epochs): # iterate over epochs\n",
    "    for (img, _) in data_loader: # iterate over batch or dataloader.\n",
    "        # img = img.reshape(-1, 28*28) # -> use for Autoencoder_Linear. No need as our intial image size is 2D\n",
    "        recon = model(img)\n",
    "        loss = criterion(recon, img)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch:{epoch+1}, Loss:{loss.item():.4f}')\n",
    "    outputs.append((epoch, img, recon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5813c232-2a82-4939-a1f2-84bd6997b9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(0, num_epochs, 4):\n",
    "    plt.figure(figsize=(9, 2))\n",
    "    plt.gray()\n",
    "    imgs = outputs[k][1].detach().numpy()\n",
    "    recon = outputs[k][2].detach().numpy()\n",
    "    for i, item in enumerate(imgs):\n",
    "        if i >= 9: break\n",
    "        plt.subplot(2, 9, i+1)\n",
    "        # item = item.reshape(-1, 28,28) # -> use for Autoencoder_Linear\n",
    "        # item: 1, 28, 28\n",
    "        plt.imshow(item[0])\n",
    "            \n",
    "    for i, item in enumerate(recon):\n",
    "        if i >= 9: break\n",
    "        plt.subplot(2, 9, 9+i+1) # row_length + i + 1\n",
    "        # item = item.reshape(-1, 28,28) # -> use for Autoencoder_Linear\n",
    "        # item: 1, 28, 28\n",
    "        plt.imshow(item[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5b678d-65b0-4506-bfdf-ee1ffa7682e8",
   "metadata": {},
   "source": [
    "#### Homework: Use MaxPool2d, inspect the encoded data (can it be plotted as img?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7f102b-2fd4-474b-a934-3e3577bea75e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
