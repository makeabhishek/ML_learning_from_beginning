{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d52c5526-a94d-4d00-9cfa-e38ae68e9730",
   "metadata": {},
   "source": [
    "# Python + PyTorch + Pygame Reinforcement Learning – Train an AI to Play Snake\n",
    "Ref: https://www.youtube.com/watch?v=L8ypSXwyBds\n",
    "⭐️ Course Contents ⭐️\n",
    "- Part 1: Theory: Basics of Reinforcement Learning and Deep Q Learning\n",
    "- Part 2: Setup environment and implement snake game\n",
    "- Part 3: Implement agent to control game\n",
    "- Part 4: Create and train neural network\n",
    "\n",
    "RL is teaching a software __agent__ how to behave in an __environemnt__ by telling it how good it's doing.\n",
    "- We will use __Deep Q Learning__: This apporaoch extends reinforcement learning by using a deep NN to predict the actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec23c9c-6c87-41de-9c60-c944a82d77bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15ce5bc5-97b7-4972-bf99-37f6db1c1549",
   "metadata": {},
   "source": [
    "### Teach Neural Network to play snake game | Reinforcement Learning | Reward maximization | AI gameplay\n",
    "### each AI To Play Snake! Reinforcement Learning With PyTorch and Pygame\n",
    "\n",
    "snakeGame.PNG\n",
    "\n",
    "- Q-learning is a form of Reinforcement Learning and Q stands for quality.\n",
    "\n",
    "The snake game: a snake that grows each time it \"eats\" a randomly placed food. the score increases by 1 everytime it eats, and the game ends when the snake's head crashes int ots tail os the wall.\n",
    "\n",
    "- __3 Actions:__ Forward, left  or right\n",
    "- __What is not allowed__: No back, no waiting\n",
    "- __Game Over:__ Collison with walls or tails\n",
    "\n",
    "- QLearning is a reinformcement learning (RL) algorithm. Its objective is to maximize the reward an __\"agent\"__ and the __\"learner\"__\n",
    "- What is agent and learning: Training a dog. You revward and punish. When you give your dog a cookiee for learning to sit, the cookiee is the reward, you are the agent, and dog is the learner.\n",
    "- Similarly,  in snake game also we can do something so you are the agent and dog is the learner if you are considering a dog and it's trainer on the other hand here you will define an agent and you will Define the learner and the learner will be a basically a deep learning algorithm or a neural network.\n",
    "\n",
    "RLdog.PNG\n",
    "\n",
    "### Steps involved in RL\n",
    "1. Environment observation:  if you have the snake and the food and the walls. You make an observation on what is the current state of the environment then based on the current state of the environment you you decide what action to take whether to move forward left or right just like how a human plays the game .\n",
    "2. Deliberating which action to take: So when you are playing The Snake Game fast you know inadvertent advertently you are basically taking these steps\n",
    "3. Executing such action: then executing that action after after deciding all of this happens in a millisecond inside our head\n",
    "4. Receiveing the reward or penalty: receiving reward or penalty or nothing if the snake has not hit the wall or the food or itself there is neither reward nor nor penalty. If it hits itself or it hit if it hits the wall if the walls are rigid the snake it's game over. so it's a penalty if the snake gets the food it's a reward.\n",
    "5. Learn from experience:  From this experience you can play multiple games and that's how as a kid you became good at the snake game and this is how the the ML model will also become good at the snake game\n",
    "6. Iterate\n",
    "\n",
    "## Qlearning overview\n",
    "- The learner maps how valuable is an action $A$, given a state $S: Q(A,S)$. \n",
    "- QLearning calculates expresses the relationship between Q of state 1 (S1), and Q of the follwoing sate 2 (S2), given a chosen action (A) for the state 1. So we can define a quality for a given state pair.\n",
    "- The environment had some state when the learner executed an action (snake moves, the environemnt changed), resulting in a chane in the environment and maybe a __reward__ obtained. The learner will use the initila state, the action, the reward, and the final state to learn how to properly choose actions that maximizes the rewards that agent grants. Reward can be poitive, negative or neutral.\n",
    "\n",
    "$$New Q(S,A) = Q(S,A) + \\alpha[R(S,A) + \\gamma max Q(S',A') - Q(S,A)]$$\n",
    "\n",
    "where, $Q(S,A)$ is the curent Q-value, $R(S,A)$ is the reward, $max Q(S',A')$ is the maximum expected future reward, $\\alpha$ is learning rate, $\\gamma$ is the discount rate. Discount rate means it means um the value of getting this reward in the future is uh less compared to getting a reward in the present so that's why we have a discount rate and Alpha is the learning rate.\n",
    "\n",
    "We can define state, everthing which is available in the environemnt.\n",
    "- State: State means all the different things that exist at a at any given point of time the position of the snake where is it headed uh its location the location of the food and the general direction like what is top what is bottom what is right and what is left and how far is the snake from the wall if it takes one step forward will it hit the wall is it dangerous Etc so the this information can be uh included in a variable called State and State can be represented by this symbol $S$.\n",
    "- Based on the state you can also take an action action is Den Ed by a the action is very simple you have only three actions in this case you can either move forward move left or move right and when I say forward it it it is with respect to the current direction of motion of the snake so if the current direction of motion is uh downwards that means the forward for the snake is downward itself and left means to this direction and right means to this direction right according to the snake.\n",
    "\n",
    "### State of the game\n",
    "snakeState.png\n",
    "\n",
    "Define the state of the game at each step as shown below.\n",
    "1) Collision straight? FALSE\n",
    "2) Collision right? FALSE\n",
    "3) Collision left? FALSE\n",
    "4) Direction West? FALSE\n",
    "5) Direction East? FALSE\n",
    "6) Direction North? FALSE\n",
    "7) Direction South? __TRUE__\n",
    "8) Food Westwards?FALSE\n",
    "9) Food Eastwards? __TRUE__\n",
    "10) Food Northwards?FALSE\n",
    "11) Food Southwards? __TRUE__\n",
    "\n",
    "we can see the image where we generated the vector for the state.\n",
    "\n",
    "### New State and reward\n",
    "Given the current state, the model (for example NN) makes a prediction. this prediction involves calculating the expected reward for each possible action (left, righ, forward). The model (NN) outputs a vector of thre values, each corresponding to one of these actions. In exploration stage we will not pick these values but in exploitation state we will pick maximum reward.\n",
    "\n",
    "after taking an action when you go from State One to to state two the question is as a result of this action was the food eaten if the answer is yes let's say we can increase the reward by 10 if the answer is no there is one more question was there a collision because Collision means the snake is dead right if there was a Collis if the answer is yes we will Define a negative reward minus 10 if there was no Calis meaning the snake just went in in that particular de ction then it's overall a neutral movement nothing happened as a result of this this particular action so we defined the reward to be zero so uh this is reward and not the Q factor,  quality factor is different from the reward right reward is uh fixed meaning. If the food was eaten this is the extra reward it will get if the if the snake was dead this is the reward otherwise zero but the Q factor will depend on the state and the action and that there are so many different states possible so there are so many different values of Q factor also possible.\n",
    "\n",
    "### The model\n",
    "- __Role of the model:__ learns to predict expected rewards (Q-values) for actions given the current game state.\n",
    "- __Function:__ The model predicts the best action to take in any given state and is trained to improve these predictions over time by learning from the past experinece. Basically NN is exposed to differnt states and its getting improved after learning more.\n",
    "- __output:__ A vector of Q-values representing the expected rewards for each possible actions. Here we have three actions possible, It can be move left, move right, move forarf. Example Output:  [0.12, 0.224, -0.189]\n",
    "- Model will predict Q-value\n",
    "\n",
    "snake_model.png\n",
    "\n",
    "### The agent \n",
    "Agent is an intermediary b/w the game loop and the model\n",
    "- __Functions of the Agent:__\n",
    "    - 1. Actions Selection: Choose actions based on models predited Q-value (exploration vs exploitation). Agent decides the action so model will predict Q value it does not mean that the highest Q value whichever whichever action has the highest Q value it is selected. No there is no such uh no such thing the mod the agent can decide among these three Q values that uh correspond responding to three actions that the model has predicted which which action to pick. if the agent is kind of in an exploration uh mood agent will not necessarily pick the one with the highest Q value it may pick some other random uh one which may have the lowest Q value um whereas if it's if the agent is in the exploitation stage model the agent might decide to pick the one with the highest Q value.\n",
    "    - 2. State Update: Update game stet after each action\n",
    "    - 3. Training: Uses experiences to update the models parameters via Q-learning. The model has weights bias uh anything else activation function is already set but basically the training has to be done so that you are modifying the weights and bias so that in the later iterations if the similar state is coming or the model is experiencing a similar state it can make a better prediction.\n",
    "     \n",
    "- __Responsibilities of the Agent:__\n",
    "    1. State Representation: converts the game state into a vector\n",
    "    2. Reward Calcualtion: e.g. +10 for eating food, -10 for collison\n",
    "    3. Data Management: Stores experiences (state, action, reward, next state)  for training\n",
    "    4. Learning Control: Instructs the model to learn from stored experiences.\n",
    "    \n",
    "- __Interation Flow:__\n",
    "    1. Get State: Capture current game state\n",
    "    2. Choose Action: Based on model's Q-value\n",
    "    3. Execute Action: Update game and get new state\n",
    "    4. Calculate Reward: Evaluate the outcome\n",
    "    5. Store Experience: Record state, action, reward, next state\n",
    "    6. Train Model: Update model using stored experiences.\n",
    "\n",
    "## Q value vs Reward\n",
    "\n",
    "- Reward\n",
    "    - Immediate and specific feedback\n",
    "    - given instantly ater an action\n",
    "    - reflects short-term benefit or penalty\n",
    "\n",
    "- Q-value\n",
    "    -  Expected long term value\n",
    "    -  Guides decison making for maximizing cumulative reward\n",
    "    -  Incorporates both immediate rewards and expected future rewards via the __Bellman eqaution.__\n",
    "We are tryiing to increase cummulatinve reward (Q-value), not trying to improve instant reward (Reward).\n",
    "\n",
    "## Exploraton vs Exploitation\n",
    "In early stages of training, we want the model to chosse random actions, to genearte many diffenret data recors that can be learnned from. That is Exploration. As the number of games progress, we allow the model to use what it learned, little by little: giving it some autonomy. This is Exploitation: exploitation of experience. Eventually, there is no more exploration (randomly choosing actions), and all actions selection is trueted to the mdoel.\n",
    "\n",
    "exploration meaning the agent will not just focus on the uh Q values that are output by the model agent may take random decisions but eventually as the number of games played by the um the learner increases agent will more and more and more decide that okay no more exploration meaning no more random actions I will take actions that are very good very well rewarding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6013086c-4492-45ef-bb59-859b22f2b398",
   "metadata": {},
   "source": [
    "### Snake game: \n",
    "After a few games he will understand what to do where exactly to take the snake.\n",
    "\n",
    "You can train a neural network exactly this way to play the snake game or any game using QLearning from Reinforcement Learning.\n",
    "\n",
    "1) Collision straight? FALSE\n",
    "2) Collision right? FALSE\n",
    "3) Collision left? FALSE\n",
    "4) Direction West? FALSE\n",
    "5) Direction East? FALSE\n",
    "6) Direction North? FALSE\n",
    "7) Direction South? __TRUE__\n",
    "8) Food Westwards?FALSE\n",
    "9) Food Eastwards? __TRUE__\n",
    "10) Food Northwards?FALSE\n",
    "11) Food Southwards? __TRUE__\n",
    "\n",
    "Then modify the state by taking an action: Move forward, left or right.\n",
    "\n",
    "There are two kinds rewards: short term and long term. \n",
    "\n",
    "If the food is touching the wall and if you are close to the food, moving perpendicular to the wall, there is immediate reward if you move straight toward the food, but you may collide with the wall in the next step.\n",
    "\n",
    "But if you are parallel to the wall and food is in front of you then immediate reward does not pose imminent danger.\n",
    "\n",
    "Short term reward makes the snake eat the food in a given moment. Long term reward makes the snake maximize the personal best score.\n",
    "\n",
    "An interesting thing to note is exploration vs exploitation. In the first few games snake will make random movements. This will result in many collisions and a lot of negative reward. Thus, it may learn that it is better to avoid collisions and earn 0 reward by just moving in circles.\n",
    "\n",
    "Once the snake plays 75-80 games there is not much exploration. At each step the snake will take the step proposed by the neural network and will move exactly toward the food. In fact the snake will play better than you even if you are a pro at the game.\n",
    "\n",
    "In this lecture I published on Vizuara’s YouTube channel, you will learn how to implement this. I also share the GitHub repo with the code. Please feel free to try this yourself. \n",
    "\n",
    "This approach can be modified to teach AI play any game. All we need to define is the state at any moment and actions to be taken and a reward/punishment system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65db769e-785e-4293-9e32-169f06a6459b",
   "metadata": {},
   "source": [
    "# Code in PyTorch\n",
    "\n",
    "Run this usign IDE, jupyternotebook and colab may not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ef8649-d256-4ed2-84dd-cd28bc8d6b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# snake_game_human.py\n",
    "\n",
    "import pygame\n",
    "import random\n",
    "from enum import Enum\n",
    "from collections import namedtuple\n",
    "\n",
    "pygame.init()\n",
    "font = pygame.font.Font('arial.ttf', 25)\n",
    "#font = pygame.font.SysFont('arial', 25)\n",
    "\n",
    "class Direction(Enum):\n",
    "    RIGHT = 1\n",
    "    LEFT = 2\n",
    "    UP = 3\n",
    "    DOWN = 4\n",
    "    \n",
    "Point = namedtuple('Point', 'x, y')\n",
    "\n",
    "# rgb colors\n",
    "WHITE = (255, 255, 255)\n",
    "RED = (200,0,0)\n",
    "BLUE1 = (0, 0, 255)\n",
    "BLUE2 = (0, 100, 255)\n",
    "BLACK = (0,0,0)\n",
    "\n",
    "BLOCK_SIZE = 20\n",
    "SPEED = 20\n",
    "\n",
    "class SnakeGame:\n",
    "    \n",
    "    def __init__(self, w=640, h=480):\n",
    "        self.w = w\n",
    "        self.h = h\n",
    "        # init display\n",
    "        self.display = pygame.display.set_mode((self.w, self.h))\n",
    "        pygame.display.set_caption('Snake')\n",
    "        self.clock = pygame.time.Clock()\n",
    "        \n",
    "        # init game state\n",
    "        self.direction = Direction.RIGHT\n",
    "        \n",
    "        self.head = Point(self.w/2, self.h/2)\n",
    "        self.snake = [self.head, \n",
    "                      Point(self.head.x-BLOCK_SIZE, self.head.y),\n",
    "                      Point(self.head.x-(2*BLOCK_SIZE), self.head.y)]\n",
    "        \n",
    "        self.score = 0\n",
    "        self.food = None\n",
    "        self._place_food()\n",
    "        \n",
    "    def _place_food(self):\n",
    "        x = random.randint(0, (self.w-BLOCK_SIZE )//BLOCK_SIZE )*BLOCK_SIZE \n",
    "        y = random.randint(0, (self.h-BLOCK_SIZE )//BLOCK_SIZE )*BLOCK_SIZE\n",
    "        self.food = Point(x, y)\n",
    "        if self.food in self.snake:\n",
    "            self._place_food()\n",
    "        \n",
    "    def play_step(self):\n",
    "        # 1. collect user input\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                quit()\n",
    "            if event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_LEFT:\n",
    "                    self.direction = Direction.LEFT\n",
    "                elif event.key == pygame.K_RIGHT:\n",
    "                    self.direction = Direction.RIGHT\n",
    "                elif event.key == pygame.K_UP:\n",
    "                    self.direction = Direction.UP\n",
    "                elif event.key == pygame.K_DOWN:\n",
    "                    self.direction = Direction.DOWN\n",
    "        \n",
    "        # 2. move\n",
    "        self._move(self.direction) # update the head\n",
    "        self.snake.insert(0, self.head)\n",
    "        \n",
    "        # 3. check if game over\n",
    "        game_over = False\n",
    "        if self._is_collision():\n",
    "            game_over = True\n",
    "            return game_over, self.score\n",
    "            \n",
    "        # 4. place new food or just move\n",
    "        if self.head == self.food:\n",
    "            self.score += 1\n",
    "            self._place_food()\n",
    "        else:\n",
    "            self.snake.pop()\n",
    "        \n",
    "        # 5. update ui and clock\n",
    "        self._update_ui()\n",
    "        self.clock.tick(SPEED)\n",
    "        # 6. return game over and score\n",
    "        return game_over, self.score\n",
    "    \n",
    "    def _is_collision(self):\n",
    "        # hits boundary\n",
    "        if self.head.x > self.w - BLOCK_SIZE or self.head.x < 0 or self.head.y > self.h - BLOCK_SIZE or self.head.y < 0:\n",
    "            return True\n",
    "        # hits itself\n",
    "        if self.head in self.snake[1:]:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "        \n",
    "    def _update_ui(self):\n",
    "        self.display.fill(BLACK)\n",
    "        \n",
    "        for pt in self.snake:\n",
    "            pygame.draw.rect(self.display, BLUE1, pygame.Rect(pt.x, pt.y, BLOCK_SIZE, BLOCK_SIZE))\n",
    "            pygame.draw.rect(self.display, BLUE2, pygame.Rect(pt.x+4, pt.y+4, 12, 12))\n",
    "            \n",
    "        pygame.draw.rect(self.display, RED, pygame.Rect(self.food.x, self.food.y, BLOCK_SIZE, BLOCK_SIZE))\n",
    "        \n",
    "        text = font.render(\"Score: \" + str(self.score), True, WHITE)\n",
    "        self.display.blit(text, [0, 0])\n",
    "        pygame.display.flip()\n",
    "        \n",
    "    def _move(self, direction):\n",
    "        x = self.head.x\n",
    "        y = self.head.y\n",
    "        if direction == Direction.RIGHT:\n",
    "            x += BLOCK_SIZE\n",
    "        elif direction == Direction.LEFT:\n",
    "            x -= BLOCK_SIZE\n",
    "        elif direction == Direction.DOWN:\n",
    "            y += BLOCK_SIZE\n",
    "        elif direction == Direction.UP:\n",
    "            y -= BLOCK_SIZE\n",
    "            \n",
    "        self.head = Point(x, y)\n",
    "            \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    game = SnakeGame()\n",
    "    \n",
    "    # game loop\n",
    "    while True:\n",
    "        game_over, score = game.play_step()\n",
    "        \n",
    "        if game_over == True:\n",
    "            break\n",
    "        \n",
    "    print('Final Score', score)\n",
    "        \n",
    "        \n",
    "    pygame.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fd902e-d84e-4295-bbad-bc68879a936c",
   "metadata": {},
   "source": [
    "## model.py\n",
    "Definition of Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2871d7-428d-45c5-9bd8-9123aa5e6733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "class Linear_QNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "    def save(self, file_name='model.pth'):\n",
    "        model_folder_path = './model'\n",
    "        if not os.path.exists(model_folder_path):\n",
    "            os.makedirs(model_folder_path)\n",
    "\n",
    "        file_name = os.path.join(model_folder_path, file_name)\n",
    "        torch.save(self.state_dict(), file_name)\n",
    "\n",
    "\n",
    "class QTrainer:\n",
    "    def __init__(self, model, lr, gamma):\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.model = model\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=self.lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def train_step(self, state, action, reward, next_state, done):\n",
    "        state = torch.tensor(state, dtype=torch.float)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float)\n",
    "        action = torch.tensor(action, dtype=torch.long)\n",
    "        reward = torch.tensor(reward, dtype=torch.float)\n",
    "        # (n, x)\n",
    "\n",
    "        if len(state.shape) == 1:\n",
    "            # (1, x)\n",
    "            state = torch.unsqueeze(state, 0)\n",
    "            next_state = torch.unsqueeze(next_state, 0)\n",
    "            action = torch.unsqueeze(action, 0)\n",
    "            reward = torch.unsqueeze(reward, 0)\n",
    "            done = (done, )\n",
    "\n",
    "        # 1: predicted Q values with current state\n",
    "        pred = self.model(state)\n",
    "\n",
    "        target = pred.clone()\n",
    "        for idx in range(len(done)):\n",
    "            Q_new = reward[idx]\n",
    "            if not done[idx]:\n",
    "                Q_new = reward[idx] + self.gamma * torch.max(self.model(next_state[idx]))\n",
    "\n",
    "            target[idx][torch.argmax(action[idx]).item()] = Q_new\n",
    "    \n",
    "        # 2: Q_new = r + y * max(next_predicted Q value) -> only do this if not done\n",
    "        # pred.clone()\n",
    "        # preds[argmax(action)] = Q_new\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.criterion(target, pred)\n",
    "        loss.backward()\n",
    "\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a93692f-80c6-408d-8d6e-f66dae77f922",
   "metadata": {},
   "source": [
    "## helper.py\n",
    "For plotting the evolution of the score. The maximum score, as the game is learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c445c80-8add-4889-aa31-65b65e6d14ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "def plot(scores, mean_scores):\n",
    "    display.clear_output(wait=True)\n",
    "    # display.display(plt.gcf())\n",
    "    # plt.clf()\n",
    "    # plt.title('Training...')\n",
    "    # plt.xlabel('Number of Games')\n",
    "    # plt.ylabel('Score')\n",
    "    # plt.plot(scores)\n",
    "    # plt.plot(mean_scores)\n",
    "    # plt.ylim(ymin=0)\n",
    "    # plt.text(len(scores)-1, scores[-1], str(scores[-1]))\n",
    "    # plt.text(len(mean_scores)-1, mean_scores[-1], str(mean_scores[-1]))\n",
    "    # plt.show(block=False)\n",
    "    # plt.pause(.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc8471c-c158-4fca-be0a-b71fe6a2ffba",
   "metadata": {},
   "source": [
    "## game.py\n",
    "This is the actual game where we defien the number for left right, we defirned the Canvas (GUI), whcih display the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b548b926-5db1-4796-9d82-fc755bbf3283",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "import random\n",
    "from enum import Enum\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "\n",
    "pygame.init()\n",
    "font = pygame.font.Font('arial.ttf', 25)\n",
    "#font = pygame.font.SysFont('arial', 25)\n",
    "\n",
    "class Direction(Enum):\n",
    "    RIGHT = 1\n",
    "    LEFT = 2\n",
    "    UP = 3\n",
    "    DOWN = 4\n",
    "\n",
    "Point = namedtuple('Point', 'x, y')\n",
    "\n",
    "# rgb colors\n",
    "WHITE = (255, 255, 255)\n",
    "RED = (0,200,0)\n",
    "BLUE1 = (0, 0, 255)\n",
    "BLUE2 = (0, 100, 255)\n",
    "BLACK = (0,0,0)\n",
    "\n",
    "BLOCK_SIZE = 20\n",
    "SPEED = 40\n",
    "\n",
    "class SnakeGameAI:\n",
    "\n",
    "    def __init__(self, w=640, h=480):\n",
    "        self.w = w\n",
    "        self.h = h\n",
    "        # init display\n",
    "        self.display = pygame.display.set_mode((self.w, self.h))\n",
    "        pygame.display.set_caption('Snake')\n",
    "        self.clock = pygame.time.Clock()\n",
    "        self.reset()\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        # init game state\n",
    "        self.direction = Direction.RIGHT\n",
    "\n",
    "        self.head = Point(self.w/2, self.h/2)\n",
    "        self.snake = [self.head,\n",
    "                      Point(self.head.x-BLOCK_SIZE, self.head.y),\n",
    "                      Point(self.head.x-(2*BLOCK_SIZE), self.head.y)]\n",
    "\n",
    "        self.score = 0\n",
    "        self.food = None\n",
    "        self._place_food()\n",
    "        self.frame_iteration = 0\n",
    "\n",
    "\n",
    "    def _place_food(self):\n",
    "        x = random.randint(0, (self.w-BLOCK_SIZE )//BLOCK_SIZE )*BLOCK_SIZE\n",
    "        y = random.randint(0, (self.h-BLOCK_SIZE )//BLOCK_SIZE )*BLOCK_SIZE\n",
    "        self.food = Point(x, y)\n",
    "        if self.food in self.snake:\n",
    "            self._place_food()\n",
    "\n",
    "\n",
    "    def play_step(self, action):\n",
    "        self.frame_iteration += 1\n",
    "        # 1. collect user input\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                quit()\n",
    "        \n",
    "        # 2. move\n",
    "        self._move(action) # update the head\n",
    "        self.snake.insert(0, self.head)\n",
    "        \n",
    "        # 3. check if game over\n",
    "        reward = 0\n",
    "        game_over = False\n",
    "        if self.is_collision() or self.frame_iteration > 100*len(self.snake):\n",
    "            game_over = True\n",
    "            reward = -10\n",
    "            return reward, game_over, self.score\n",
    "\n",
    "        # 4. place new food or just move\n",
    "        if self.head == self.food:\n",
    "            self.score += 1\n",
    "            reward = 10\n",
    "            self._place_food()\n",
    "        else:\n",
    "            self.snake.pop()\n",
    "        \n",
    "        # 5. update ui and clock\n",
    "        self._update_ui()\n",
    "        self.clock.tick(SPEED)\n",
    "        # 6. return game over and score\n",
    "        return reward, game_over, self.score\n",
    "\n",
    "\n",
    "    def is_collision(self, pt=None):\n",
    "        if pt is None:\n",
    "            pt = self.head\n",
    "        # hits boundary\n",
    "        if pt.x > self.w - BLOCK_SIZE or pt.x < 0 or pt.y > self.h - BLOCK_SIZE or pt.y < 0:\n",
    "            return True\n",
    "        # hits itself\n",
    "        if pt in self.snake[1:]:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "\n",
    "    def _update_ui(self):\n",
    "        self.display.fill(BLACK)\n",
    "\n",
    "        for pt in self.snake:\n",
    "            pygame.draw.rect(self.display, BLUE1, pygame.Rect(pt.x, pt.y, BLOCK_SIZE, BLOCK_SIZE))\n",
    "            pygame.draw.rect(self.display, BLUE2, pygame.Rect(pt.x+4, pt.y+4, 12, 12))\n",
    "\n",
    "        pygame.draw.rect(self.display, RED, pygame.Rect(self.food.x, self.food.y, BLOCK_SIZE, BLOCK_SIZE))\n",
    "\n",
    "        text = font.render(\"Score: \" + str(self.score), True, WHITE)\n",
    "        self.display.blit(text, [0, 0])\n",
    "        pygame.display.flip()\n",
    "\n",
    "\n",
    "    def _move(self, action):\n",
    "        # [straight, right, left]\n",
    "\n",
    "        clock_wise = [Direction.RIGHT, Direction.DOWN, Direction.LEFT, Direction.UP]\n",
    "        idx = clock_wise.index(self.direction)\n",
    "\n",
    "        if np.array_equal(action, [1, 0, 0]):\n",
    "            new_dir = clock_wise[idx] # no change\n",
    "        elif np.array_equal(action, [0, 1, 0]):\n",
    "            next_idx = (idx + 1) % 4\n",
    "            new_dir = clock_wise[next_idx] # right turn r -> d -> l -> u\n",
    "        else: # [0, 0, 1]\n",
    "            next_idx = (idx - 1) % 4\n",
    "            new_dir = clock_wise[next_idx] # left turn r -> u -> l -> d\n",
    "\n",
    "        self.direction = new_dir\n",
    "\n",
    "        x = self.head.x\n",
    "        y = self.head.y\n",
    "        if self.direction == Direction.RIGHT:\n",
    "            x += BLOCK_SIZE\n",
    "        elif self.direction == Direction.LEFT:\n",
    "            x -= BLOCK_SIZE\n",
    "        elif self.direction == Direction.DOWN:\n",
    "            y += BLOCK_SIZE\n",
    "        elif self.direction == Direction.UP:\n",
    "            y -= BLOCK_SIZE\n",
    "\n",
    "        self.head = Point(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ea7404-d336-49be-ba1d-10b1708918b4",
   "metadata": {},
   "source": [
    "## agent.py\n",
    "Agent is one who calles the game and helper, and model. \n",
    "\n",
    "Agent is the one whoi is responsible for everthing. Calcuating the state, giving the input to model, deciding the out to the model, what decesion to make, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd89b85-d11e-42e7-9062-29b0de112b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from game import SnakeGameAI, Direction, Point\n",
    "from model import Linear_QNet, QTrainer\n",
    "from helper import plot\n",
    "\n",
    "MAX_MEMORY = 100_000 # underscore meaning visualization purpose.\n",
    "BATCH_SIZE = 1000\n",
    "LR = 0.001 # learning rate \\alpha\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.n_games = 0\n",
    "        self.epsilon = 0 # randomness\n",
    "        self.gamma = 0.9 # discount rate\n",
    "        self.memory = deque(maxlen=MAX_MEMORY) # popleft()\n",
    "        self.model = Linear_QNet(11, 256, 3)\n",
    "        self.trainer = QTrainer(self.model, lr=LR, gamma=self.gamma)\n",
    "\n",
    "\n",
    "    def get_state(self, game):\n",
    "        head = game.snake[0]\n",
    "        point_l = Point(head.x - 20, head.y)\n",
    "        point_r = Point(head.x + 20, head.y)\n",
    "        point_u = Point(head.x, head.y - 20)\n",
    "        point_d = Point(head.x, head.y + 20)\n",
    "        \n",
    "        dir_l = game.direction == Direction.LEFT\n",
    "        dir_r = game.direction == Direction.RIGHT\n",
    "        dir_u = game.direction == Direction.UP\n",
    "        dir_d = game.direction == Direction.DOWN\n",
    "\n",
    "        state = [\n",
    "            # Danger straight\n",
    "            (dir_r and game.is_collision(point_r)) or \n",
    "            (dir_l and game.is_collision(point_l)) or \n",
    "            (dir_u and game.is_collision(point_u)) or \n",
    "            (dir_d and game.is_collision(point_d)),\n",
    "\n",
    "            # Danger right\n",
    "            (dir_u and game.is_collision(point_r)) or \n",
    "            (dir_d and game.is_collision(point_l)) or \n",
    "            (dir_l and game.is_collision(point_u)) or \n",
    "            (dir_r and game.is_collision(point_d)),\n",
    "\n",
    "            # Danger left\n",
    "            (dir_d and game.is_collision(point_r)) or \n",
    "            (dir_u and game.is_collision(point_l)) or \n",
    "            (dir_r and game.is_collision(point_u)) or \n",
    "            (dir_l and game.is_collision(point_d)),\n",
    "            \n",
    "            # Move direction\n",
    "            dir_l,\n",
    "            dir_r,\n",
    "            dir_u,\n",
    "            dir_d,\n",
    "            \n",
    "            # Food location \n",
    "            game.food.x < game.head.x,  # food left\n",
    "            game.food.x > game.head.x,  # food right\n",
    "            game.food.y < game.head.y,  # food up\n",
    "            game.food.y > game.head.y  # food down\n",
    "            ]\n",
    "\n",
    "        return np.array(state, dtype=int)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done)) # popleft if MAX_MEMORY is reached\n",
    "\n",
    "    def train_long_memory(self):\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            mini_sample = random.sample(self.memory, BATCH_SIZE) # list of tuples\n",
    "        else:\n",
    "            mini_sample = self.memory\n",
    "\n",
    "        states, actions, rewards, next_states, dones = zip(*mini_sample)\n",
    "        self.trainer.train_step(states, actions, rewards, next_states, dones)\n",
    "        #for state, action, reward, nexrt_state, done in mini_sample:\n",
    "        #    self.trainer.train_step(state, action, reward, next_state, done)\n",
    "\n",
    "    def train_short_memory(self, state, action, reward, next_state, done):\n",
    "        self.trainer.train_step(state, action, reward, next_state, done)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        # random moves: tradeoff exploration / exploitation\n",
    "        self.epsilon = 80 - self.n_games\n",
    "        final_move = [0,0,0]\n",
    "        if random.randint(0, 200) < self.epsilon:\n",
    "            move = random.randint(0, 2)\n",
    "            final_move[move] = 1\n",
    "        else:\n",
    "            state0 = torch.tensor(state, dtype=torch.float)\n",
    "            prediction = self.model(state0)\n",
    "            move = torch.argmax(prediction).item()\n",
    "            final_move[move] = 1\n",
    "\n",
    "        return final_move\n",
    "\n",
    "\n",
    "def train():\n",
    "    plot_scores = []\n",
    "    plot_mean_scores = []\n",
    "    total_score = 0\n",
    "    record = 0\n",
    "    agent = Agent()\n",
    "    game = SnakeGameAI()\n",
    "    while True:\n",
    "        # get old state\n",
    "        state_old = agent.get_state(game)\n",
    "\n",
    "        # get move\n",
    "        final_move = agent.get_action(state_old)\n",
    "\n",
    "        # perform move and get new state\n",
    "        reward, done, score = game.play_step(final_move)\n",
    "        state_new = agent.get_state(game)\n",
    "\n",
    "        # train short memory\n",
    "        agent.train_short_memory(state_old, final_move, reward, state_new, done)\n",
    "\n",
    "        # remember\n",
    "        agent.remember(state_old, final_move, reward, state_new, done)\n",
    "\n",
    "        if done:\n",
    "            # train long memory, plot result\n",
    "            game.reset()\n",
    "            agent.n_games += 1\n",
    "            agent.train_long_memory()\n",
    "\n",
    "            if score > record:\n",
    "                record = score\n",
    "                agent.model.save()\n",
    "\n",
    "            print('Game', agent.n_games, 'Score', score, 'Record:', record)\n",
    "\n",
    "            plot_scores.append(score)\n",
    "            total_score += score\n",
    "            mean_score = total_score / agent.n_games\n",
    "            plot_mean_scores.append(mean_score)\n",
    "            plot(plot_scores, plot_mean_scores)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
