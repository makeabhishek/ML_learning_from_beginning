{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0704c8a-98dc-4d68-967d-73e45cab74ea",
   "metadata": {},
   "source": [
    "# Diffusion models\n",
    "Denosiing an image suign diffusion model in PyTorch\n",
    "\n",
    "- With diffusion models we are in the domain of generativedeep learning. that measn we want to learn distribution over the data in order to generate new data. there are lots of model available to generate new data such as variaional autoencoder (VAE) and generative adversirial NN.\n",
    "<center><img src='./images/GANs_Diffusion_Autoencoders.png' width=250px></center> \n",
    "Ref: https://developer.nvidia.com/blog/improving-diffusion-models-as-an-alternative-to-gans-part-1/\n",
    "\n",
    "- VAE and Normalizing flows have shown to produce diverse samples quickly but usually the quality is not great compared to GANS.\n",
    "- As a recap VAE compresses an input into a latent distribution and then samples from this distribution to recover the inputs. After training we can sample from the latent space to generate new data points usually VAE's are quite easy to train but as mentioned the outputs can be blurry.\n",
    "<center><img src='./images/vae.png' width=250px></center> \n",
    "\n",
    "- GANS on the other hand produce high quality outputs but are most of the time difficult to train. This stems from the adversarial setup which can cause problems such as vanishing gradients or mode collapse. Lots of these issues myself throughout the years of course there are already lots of improvements nowadays but still it's not that easy to fit such a model\n",
    "<center><img src='./images/GAN.png' width=250px></center> \n",
    "\n",
    "- You may probably know the diffusion model is a rather new generative deep learning model that has shown to produce high quality samples which are also quite diverse the fusion models are part of a lot of modern deep learning architectures and recently showed great success in text-guided image generation such as in DALL.E2 or imogen\n",
    "- Diffusion models work by destroying the input until noise is left over and then recovering the input from noise using a neural network in the backward pass also called denoising. This is also called a markov chain because it's a sequence of stochastic events where each time step depends on the previous time step. a special property of the Diffusion models is that the latent states have the same dimensionality as the input. The task of the model can be described as predicting the noise that was added in each of the images that's why the backward process is called parametrized we use a neural network. In order to generate new data one can simply perform the backward process from\n",
    "random noise and new data points are constructed. A typical number of steps that is chosen in this sequential process is 1000 however of course the larger this number is the slower the sampling will be.\n",
    "\n",
    "- Diffusion models also have their downsides for example the sampling speed because of the sequential reverse process they are much slower compared to GANS or VAEs.\n",
    "<center><img src='./images/diffusionModel.png' width=250px></center> \n",
    "\n",
    "- we will fit a diffusion model on an image data set. The architecture and model design are mainly based on the following two papers the paper on the left from researchers from berkeley university was one of the first publications that uses diffusion\n",
    "- Denoising Diffusion Probabilistic Models\n",
    "    - (1) DDPM: https://arxiv.org/pdf/2006.11239.pdf\n",
    "    - (2) DDPM Improved: https://arxiv.org/pdf/2105.05233.pdf\n",
    "- Second paper by openAI is the follow up paper whcih propose improvements that make the image quality even better for example they use additional normalization layers, residual connections.\n",
    "\n",
    "Read more about diffusion models: Awesome-Diffusion-Models https://github.com/diff-usion/Awesome-Diffusion-Models\n",
    "\n",
    "### How to implement them?\n",
    "- we will need mainly three things for that a scheduler that sequentially adds noise a model that predicts the noise in an image which will be a unit in our case and finally a way to encode the current time step. \n",
    "<center><img src='./images/diff_implement.png' width=250px></center> \n",
    "\n",
    "It consists of several steps mainly the \n",
    "- forward process: Add noise to the image\n",
    "- Backward process,\n",
    "- the loss function and then we can already define sampling and training.\n",
    "\n",
    "\n",
    "converge towards zero the important part is to add the right amount of noise such that we arrive at an isotropic gaussian distribution with a mean of 0 and a fixed variance in all directions otherwise the sampling later will not work that means we don't want to add too few noise but also not too much in order to have a too noisy image too early there are different scheduling strategies for that in our case we will simply add the noise linearly but in the literature there are also choices like quadratic cosine sigmoidal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2424b4eb-5627-4ea8-92b2-b25ce3c40b76",
   "metadata": {},
   "source": [
    "### Investigating the dataset\n",
    "- As dataset we use the StandordCars Dataset, which consists of total 16000 images. Around 8000 images in the train set.\n",
    "- images are in many different poses with a variety of backgrounds so we should expect a lot of diversity in our generated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7573a1ee-f657-44a0-be1d-9e8a4096a163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_images(datset, num_samples=20, cols=4):\n",
    "    \"\"\" Plots some samples from the dataset \"\"\"\n",
    "    plt.figure(figsize=(15,15)) \n",
    "    for i, img in enumerate(data):\n",
    "        if i == num_samples:\n",
    "            break\n",
    "        plt.subplot(int(num_samples/cols) + 1, cols, i + 1)\n",
    "        plt.imshow(img[0])\n",
    "\n",
    "data = torchvision.datasets.StanfordCars(root=\".\", download=True)\n",
    "show_images(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f21d537-fe2a-4cd6-89dd-324b94b4145c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
