{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcf0f274-23d7-464f-ab36-e270ad6a371b",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network (CNN)\n",
    "implement our first convolutional neural network (CNN) that can do image classification based on the famous CIFAR-10 dataset.\n",
    "\n",
    "We will learn:\n",
    "- Dataset: CIFAR-10 dataset available in PyTorch.  https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "    - dataset with 10 differnet classes like airplane, automobile, bird, cat, deer,\tdog, frog, horses, ship, truck\n",
    "- Architecture of CNNs\n",
    "- Convolutional Filter\n",
    "- Max Pooling\n",
    "- Determine the correct layer size\n",
    "- Implement the CNN architecture in PyTorch\n",
    "\n",
    "- Similar to other neural nets they are made of neurons that have learnable weights and biases. The difference is that Conv Nets were mainly for image data and apply the convolutional filters. We have image --> conv layers --> activation fucntion --> pooling --> fully connected layer\n",
    "- After applying convolution the resulting image will may have a smalller size because our filter does not fit in the corner. So we use a technique called padding. Getting correct size is import.\n",
    "- Pooling reduce the size of the image so reduice the cost of computation. OS this reduces the number of parameters our model has to run and avoaid overfitting by giving abstract features,\n",
    "\n",
    "<center><img src='./images/cnn_Cifar.PNG' width=550px></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9241b3-c81b-454c-a065-d513f7325d1b",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network Intro \n",
    "<center><img src='./images/cnn_example.PNG' width=800px></center> \n",
    "\n",
    "### Image Filter / Image Kernel \n",
    "- First, we do the convolution to the image to extract features. We create a filter/kernel (whcih uis basically a matrix) and apply convolution.\n",
    "- In this kernel the values are designed by NN to extract features. NN will learn those and convolute with image.\n",
    "- CNN is usually detect the edges (like sobel filter-right, left, top, bottom)\n",
    "- In colour images we have 3 colour channels (RGB) with hights and widht of the image.\n",
    "- After pooling we have fully connected layer.\n",
    "\n",
    "<center><img src='./images/cnn_kernel.PNG' width=600px></center> \n",
    "\n",
    "Ref: Visualize the convolution: https://setosa.io/ev/image-kernels/\n",
    "\n",
    "### Why to use CNN instead of ANN or FCNN?\n",
    "- When the data becaomes very big, in ANN all neurons are fully connected. Its diffilcult to process data. However in CNN its not fully connected. it is locally connected.\n",
    "- Once we extract feateures we do pooling tor reduce feature\n",
    "- Once we do this process we than use fully connected layer after flattening.\n",
    "- CNN is crunching the parameters down by doing convolution using filetering and further by pooling.\n",
    "\n",
    "<center><img src='./images/cnn_local_connect.PNG' width=600px></center>  \n",
    "\n",
    "### Pooling Layer\n",
    "- Reduce the features further. It reduces the amount of data in an image by combining information from multiple vectors into fewer vectors\n",
    "<center><img src='./images/pool_concept.jpg' width=600px></center>  \n",
    "\n",
    "<center><img src='./images/pooling.PNG' width=600px></center>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77754b2d-3fb8-4af8-bfb0-3db6cffd8a88",
   "metadata": {},
   "source": [
    "# CNN in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7ebc53-9fc0-4f80-8758-65c3bf3095b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters \n",
    "num_epochs = 5\n",
    "batch_size = 4\n",
    "learning_rate = 0.001\n",
    "\n",
    "# dataset has PILImage images of range [0, 1]. \n",
    "# We transform them to Tensors of normalized range [-1, 1]\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# CIFAR10: 60000 32x32 color images in 10 classes, with 6000 images per class\n",
    "# PyToech dataset and PyTorch dataloader. This can help in batch optimization and batch training\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                         shuffle=False)\n",
    "\n",
    "# hard coded the classes\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f04521eb-c557-4dc2-8f53-f906635cd829",
   "metadata": {},
   "source": [
    "<center><img src='./images/cnn_Cifar_.PNG' width=600px></center> \n",
    "<center><img src='./images/convFilter.PNG' width=550px></center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6edcfca-c1a6-4949-8502-fafb1a5764c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "\n",
    "# First conv layer\n",
    "conv1 = nn.Conv2d(3, 6, 5)\n",
    "pool = nn.MaxPool2d(2,2)\n",
    "# second conv layer\n",
    "conv2 = nn.Conv2d(6, 16, 5)\n",
    "print(images.shape)\n",
    "# >> torch.Size([4, 3, 32, 32]) beacuse the battch size is 4, 3 color channel, image size of 32x32\n",
    "\n",
    "# Apply first convolutional layer\n",
    "x = conv1(images)\n",
    "# print(x.shape)\n",
    "# >> torch.Size([4, 6, 28, 28]) # 6 output channels. image size 28x28 because of convolution\n",
    "x= pool(x)\n",
    "# print(x.shape)\n",
    "# >> torch.Size([4, 6, 14, 14]) # pooling reduce the images by factor of two with kernel of 2\n",
    "\n",
    "x= conv2(x)\n",
    "# print(x.shape)\n",
    "# >> torch.Size([4, 16, 10, 10])  # 16 because w especified the 16 \n",
    "\n",
    "x= pool(x)\n",
    "print(x.shape)\n",
    "# >> torch.Size([4, 16, 5, 5]) \n",
    "\n",
    "# Now flatten 3D tensor to 1D tensor. So in fully connected layer our size is (16 * 5 * 5)\n",
    "# self.fc1 = nn.Linear(16 * 5 * 5, 120) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd2240c-462a-42b5-9061-9e852166f8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement ConvNet\n",
    "\"\"\"\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "# Refer the architecture. \n",
    "(1) First we have convolutional layer followed by ReLu activation funciton --> than Pooling.\n",
    "(2) We have second convolution layer + activation + pooling \n",
    "(3) we have three differnt fully connected layers\n",
    "(4) At end we have softMax and crossEntropy. In PyTorch softMax is already included in crossEntropy Loss\n",
    "\"\"\"\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # First conv layer + Pooling  (No activation)\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5) # input channel size, output channel size, kernel size\n",
    "        self.pool = nn.MaxPool2d(2, 2) # pooling size and stride\n",
    "        # Second conv layer\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # Set fully connected layer. 16 * 5 * 5 and at end 10 must be fixed\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120) # input size, output Size (choose what you want). this is what we obtain after convolution. Check above cell \n",
    "        self.fc2 = nn.Linear(120, 84) # 120 input features and 84 output features\n",
    "        self.fc3 = nn.Linear(84, 10) # ouput must be 10 for 10 differnt classes\n",
    "\n",
    "    # We have all the layers and than do the forward pass.\n",
    "    def forward(self, x):\n",
    "        # -> n, 3, 32, 32\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # -> n, 6, 14, 14 # First convolution and polling layer\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # -> n, 16, 5, 5  # Second convolution and polling layer\n",
    "        x = x.view(-1, 16 * 5 * 5)            # -> n, 400       # Flatten the output of convolution\n",
    "        x = F.relu(self.fc1(x))               # -> n, 120       # First fully connected layer with activation\n",
    "        x = F.relu(self.fc2(x))               # -> n, 84        # Second fully connected layer with activation\n",
    "        x = self.fc3(x)                       # -> n, 10        # Third fully connected layer, no activation\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dc98f0-9342-4082-abbe-13a21989809e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model and define loss and optimizer\n",
    "model = ConvNet().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # multiclass classification problem\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273ae987-079e-42dc-8501-f5a234ced263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop for batch optimization. Loop over epoch and than loop over train laoder\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # origin shape: [4, 3, 32, 32] = 4, 3, 1024\n",
    "        # input_layer: 3 input channels, 6 output channels, 5 kernel size\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad() # empty the gradients\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 2000 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print('Finished Training')\n",
    "PATH = './cnn.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06470c4b-4809-40ce-83de-d71f0dc54c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap using `with` because w e dont need backward propagation\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(10)]\n",
    "    n_class_samples = [0 for i in range(10)]\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        # max returns (value ,index)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            label = labels[i]\n",
    "            pred = predicted[i]\n",
    "            if (label == pred):\n",
    "                n_class_correct[label] += 1\n",
    "            n_class_samples[label] += 1\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network: {acc} %')\n",
    "\n",
    "    for i in range(10):\n",
    "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "        print(f'Accuracy of {classes[i]}: {acc} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f22e1e4-ca60-488b-9846-38fc702ad5c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
