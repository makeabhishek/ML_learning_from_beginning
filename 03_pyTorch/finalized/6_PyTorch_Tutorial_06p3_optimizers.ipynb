{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b0a424c-c9b3-4516-bb92-da087cc048b3",
   "metadata": {},
   "source": [
    "# **Optimization Methods: Newton, Gauss-Newton, Quasi-Newton, and Levenberg-Marquardt**\n",
    "\n",
    "## **1. Introduction**\n",
    "Optimization methods are used to find the minimum (or maximum) of a function. Many advanced optimization techniques rely on **first-order** and **second-order** derivatives.\n",
    "\n",
    "Before discussing **Newtonâ€™s method, Gauss-Newton, Quasi-Newton, and Levenberg-Marquardt**, let's understand **gradients and Hessians**.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Fundamentals: Gradient and Hessian**\n",
    "\n",
    "### **2.1 Gradient**\n",
    "The **gradient** is a vector containing the **first-order partial derivatives** of a function. It represents the direction of **steepest ascent**.\n",
    "\n",
    "For a function $ f(x) $ with multiple variables:\n",
    "$$\n",
    "\\nabla f(x) = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial x_1} \\\\\n",
    "\\frac{\\partial f}{\\partial x_2} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial f}{\\partial x_n}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "- **Gradient descent** moves in the **negative** gradient direction to minimize a function.\n",
    "\n",
    "---\n",
    "\n",
    "### **2.2 Hessian**\n",
    "The **Hessian matrix** is a **square matrix of second-order partial derivatives**. It describes how the gradient changes and is useful in second-order optimization methods.\n",
    "\n",
    "For a function $ f(x) $ the Hessian matrix is:\n",
    "$$\n",
    "H_f(x) =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} & \\dots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\\n",
    "\\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} & \\dots & \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} & \\dots & \\frac{\\partial^2 f}{\\partial x_n^2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "- The Hessian helps determine **convexity**:\n",
    "  - If all eigenvalues of $ H_f(x) $ are **positive**, $ f(x) $ is **convex** (has a unique minimum).\n",
    "  - If some eigenvalues are **negative**, $ f(x) $ is **non-convex** (may have multiple minima).\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Newtonâ€™s Method**\n",
    "Newtonâ€™s Method is a second-order optimization algorithm that **uses the Hessian** to find a functionâ€™s minimum.\n",
    "\n",
    "### **Formula**\n",
    "$$\n",
    "x_{k+1} = x_k - H_f^{-1}(x_k) \\nabla f(x_k)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ H_f^{-1}(x_k) $ is the **inverse Hessian**,\n",
    "- $ \\nabla f(x_k) $ is the **gradient**,\n",
    "- $ x_k $ is the current estimate.\n",
    "\n",
    "### **Advantages:**\n",
    "âœ” Faster convergence than gradient descent when near a minimum.  \n",
    "âœ” Works well for quadratic functions.\n",
    "\n",
    "### **Disadvantages:**\n",
    "âœ– Computing the **Hessian inverse** is expensive for high-dimensional problems.  \n",
    "âœ– Sensitive to initial conditions.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Gauss-Newton Method**\n",
    "Gauss-Newton is a **simplified version of Newtonâ€™s Method** used for **nonlinear least squares problems**.\n",
    "\n",
    "### **Formula**\n",
    "Instead of computing the full Hessian, we approximate it:\n",
    "$$\n",
    "H_f(x) \\approx J^T J\n",
    "$$\n",
    "where:\n",
    "- $ J $ is the **Jacobian matrix** (first-order derivatives of residuals in a least squares problem).\n",
    "\n",
    "Then, the Gauss-Newton update rule is:\n",
    "$$\n",
    "x_{k+1} = x_k - (J^T J)^{-1} J^T r\n",
    "$$\n",
    "where $ r $ is the residual vector.\n",
    "\n",
    "### **Advantages:**\n",
    "âœ” Faster than Newtonâ€™s method since it avoids computing the full Hessian.  \n",
    "âœ” Works well for **least squares** problems.\n",
    "\n",
    "### **Disadvantages:**\n",
    "âœ– Only works well if **residual errors are small**.  \n",
    "âœ– Can fail when $ J^T J $ is **singular**.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Quasi-Newton Methods**\n",
    "Quasi-Newton methods **approximate the inverse Hessian** instead of computing it explicitly.\n",
    "\n",
    "### **Popular Quasi-Newton Algorithms:**\n",
    "1. **BFGS (Broyden-Fletcher-Goldfarb-Shanno)**\n",
    "   - Uses past gradients to estimate the Hessian inverse.\n",
    "   - Update formula:\n",
    "     $$\n",
    "     B_{k+1} = B_k + \\frac{y_k y_k^T}{y_k^T s_k} - \\frac{B_k s_k s_k^T B_k}{s_k^T B_k s_k}\n",
    "     $$\n",
    "     where $ y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k) \\) and \\( s_k = x_{k+1} - x_k $.\n",
    "\n",
    "2. **L-BFGS (Limited-memory BFGS)**\n",
    "   - Stores only a few past updates, reducing memory usage.\n",
    "   - Often used in **deep learning optimization**.\n",
    "\n",
    "### **Advantages:**\n",
    "âœ” Faster than Newtonâ€™s method without computing full Hessian.  \n",
    "âœ” Works well for **large-scale problems**.\n",
    "\n",
    "### **Disadvantages:**\n",
    "âœ– Not as accurate as full Newtonâ€™s method in some cases.  \n",
    "âœ– More tuning required than standard gradient methods.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Levenberg-Marquardt Algorithm**\n",
    "The **Levenberg-Marquardt (LM) method** is a **combination of Gauss-Newton and gradient descent**. It is used for **nonlinear least squares optimization**.\n",
    "\n",
    "### **Formula**\n",
    "$$\n",
    "x_{k+1} = x_k - (J^T J + \\lambda I)^{-1} J^T r\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ J $ is the **Jacobian** of residuals,\n",
    "- $ \\lambda $ is a **damping factor**,\n",
    "- $ I $ is the identity matrix.\n",
    "\n",
    "### **How It Works**\n",
    "- If $ \\lambda $ is **small**, LM behaves like **Gauss-Newton**.\n",
    "- If $ \\lambda $ is **large**, LM behaves like **Gradient Descent**.\n",
    "- The damping factor \\( \\lambda \\) adjusts adaptively.\n",
    "\n",
    "### **Advantages:**\n",
    "âœ” **Stable**: Avoids issues with singular matrices in Gauss-Newton.  \n",
    "âœ” **Combines benefits of second-order and first-order methods**.  \n",
    "âœ” **Works well for least squares problems**.\n",
    "\n",
    "### **Disadvantages:**\n",
    "âœ– Requires tuning of $ \\lambda \\).  \n",
    "âœ– Computationally expensive for high-dimensional problems.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Summary Table**\n",
    "\n",
    "| Method | Type | Requires Hessian? | Best Use Case | Pros | Cons |\n",
    "|--------|------|------------------|--------------|------|------|\n",
    "| **Gradient Descent** | First-order | No | General optimization | Simple | Slow convergence |\n",
    "| **Newton's Method** | Second-order | Yes | Quadratic problems | Fast near minimum | Expensive Hessian computation |\n",
    "| **Gauss-Newton** | Approximate second-order | No (uses Jacobian) | Least squares | Faster than Newton | Can fail if \\( J^T J \\) is singular |\n",
    "| **Quasi-Newton (BFGS)** | Approximate second-order | No (approximates Hessian) | Large-scale problems | Memory efficient | Less accurate |\n",
    "| **Levenberg-Marquardt** | Hybrid (Newton + Gradient Descent) | No (uses damping) | Nonlinear least squares | Stable & fast | Needs \\( \\lambda \\) tuning |\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "- **Newtonâ€™s Method** is fast but expensive.\n",
    "- **Gauss-Newton** is efficient for **least squares**.\n",
    "- **Quasi-Newton (BFGS, L-BFGS)** balances speed and efficiency.\n",
    "- **Levenberg-Marquardt** is the most stable for nonlinear least squares.\n",
    "\n",
    "ðŸš€ **Choosing the right method depends on problem size, constraints, and available computational resources.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc38040a-e4c6-4943-81d7-4e70f7185553",
   "metadata": {},
   "source": [
    "# Different Types of Optimizers in PyTorch\n",
    "\n",
    "- Optimizers in deep learning help in updating the model's parameters to minimize the loss function. Below are different optimizers used in PyTorch with their mathematical formulas and examples.\n",
    "- This document provides mathematical insights and PyTorch implementations for various optimizers. Choose the one that best fits your problem! \n",
    "---\n",
    "---\n",
    "## 0. **Gradient Descent**\n",
    "### **Formula**\n",
    "$$\n",
    "\\theta = \\theta - \\eta \\nabla J(\\theta)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ \\theta $ represents the model parameters,\n",
    "- $ \\eta $ is the learning rate,\n",
    "- $ \\nabla J(\\theta) $ is the gradient of the loss function with respect to $ \\theta $\n",
    "\n",
    "### **Explanation**\n",
    "- **Gradient Descent** is an optimization algorithm used to minimize a function by iteratively moving in the direction of steepest descent.\n",
    "- In **Batch Gradient Descent (BGD)**, the **entire dataset** is used to compute the gradient before updating the parameters.\n",
    "- The **learning rate $ \\eta $ controls the step size** of each update.\n",
    "\n",
    "### **Variants of Gradient Descent**\n",
    "- **Batch Gradient Descent (BGD)** â†’ Uses the entire dataset.\n",
    "- **Stochastic Gradient Descent (SGD)** â†’ Updates parameters after each sample.\n",
    "- **Mini-batch Gradient Descent** â†’ Uses small batches of data for updates.\n",
    "\n",
    "### **PyTorch Implementation**\n",
    "```python\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# Example tensor parameters\n",
    "params = torch.randn((2, 2), requires_grad=True)\n",
    "\n",
    "# Define a simple gradient descent optimizer\n",
    "optimizer = optim.SGD([params], lr=0.01)  # Standard Gradient Descent\n",
    "\n",
    "# Simulate a loss computation\n",
    "loss = params.sum()\n",
    "loss.backward()\n",
    "\n",
    "# Perform an optimization step\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()\n",
    "```\n",
    "\n",
    "---\n",
    "## 1. **Stochastic Gradient Descent (SGD)**\n",
    "## **Formula**\n",
    "$$\n",
    "\\theta = \\theta - \\eta \\nabla J(\\theta)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ \\theta $ represents the model parameters,\n",
    "- $ \\eta $ is the learning rate,\n",
    "- $ \\nabla J(\\theta) $ is the gradient of the loss function with respect to \\( \\theta \\).\n",
    "\n",
    "## **Explanation**\n",
    "- **SGD** updates the parameters in the direction that minimizes the loss function.\n",
    "- Unlike batch gradient descent, **SGD** updates parameters **after each training example**, making it more suitable for large datasets.\n",
    "\n",
    "## **PyTorch Implementation**\n",
    "```python\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# Example tensor parameters\n",
    "params = torch.randn((2, 2), requires_grad=True)\n",
    "\n",
    "# Define the SGD optimizer\n",
    "optimizer = optim.SGD([params], lr=0.01)\n",
    "\n",
    "# Simulate a loss computation\n",
    "loss = params.sum()\n",
    "loss.backward()\n",
    "\n",
    "# Perform an optimization step\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Momentum-Based SGD**\n",
    "### **Formula**\n",
    "$$\n",
    "v_t = \\beta v_{t-1} + (1 - \\beta) \\nabla J(\\theta)\n",
    "$$\n",
    "$$\n",
    "\\theta = \\theta - \\eta v_t\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ v_t $ is the velocity term that helps smooth updates,\n",
    "- $ \\beta $ is the momentum factor (typically between 0.9 and 0.99),\n",
    "- $ \\eta $ is the learning rate,\n",
    "- $ \\nabla J(\\theta) $ is the gradient of the loss function.\n",
    "\n",
    "### **Explanation**\n",
    "- **Momentum-Based SGD** helps speed up convergence and prevents oscillations.\n",
    "- Instead of updating only using the current gradient, it accumulates past gradients to **build momentum** in the direction of faster convergence.\n",
    "- This technique is particularly useful for navigating **ravines** in loss surfaces.\n",
    "\n",
    "### **PyTorch Implementation**\n",
    "```python\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# Example tensor parameters\n",
    "params = torch.randn((2, 2), requires_grad=True)\n",
    "\n",
    "# Define the SGD optimizer with momentum\n",
    "optimizer = optim.SGD([params], lr=0.01, momentum=0.9)\n",
    "\n",
    "# Simulate a loss computation\n",
    "loss = params.sum()\n",
    "loss.backward()\n",
    "\n",
    "# Perform an optimization step\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()\n",
    "```\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **Adam (Adaptive Moment Estimation)**\n",
    "### **Formula**\n",
    "$$\n",
    "m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) \\nabla J(\\theta)\n",
    "$$\n",
    "$$\n",
    "v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) (\\nabla J(\\theta))^2\n",
    "$$\n",
    "$$\n",
    "\\hat{m_t} = \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v_t} = \\frac{v_t}{1 - \\beta_2^t}\n",
    "$$\n",
    "$$\n",
    "\\theta = \\theta - \\eta \\frac{\\hat{m_t}}{\\sqrt{\\hat{v_t}} + \\epsilon}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ m_t $ is the **first moment estimate** (moving average of gradients),\n",
    "- $ v_t $ is the **second moment estimate** (moving average of squared gradients),\n",
    "- $ \\beta_1 $ and \\( \\beta_2 \\) are decay rates (default: \\( 0.9 \\) and \\( 0.999 \\)),\n",
    "- $ \\epsilon $ is a small number to prevent division by zero,\n",
    "- $ \\eta $ is the learning rate.\n",
    "\n",
    "### **Explanation**\n",
    "- Adam combines the benefits of **Momentum-Based SGD** and **RMSprop**.\n",
    "- It adapts learning rates for each parameter dynamically using **first-moment (mean of gradients)** and **second-moment (variance of gradients)** estimates.\n",
    "- **Well-suited for non-stationary problems and sparse gradients**.\n",
    "\n",
    "### **PyTorch Implementation**\n",
    "```python\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# Example tensor parameters\n",
    "params = torch.randn((2, 2), requires_grad=True)\n",
    "\n",
    "# Define the Adam optimizer\n",
    "optimizer = optim.Adam([params], lr=0.001)\n",
    "\n",
    "# Simulate a loss computation\n",
    "loss = params.sum()\n",
    "loss.backward()\n",
    "\n",
    "# Perform an optimization step\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **RMSprop (Root Mean Square Propagation)**\n",
    "### **Formula**\n",
    "$$\n",
    "v_t = \\beta v_{t-1} + (1 - \\beta) (\\nabla J(\\theta))^2\n",
    "$$\n",
    "$$\n",
    "\\theta = \\theta - \\eta \\frac{\\nabla J(\\theta)}{\\sqrt{v_t} + \\epsilon}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ v_t $ is the exponentially decaying average of past squared gradients,\n",
    "- $ \\beta $ is the decay factor (typically **0.9**),\n",
    "- $ \\eta $ is the learning rate,\n",
    "- $ \\epsilon $ is a small constant for numerical stability.\n",
    "\n",
    "### **Explanation**\n",
    "- RMSprop is designed to deal with **vanishing gradients** and **smoother convergence**.\n",
    "- It **normalizes the learning rate** for each parameter using a **moving average** of squared gradients.\n",
    "- Often used for **recurrent neural networks (RNNs)** and **deep learning models**.\n",
    "\n",
    "### **PyTorch Implementation**\n",
    "```python\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# Example tensor parameters\n",
    "params = torch.randn((2, 2), requires_grad=True)\n",
    "\n",
    "# Define the RMSprop optimizer\n",
    "optimizer = optim.RMSprop([params], lr=0.01, alpha=0.99)\n",
    "\n",
    "# Simulate a loss computation\n",
    "loss = params.sum()\n",
    "loss.backward()\n",
    "\n",
    "# Perform an optimization step\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. **Adagrad (Adaptive Gradient Algorithm)**\n",
    "### **Formula**\n",
    "$$\n",
    "v_t = v_{t-1} + (\\nabla J(\\theta))^2\n",
    "$$\n",
    "$$\n",
    "\\theta = \\theta - \\eta \\frac{\\nabla J(\\theta)}{\\sqrt{v_t} + \\epsilon}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ v_t $ accumulates the sum of past squared gradients,\n",
    "- $ \\eta $ is the learning rate,\n",
    "- $ \\epsilon $ is a small constant to prevent division by zero.\n",
    "\n",
    "### **Explanation**\n",
    "- Adagrad **adapts the learning rate** for each parameter **individually** based on past gradients.\n",
    "- It is **well-suited for sparse data** (e.g., NLP applications, embeddings).\n",
    "- However, the **accumulated squared gradients keep growing**, which can **lead to vanishing learning rates over time**.\n",
    "\n",
    "### **PyTorch Implementation**\n",
    "```python\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# Example tensor parameters\n",
    "params = torch.randn((2, 2), requires_grad=True)\n",
    "\n",
    "# Define the Adagrad optimizer\n",
    "optimizer = optim.Adagrad([params], lr=0.01)\n",
    "\n",
    "# Simulate a loss computation\n",
    "loss = params.sum()\n",
    "loss.backward()\n",
    "\n",
    "# Perform an optimization step\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. **Adadelta**\n",
    "Adadelta is an adaptive learning rate optimization algorithm that dynamically adjusts the learning rate without requiring a manual setting.\n",
    "\n",
    "## **Mathematical Formulation**\n",
    "\n",
    "### **1. Compute the running average of squared gradients**\n",
    "$$\n",
    "E[g^2]_t = \\rho E[g^2]_{t-1} + (1 - \\rho) g_t^2\n",
    "$$\n",
    "where:\n",
    "- $ E[g^2]_t $ is the exponentially decaying average of past squared gradients,\n",
    "- $ \\rho $ is the decay rate (typically 0.9 or 0.95),\n",
    "- $ g_t $ is the gradient at time step \\( t \\).\n",
    "\n",
    "### **2. Compute update step**\n",
    "$$\n",
    "\\Delta \\theta_t = - \\frac{\\text{RMS}[\\Delta \\theta]_{t-1}}{\\sqrt{E[g^2]_t + \\epsilon}} g_t\n",
    "$$\n",
    "where:\n",
    "- $ \\Delta \\theta_t $ is the parameter update,\n",
    "- $ \\text{RMS}[\\Delta \\theta]_{t-1} = \\sqrt{E[\\Delta \\theta^2]_{t-1} + \\epsilon} $ is the root mean square of past updates,\n",
    "- $ \\epsilon $ is a small constant for numerical stability.\n",
    "\n",
    "### **3. Update running average of parameter updates**\n",
    "$$\n",
    "E[\\Delta \\theta^2]_t = \\rho E[\\Delta \\theta^2]_{t-1} + (1 - \\rho) \\Delta \\theta_t^2\n",
    "$$\n",
    "\n",
    "### **4. Update the parameters**\n",
    "$$\n",
    "\\theta_t = \\theta_{t-1} + \\Delta \\theta_t\n",
    "$$\n",
    "\n",
    "## **Key Characteristics of Adadelta**\n",
    "- Unlike Adagrad, Adadelta **does not require a manually set learning rate**.\n",
    "- It ensures that updates are **scale-invariant**.\n",
    "- Helps in cases where learning rates diminish too quickly.\n",
    "\n",
    "## **PyTorch Implementation**\n",
    "```python\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# Example tensor parameters\n",
    "params = torch.randn((2, 2), requires_grad=True)\n",
    "\n",
    "# Define the Adadelta optimizer\n",
    "optimizer = optim.Adadelta([params], lr=1.0)\n",
    "\n",
    "# Simulate a loss computation\n",
    "loss = params.sum()\n",
    "loss.backward()\n",
    "\n",
    "# Perform an optimization step\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. **AdamW (Adam with Weight Decay)**\n",
    "### **Formula**\n",
    "$$\n",
    "m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) \\nabla J(\\theta)\n",
    "$$\n",
    "$$\n",
    "v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) (\\nabla J(\\theta))^2\n",
    "$$\n",
    "$$\n",
    "\\hat{m_t} = \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v_t} = \\frac{v_t}{1 - \\beta_2^t}\n",
    "$$\n",
    "$$\n",
    "\\theta = \\theta - \\eta \\left( \\frac{\\hat{m_t}}{\\sqrt{\\hat{v_t}} + \\epsilon} + \\lambda \\theta \\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ m_t $ and $ v_t $ are the first and second moment estimates (moving averages of gradients and squared gradients),\n",
    "- $ \\beta_1 $ and $ \\beta_2 $ are decay rates (typically **0.9** and **0.999**),\n",
    "- $ \\lambda $ is the weight decay factor (L2 regularization),\n",
    "- $ \\eta $ is the learning rate.\n",
    "\n",
    "### **Explanation**\n",
    "- AdamW **modifies Adam by decoupling weight decay from the optimization step**.\n",
    "- Unlike **Adam**, where weight decay is applied via L2 regularization in gradients, **AdamW applies weight decay directly to parameters**.\n",
    "- **Better suited for deep learning models** such as transformers.\n",
    "\n",
    "### **PyTorch Implementation**\n",
    "```python\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# Example tensor parameters\n",
    "params = torch.randn((2, 2), requires_grad=True)\n",
    "\n",
    "# Define the AdamW optimizer with weight decay\n",
    "optimizer = optim.AdamW([params], lr=0.001, weight_decay=1e-2)\n",
    "\n",
    "# Simulate a loss computation\n",
    "loss = params.sum()\n",
    "loss.backward()\n",
    "\n",
    "# Perform an optimization step\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Summary Table\n",
    "\n",
    "| Optimizer | Key Feature | When to Use |\n",
    "|-----------|------------|-------------|\n",
    "| **SGD** | Basic gradient descent | Simple problems, small datasets |\n",
    "| **Momentum SGD** | Adds velocity to updates | Helps escape local minima |\n",
    "| **Adam** | Combines momentum and RMSprop | Most commonly used, adaptive learning rates |\n",
    "| **RMSprop** | Normalizes updates | Used in RNNs, deep networks |\n",
    "| **Adagrad** | Adapts learning rates per parameter | Sparse data, NLP applications |\n",
    "| **Adadelta** | No need for manual learning rate tuning | When learning rate tuning is difficult |\n",
    "| **AdamW** | Adam with weight decay | Large models with regularization |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d9a6d6-aa51-49fe-bf25-d86a25db46bc",
   "metadata": {},
   "source": [
    "# **Adjoint-Based Optimization**\n",
    "\n",
    "## **Introduction**\n",
    "Adjoint-based optimization is commonly used in physics-informed machine learning, control systems, tomography, waveform inversion, elastodynamicsand computational fluid dynamics (CFD). It is particularly useful for optimizing functions where direct gradient computation is expensive.\n",
    "\n",
    "Instead of computing gradients directly using standard backpropagation, adjoint-based optimization leverages the **adjoint method**, which efficiently computes gradients of a function concerning multiple parameters using **Lagrange multipliers**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Mathematical Formulation**\n",
    "\n",
    "Given an objective function:\n",
    "$$\n",
    "J(\\theta) = f(x, \\theta)\n",
    "$$\n",
    "where:\n",
    "- $ x $ is the system state,\n",
    "- $ \\theta $ represents parameters to be optimized.\n",
    "\n",
    "The system dynamics are defined as:\n",
    "$$\n",
    "\\frac{dx}{dt} = g(x, \\theta)\n",
    "$$\n",
    "\n",
    "To compute gradients efficiently, the **adjoint equation** is introduced:\n",
    "$$\n",
    "\\frac{d\\lambda}{dt} = - \\left( \\frac{\\partial g}{\\partial x} \\right)^T \\lambda - \\frac{\\partial J}{\\partial x}\n",
    "$$\n",
    "\n",
    "where $ \\lambda $ is the **adjoint variable**.\n",
    "\n",
    "The **gradient of the objective function** with respect to the parameters is then:\n",
    "$$\n",
    "\\frac{dJ}{d\\theta} = \\int \\lambda^T \\frac{\\partial g}{\\partial \\theta} dt\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## **Why Use Adjoint-Based Optimization?**\n",
    "- **Efficient for Large-Scale Problems**: Avoids storing full gradients like automatic differentiation.\n",
    "- **Common in PDE-Constrained Optimization**: Used in optimizing physical systems governed by differential equations.\n",
    "- **Memory Efficient**: Requires storing only the adjoint state instead of full forward computations.\n",
    "\n",
    "---\n",
    "\n",
    "## **PyTorch Implementation of Adjoint-Based Optimization**\n",
    "\n",
    "This example demonstrates a **basic adjoint method** using PyTorch.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple system: dx/dt = -theta * x\n",
    "class System(nn.Module):\n",
    "    def __init__(self, theta_init=0.1):\n",
    "        super(System, self).__init__()\n",
    "        self.theta = nn.Parameter(torch.tensor(theta_init, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return -self.theta * x  # System dynamics\n",
    "\n",
    "# Define the objective function J = x^2\n",
    "def objective(x):\n",
    "    return x**2\n",
    "\n",
    "# Initialize the system\n",
    "system = System()\n",
    "optimizer = optim.Adam(system.parameters(), lr=0.01)\n",
    "\n",
    "# Initial state\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "# Forward simulation\n",
    "for _ in range(100):\n",
    "    x_next = x + system(x) * 0.1  # Euler step\n",
    "    loss = objective(x_next)  # Compute objective function\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()  # Compute adjoint gradient\n",
    "    optimizer.step()  # Update parameters\n",
    "    x = x_next.detach().requires_grad_(True)  # Reset computational graph\n",
    "\n",
    "print(f\"Optimized theta: {system.theta.item()}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d070e9ae-9d06-409e-b491-544aeae15777",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
