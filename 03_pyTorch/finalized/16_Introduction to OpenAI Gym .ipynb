{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3a4da9a-d230-498a-a450-ee3ea9375f2f",
   "metadata": {},
   "source": [
    "## Reinforcement learning\n",
    "A brief introduction and tutorial on the __OpenAI Gym Python library__. OpenAI Gym is a powerful library for simulating and visualising the performance of Reinformcent learning.\n",
    "\n",
    "\n",
    "This part of reinforcement learning tutorials that I am currently creating. We first explain how to install OpenAI Gym by using Anaconda Python environment. Then, we introduce the Frozen Lake OpenAI Gym environment. Then, we explain how to generate random actions and how to render them. We explain how to obtain information about transition probabilities. Finally, we explain how to simulate random episodes in OpenAI Gym.\n",
    "\n",
    "https://www.gymlibrary.dev/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cf0c33-cd03-4c6c-8a2c-8f490a771075",
   "metadata": {},
   "source": [
    "reinforcement learning concepts\n",
    "\n",
    "- State: States are the fields such as from 0 to 15 in frozen lake. We always starts to enumerate state as 0.\n",
    "- Episode: A series of steps to start from inital state and end at any temrianl astate. Episode can be winning or loosing episode.\n",
    "- Terminal states: states whcih termnate the staes. It can be holes and Goal in frozen lake example.\n",
    "- Actions and Actions space: At every state, except at hole states and goal state, we can perform an action. For example, consider Fig. 2. At the state 6 which represents a frozen field, we can perform 4 actions: UP, DOWN, LEFT, and RIGHT.\n",
    "    - These actions are elements of an action space. That is, the action space consists of 4 actions: 0, 1, 2, and 3, corresponding to LEFT, DOWN, RIGHT, and UP actions.\n",
    "\n",
    "    - However, desired actions will not guarantee desired transitions. Here, we should keep in mind that due to the probabilistic nature of environment that will be explained later, certain desired actions at certain states will NOT necessarily lead us to the desired state. For example, let us say that we are at the state 6 and that we perform action DOWN. We expect to go to state 10. However, this will only happen with SOME PROBABILITY. That is, by performing this action, there is some chance that we might end up at some other neighboring state, such as for example, at the state 5. This will be explained later in the text after we introduce transition probabilities.\n",
    " \n",
    "    - In deterministic scenario we know that if its up than its up, not in other direction whcih may happen in stochastic probability.\n",
    "- Rewards: When going from one state to another, we receive a reward after we arrive at the destination state. Rewards are important since the reinforcement learning algorithm aims at finding a sequence of actions that maximize the expected sum of rewards. Here, it is important to mention that in the general case, rewards are also not deterministic. This means that the reward obtained by reaching a certain state is not in the general case deterministic. That is, we can also associate a probability with rewards. However, in the Frozen Lake case, the rewards are purely deterministic. In the Frozen Lake example, the rewards are distributed as follows:\n",
    "    - Reach frozen field (F) – obtain the reward of 0.\n",
    "    - Reach hole field (H) – obtain the reward of 0\n",
    "    - Reach goal field (G) – obtain the reward of 1\n",
    "  \n",
    "- State Transition probabilities: how we go from one state to other state. The probabilities, p_{1}, p_{2} and p_{3} are called the state transition probabilities. They are defined as follows. Let t-1 be the current discrete-time step and let t be the next time step. L\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ad29d4b-5aa3-4cdb-8989-02778e50207d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\376189\\anaconda3\\envs\\pytorch\\lib\\site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\376189\\anaconda3\\envs\\pytorch\\lib\\site-packages (from gym) (2.2.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\376189\\anaconda3\\envs\\pytorch\\lib\\site-packages (from gym) (3.1.1)\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in c:\\users\\376189\\anaconda3\\envs\\pytorch\\lib\\site-packages (from gym) (0.0.8)\n"
     ]
    }
   ],
   "source": [
    "# !pip install gym\n",
    "# !pip install gym[toy_text]\n",
    "# !pip install --proxy http://proxyout.lanl.gov:8080 gym\n",
    " # environemtn from OpEN AI Gym library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df8c9675-d643-48f3-8802-cf729ae35eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym[toy_text] in c:\\users\\376189\\anaconda3\\envs\\pytorch\\lib\\site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\376189\\anaconda3\\envs\\pytorch\\lib\\site-packages (from gym[toy_text]) (2.2.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\376189\\anaconda3\\envs\\pytorch\\lib\\site-packages (from gym[toy_text]) (3.1.1)\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in c:\\users\\376189\\anaconda3\\envs\\pytorch\\lib\\site-packages (from gym[toy_text]) (0.0.8)\n",
      "Collecting pygame==2.1.0 (from gym[toy_text])\n",
      "  Using cached pygame-2.1.0.tar.gz (5.8 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Getting requirements to build wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [118 lines of output]\n",
      "  \n",
      "  \n",
      "  WARNING, No \"Setup\" File Exists, Running \"buildconfig/config.py\"\n",
      "  Using WINDOWS configuration...\n",
      "  \n",
      "  Making dir :prebuilt_downloads:\n",
      "  Downloading... https://www.libsdl.org/release/SDL2-devel-2.0.16-VC.zip 13d952c333f3c2ebe9b7bc0075b4ad2f784e7584\n",
      "  \n",
      "  ---\n",
      "  For help with compilation see:\n",
      "      https://www.pygame.org/wiki/CompileWindows\n",
      "  To contribute to pygame development see:\n",
      "      https://www.pygame.org/contribute.html\n",
      "  ---\n",
      "  \n",
      "  Traceback (most recent call last):\n",
      "    File \u001b[35m\"C:\\Users\\376189\\Anaconda3\\envs\\pytorch\\Lib\\urllib\\request.py\"\u001b[0m, line \u001b[35m1319\u001b[0m, in \u001b[35mdo_open\u001b[0m\n",
      "      \u001b[31mh.request\u001b[0m\u001b[1;31m(req.get_method(), req.selector, req.data, headers,\u001b[0m\n",
      "      \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "                \u001b[1;31mencode_chunked=req.has_header('Transfer-encoding'))\u001b[0m\n",
      "                \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    File \u001b[35m\"C:\\Users\\376189\\Anaconda3\\envs\\pytorch\\Lib\\http\\client.py\"\u001b[0m, line \u001b[35m1338\u001b[0m, in \u001b[35mrequest\u001b[0m\n",
      "      \u001b[31mself._send_request\u001b[0m\u001b[1;31m(method, url, body, headers, encode_chunked)\u001b[0m\n",
      "      \u001b[31m~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    File \u001b[35m\"C:\\Users\\376189\\Anaconda3\\envs\\pytorch\\Lib\\http\\client.py\"\u001b[0m, line \u001b[35m1384\u001b[0m, in \u001b[35m_send_request\u001b[0m\n",
      "      \u001b[31mself.endheaders\u001b[0m\u001b[1;31m(body, encode_chunked=encode_chunked)\u001b[0m\n",
      "      \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    File \u001b[35m\"C:\\Users\\376189\\Anaconda3\\envs\\pytorch\\Lib\\http\\client.py\"\u001b[0m, line \u001b[35m1333\u001b[0m, in \u001b[35mendheaders\u001b[0m\n",
      "      \u001b[31mself._send_output\u001b[0m\u001b[1;31m(message_body, encode_chunked=encode_chunked)\u001b[0m\n",
      "      \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    File \u001b[35m\"C:\\Users\\376189\\Anaconda3\\envs\\pytorch\\Lib\\http\\client.py\"\u001b[0m, line \u001b[35m1093\u001b[0m, in \u001b[35m_send_output\u001b[0m\n",
      "      \u001b[31mself.send\u001b[0m\u001b[1;31m(msg)\u001b[0m\n",
      "      \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^\u001b[0m\n",
      "    File \u001b[35m\"C:\\Users\\376189\\Anaconda3\\envs\\pytorch\\Lib\\http\\client.py\"\u001b[0m, line \u001b[35m1037\u001b[0m, in \u001b[35msend\u001b[0m\n",
      "      \u001b[31mself.connect\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "      \u001b[31m~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "    File \u001b[35m\"C:\\Users\\376189\\Anaconda3\\envs\\pytorch\\Lib\\http\\client.py\"\u001b[0m, line \u001b[35m1479\u001b[0m, in \u001b[35mconnect\u001b[0m\n",
      "      self.sock = \u001b[31mself._context.wrap_socket\u001b[0m\u001b[1;31m(self.sock,\u001b[0m\n",
      "                  \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^\u001b[0m\n",
      "                                            \u001b[1;31mserver_hostname=server_hostname)\u001b[0m\n",
      "                                            \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    File \u001b[35m\"C:\\Users\\376189\\Anaconda3\\envs\\pytorch\\Lib\\ssl.py\"\u001b[0m, line \u001b[35m455\u001b[0m, in \u001b[35mwrap_socket\u001b[0m\n",
      "      return \u001b[31mself.sslsocket_class._create\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "             \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "          \u001b[1;31msock=sock,\u001b[0m\n",
      "          \u001b[1;31m^^^^^^^^^^\u001b[0m\n",
      "      ...<5 lines>...\n",
      "          \u001b[1;31msession=session\u001b[0m\n",
      "          \u001b[1;31m^^^^^^^^^^^^^^^\u001b[0m\n",
      "      \u001b[1;31m)\u001b[0m\n",
      "      \u001b[1;31m^\u001b[0m\n",
      "    File \u001b[35m\"C:\\Users\\376189\\Anaconda3\\envs\\pytorch\\Lib\\ssl.py\"\u001b[0m, line \u001b[35m1076\u001b[0m, in \u001b[35m_create\u001b[0m\n",
      "      \u001b[31mself.do_handshake\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "      \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "    File \u001b[35m\"C:\\Users\\376189\\Anaconda3\\envs\\pytorch\\Lib\\ssl.py\"\u001b[0m, line \u001b[35m1372\u001b[0m, in \u001b[35mdo_handshake\u001b[0m\n",
      "      \u001b[31mself._sslobj.do_handshake\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "      \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  \u001b[1;35mssl.SSLCertVerificationError\u001b[0m: \u001b[35m[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: Basic Constraints of CA cert not marked critical (_ssl.c:1028)\u001b[0m\n",
      "  \n",
      "  During handling of the above exception, another exception occurred:\n",
      "  \n",
      "  Traceback (most recent call last):\n",
      "    File \u001b[35m\"C:\\Users\\376189\\Anaconda3\\envs\\pytorch\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\"\u001b[0m, line \u001b[35m389\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "      \u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "      \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "    File \u001b[35m\"C:\\Users\\376189\\Anaconda3\\envs\\pytorch\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\"\u001b[0m, line \u001b[35m373\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "      json_out[\"return_val\"] = \u001b[31mhook\u001b[0m\u001b[1;31m(**hook_input[\"kwargs\"])\u001b[0m\n",
      "                               \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    File \u001b[35m\"C:\\Users\\376189\\Anaconda3\\envs\\pytorch\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\"\u001b[0m, line \u001b[35m143\u001b[0m, in \u001b[35mget_requires_for_build_wheel\u001b[0m\n",
      "      return hook(config_settings)\n",
      "    File \u001b[35m\"C:\\Users\\376189\\AppData\\Local\\Temp\\pip-build-env-j9dv63ow\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m334\u001b[0m, in \u001b[35mget_requires_for_build_wheel\u001b[0m\n",
      "      return \u001b[31mself._get_build_requires\u001b[0m\u001b[1;31m(config_settings, requirements=[])\u001b[0m\n",
      "             \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    File \u001b[35m\"C:\\Users\\376189\\AppData\\Local\\Temp\\pip-build-env-j9dv63ow\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m304\u001b[0m, in \u001b[35m_get_build_requires\u001b[0m\n",
      "      \u001b[31mself.run_setup\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "      \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "    File \u001b[35m\"C:\\Users\\376189\\AppData\\Local\\Temp\\pip-build-env-j9dv63ow\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m522\u001b[0m, in \u001b[35mrun_setup\u001b[0m\n",
      "      \u001b[31msuper().run_setup\u001b[0m\u001b[1;31m(setup_script=setup_script)\u001b[0m\n",
      "      \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    File \u001b[35m\"C:\\Users\\376189\\AppData\\Local\\Temp\\pip-build-env-j9dv63ow\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m320\u001b[0m, in \u001b[35mrun_setup\u001b[0m\n",
      "      \u001b[31mexec\u001b[0m\u001b[1;31m(code, locals())\u001b[0m\n",
      "      \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m388\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    File \u001b[35m\"C:\\Users\\376189\\AppData\\Local\\Temp\\pip-install-deqmlcia\\pygame_1f22ea09b1f54279a6da59e3f00aa105\\buildconfig\\config.py\"\u001b[0m, line \u001b[35m234\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "      deps = CFG.main(**kwds)\n",
      "    File \u001b[35m\"C:\\Users\\376189\\AppData\\Local\\Temp\\pip-install-deqmlcia\\pygame_1f22ea09b1f54279a6da59e3f00aa105\\buildconfig\\config_win.py\"\u001b[0m, line \u001b[35m497\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "      and \u001b[31mdownload_win_prebuilt.ask\u001b[0m\u001b[1;31m(**download_kwargs)\u001b[0m:\n",
      "          \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    File \u001b[35m\"C:\\Users\\376189\\AppData\\Local\\Temp\\pip-install-deqmlcia\\pygame_1f22ea09b1f54279a6da59e3f00aa105\\buildconfig\\download_win_prebuilt.py\"\u001b[0m, line \u001b[35m290\u001b[0m, in \u001b[35mask\u001b[0m\n",
      "      \u001b[31mupdate\u001b[0m\u001b[1;31m(x86=x86, x64=x64)\u001b[0m\n",
      "      \u001b[31m~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    File \u001b[35m\"C:\\Users\\376189\\AppData\\Local\\Temp\\pip-install-deqmlcia\\pygame_1f22ea09b1f54279a6da59e3f00aa105\\buildconfig\\download_win_prebuilt.py\"\u001b[0m, line \u001b[35m273\u001b[0m, in \u001b[35mupdate\u001b[0m\n",
      "      \u001b[31mdownload_prebuilts\u001b[0m\u001b[1;31m(download_dir, x86=x86, x64=x64)\u001b[0m\n",
      "      \u001b[31m~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    File \u001b[35m\"C:\\Users\\376189\\AppData\\Local\\Temp\\pip-install-deqmlcia\\pygame_1f22ea09b1f54279a6da59e3f00aa105\\buildconfig\\download_win_prebuilt.py\"\u001b[0m, line \u001b[35m124\u001b[0m, in \u001b[35mdownload_prebuilts\u001b[0m\n",
      "      \u001b[31mdownload_sha1_unzip\u001b[0m\u001b[1;31m(url, checksum, temp_dir, 1)\u001b[0m\n",
      "      \u001b[31m~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    File \u001b[35m\"C:\\Users\\376189\\AppData\\Local\\Temp\\pip-install-deqmlcia\\pygame_1f22ea09b1f54279a6da59e3f00aa105\\buildconfig\\download_win_prebuilt.py\"\u001b[0m, line \u001b[35m54\u001b[0m, in \u001b[35mdownload_sha1_unzip\u001b[0m\n",
      "      response = \u001b[31murllib.urlopen\u001b[0m\u001b[1;31m(request)\u001b[0m.read()\n",
      "                 \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^\u001b[0m\n",
      "    File \u001b[35m\"C:\\Users\\376189\\Anaconda3\\envs\\pytorch\\Lib\\urllib\\request.py\"\u001b[0m, line \u001b[35m189\u001b[0m, in \u001b[35murlopen\u001b[0m\n",
      "      return \u001b[31mopener.open\u001b[0m\u001b[1;31m(url, data, timeout)\u001b[0m\n",
      "             \u001b[31m~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    File \u001b[35m\"C:\\Users\\376189\\Anaconda3\\envs\\pytorch\\Lib\\urllib\\request.py\"\u001b[0m, line \u001b[35m489\u001b[0m, in \u001b[35mopen\u001b[0m\n",
      "      response = self._open(req, data)\n",
      "    File \u001b[35m\"C:\\Users\\376189\\Anaconda3\\envs\\pytorch\\Lib\\urllib\\request.py\"\u001b[0m, line \u001b[35m506\u001b[0m, in \u001b[35m_open\u001b[0m\n",
      "      result = self._call_chain(self.handle_open, protocol, protocol +\n",
      "                                '_open', req)\n",
      "    File \u001b[35m\"C:\\Users\\376189\\Anaconda3\\envs\\pytorch\\Lib\\urllib\\request.py\"\u001b[0m, line \u001b[35m466\u001b[0m, in \u001b[35m_call_chain\u001b[0m\n",
      "      result = func(*args)\n",
      "    File \u001b[35m\"C:\\Users\\376189\\Anaconda3\\envs\\pytorch\\Lib\\urllib\\request.py\"\u001b[0m, line \u001b[35m1367\u001b[0m, in \u001b[35mhttps_open\u001b[0m\n",
      "      return \u001b[31mself.do_open\u001b[0m\u001b[1;31m(http.client.HTTPSConnection, req,\u001b[0m\n",
      "             \u001b[31m~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "                          \u001b[1;31mcontext=self._context)\u001b[0m\n",
      "                          \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    File \u001b[35m\"C:\\Users\\376189\\Anaconda3\\envs\\pytorch\\Lib\\urllib\\request.py\"\u001b[0m, line \u001b[35m1322\u001b[0m, in \u001b[35mdo_open\u001b[0m\n",
      "      raise URLError(err)\n",
      "  \u001b[1;35murllib.error.URLError\u001b[0m: \u001b[35m<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: Basic Constraints of CA cert not marked critical (_ssl.c:1028)>\u001b[0m\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "Getting requirements to build wheel did not run successfully.\n",
      "exit code: 1\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "# !pip install --proxy http://proxyout.lanl.gov:8080 gym[toy_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cea6301-8f6f-4c14-8e8e-75bda8be2064",
   "metadata": {},
   "source": [
    "# Frozen Lake Environment in Python\n",
    "Frozen Lake Environment in Python contains 16 fiels starting from 0 to 15/ It is partitioned in 4x4 grid. \n",
    "S-> Start field \\\n",
    "F -> Frozen \\\n",
    "H -> Hole field \\\n",
    "G-> Goal field : find path from start , going through frozen  field and reacg to goal. \\\n",
    "H and G are called terminate field. \n",
    "\n",
    "We can only step left, right, up and down. So the objective is to find a path from start to goal only  \\\n",
    "Frozen Lake Environment in a completely stochastic envirenment. This means there will some operation whicih may not lead to desired goal. this menas there is transition probabaility asspocitated with that particula operation and inital stae.\n",
    "In deterministic approach we mostly find the path.\n",
    "\n",
    "In reality is also fpossible because in real case frozen lake is slipry and applying a particular operation may slip to right , left o rdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a80d294-8706-40a5-aa44-045a322159ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Fri Nov 11 14:32:39 2022\n",
    "\n",
    "Demonstration of the OpenAI Gym Library \n",
    "and the Fronzen Lake reinforcement learning environment\n",
    "\n",
    "@author: Aleksandar Haber \n",
    "\n",
    "Website accompanying this code with background information\n",
    "and theoretical explanations is given here:\n",
    "\n",
    "https://aleksandarhaber.com/introduction-to-state-transition-probabilities-actions-and-rewards-with-openai-gym-reinforcement-learning-tutorial/\n",
    "\n",
    "\"\"\"\n",
    "import gym\n",
    "# specify forzen lake version, specify render mode: human\n",
    "env=gym.make(\"FrozenLake-v1\", render_mode=\"human\")\n",
    "# Reset the environemnt\n",
    "env.reset()\n",
    "# render the environment\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd0f222-b30c-4861-b76d-dce9c8881163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate our environemnt\n",
    "# observation space - states \n",
    "env.observation_space # dimentions of observation space. We have 16 states. Observation state consit of all the states whcih we can observe\n",
    "\n",
    "# Action sapce consit 4 actions\n",
    "# actions: left -0, down - 1, right - 2, up- 3\n",
    "env.action_space # action space refers to possible actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2538e08-8e70-41d9-b6b3-ce336cba99bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate random action froma set {0, down - 1, right - 2, up- 3}\n",
    "randomAction= env.action_space.sample()\n",
    "\n",
    "# Apply random action\n",
    "returnValue = env.step(randomAction)\n",
    "# format of returnValue is (observation,reward, terminated, truncated, info)\n",
    "# observation (object)  - observed state\n",
    "# reward (float)        - reward that is the result of taking the action\n",
    "# terminated (bool)     - is it a terminal state\n",
    "# truncated (bool)      - it is not important in our case\n",
    "# info (dictionary)     - in our case transition probability\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176ce8a9-dab7-4684-8873-d4fd3e462624",
   "metadata": {},
   "source": [
    "in Reinforcement learning setup, every action from certain state will give us certain reward and these rewards can be randomly defined\n",
    "or they can be user defined. For example if you're over here and if we step over here we should obtain a certain reward this reward \n",
    "can be negative positive or it can be zero. Generally speaking more favorable the actions are we should obtain a betterreward or a\n",
    "higher value of reward.\n",
    "what is a favorable action or what is a favorable step the favorable step or more favorable step is a step that will that will lead us closer to our goal\n",
    "however in the Frozen Lake environment all the rewards are mapped like this so the rewards or the reward by stepping at the frozen field\n",
    "easier the reward  that we obtained by stepping at the whole is also zero and the reward obtained by stepping at the goal field is one rewards are very important in reinforcement learning because the whole purpose of the reinforcement learning algorithm is to design a policy or a set of actions that will maximize the\n",
    " expected number or the expected sum of rewards obtained from a certain State until the final state that's why the rewards are very important \n",
    "\n",
    " Transition probabailyt: this means there was basically a chance of one over three to go back to this field from the original field by \n",
    " applying the action that's in in our case is random action and the value of the random action is zero so there is one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccc523c-0eaf-4d10-8b4d-f59ddb47ac09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# perform deterministic step 0,1,2,3\n",
    "returnValue = env.step(1)\n",
    "\n",
    "\n",
    "# reset the environment and return to intial position.\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae89474-a8aa-47a5-b019-cdb57176fcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transition probabilities\n",
    "#p(s'|s,a) probability of going to state s' \n",
    "#          starting from the state s and by applying the action a\n",
    "\n",
    "# env.P[state][action]\n",
    "env.P[0][1]  # access all the probabailities 1st argument: state, second argument: action\n",
    "# output is a list having the following entries\n",
    "# (transition probability, next state, reward, Is terminal state?)\n",
    "\n",
    "# close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bede4cd9-dfef-47dc-bd84-accaf14bf71b",
   "metadata": {},
   "source": [
    "### Episode in Python\n",
    "how to simulate an episode in Python \n",
    "\n",
    "however let us go back to our original graph over here and let us explain what is an episode. An episode is a sequence of actions and corresponding fields that start from S and that and at any of the terminal States. For example this can be an episode we are in this state we apply action down we go to state number four then we basically apply any of the actions down right however there is certain probability that that we will reach over here and if it really happens that we reach the hole this would be one episode. \n",
    "\n",
    "let us illustrate another episode so the episode can also be this episode and this is a successful or the winning episode since we start at the initial State we go like this and we end at our terminal State and in this case the terminal state is the goal state.\n",
    "\n",
    "let us try to simulate this case that is let us try to simulate a complete stochastic process and the stochastic nature of our frozen lake\n",
    "environment so to do that we first need to close our environment don't forget to do that and let us erase our workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a2cfbf-a3ef-4421-8829-6e78dcf17e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Nov  9 21:29:24 2022\n",
    "\n",
    "@author: Aleksandar Haber\n",
    "\n",
    "This code demonstrates how to simulate episodes in OpenAI Gym\n",
    "\n",
    "\"\"\"\n",
    "import gym\n",
    "import time\n",
    "env=gym.make(\"FrozenLake-v1\",render_mode='human')\n",
    "env.reset()\n",
    "env.render()\n",
    "print('Initial state of the system')\n",
    "\n",
    "\n",
    "numberOfIterations = 30 # generate number of simulaiton\n",
    "\n",
    "for i in range(numberOfIterations):\n",
    "    randomAction= env.action_space.sample()\n",
    "    returnValue=env.step(randomAction)\n",
    "    env.render() # i dont need to render because \n",
    "    print('Iteration: {} and action {}'.format(i+1,randomAction))\n",
    "    time.sleep(2)\n",
    "    if returnValue[2]: # if the state is terminal state\n",
    "        break\n",
    "\n",
    "env.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a34d240-37a2-4d6f-9ae8-871db491add5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Nov  9 21:29:24 2022\n",
    "\n",
    "@author: Aleksandar Haber\n",
    "\n",
    "This code demonstrates how to simulate episodes in OpenAI Gym\n",
    "\n",
    "\"\"\"\n",
    "import gym\n",
    "import time\n",
    "env=gym.make(\"FrozenLake-v1\",render_mode='human')\n",
    "env.reset()\n",
    "env.render()\n",
    "print('Initial state of the system')\n",
    "\n",
    "\n",
    "numberOfIterations = 30 # generate number of simulaiton\n",
    "\n",
    "for i in range(numberOfIterations):\n",
    "    randomAction= env.action_space.sample()\n",
    "    returnValue=env.step(randomAction)\n",
    "    env.render() # i dont need to render because \n",
    "    print('Iteration: {} and action {}'.format(i+1,randomAction))\n",
    "    time.sleep(2)\n",
    "    if returnValue[2]: # if the state is terminal state\n",
    "        break\n",
    "\n",
    "# if Break reset environemtn  and start problem  again\n",
    "env.reset()\n",
    "\n",
    "# it may dicversging because we do not have any otimil policy in tis reinformcement learning\n",
    "env.close()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
