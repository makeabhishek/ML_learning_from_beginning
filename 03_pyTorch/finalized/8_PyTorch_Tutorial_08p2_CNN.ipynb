{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d595a2e-2e53-457d-a282-dabc43b59567",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network Intro \n",
    "<center><img src='./images/cnn_example.PNG' width=800px></center> \n",
    "\n",
    "### Image Filter / Image Kernel \n",
    "- First, we do the convolution to the image to extract features. We create a filter/kernel (whcih uis basically a matrix) and apply convolution.\n",
    "- In this kernel the values are designed by NN to extract features. NN will learn those and convolute with image.\n",
    "- CNN is usually detect the edges (like sobel filter-right, left, top, bottom)\n",
    "- In colour images we have 3 colour channels (RGB) with hights and widht of the image.\n",
    "- After pooling we have fully connected layer.\n",
    "\n",
    "<center><img src='./images/cnn_kernel.PNG' width=600px></center> \n",
    "\n",
    "Ref: Visualize the convolution: https://setosa.io/ev/image-kernels/\n",
    "\n",
    "### Why to use CNN instead of ANN or FCNN?\n",
    "- When the data becaomes very big, in ANN all neurons are fully connected. Its diffilcult to process data. However in CNN its not fully connected. it is locally connected.\n",
    "- Once we extract feateures we do pooling tor reduce feature\n",
    "- Once we do this process we than use fully connected layer after flattening.\n",
    "- CNN is crunching the parameters down by doing convolution using filetering and further by pooling.\n",
    "\n",
    "<center><img src='./images/cnn_local_connect.PNG' width=600px></center>  \n",
    "\n",
    "### Pooling Layer\n",
    "- Reduce the features further. It reduces the amount of data in an image by combining information from multiple vectors into fewer vectors\n",
    "<center><img src='./images/pool_concept.jpg' width=600px></center>  \n",
    "\n",
    "<center><img src='./images/pooling.PNG' width=600px></center>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f7669c-d507-4d30-bf51-b2473e0ea249",
   "metadata": {},
   "source": [
    "## Dataset: MNIST\n",
    "- It has hand written numbers from 0 to 9.\n",
    "- 60,000 images in the training set. 10,000 images for testing\n",
    "- It has 28x28 pixels in an array. white = 0 , black =1.\n",
    "- The images in the dataset are binarized, normalized, and centered to remove unnecessary noise and variations.\n",
    "- Machine learning algorithms, such as neural networks or support vector machines, are used to classify the images into their respective categories (0-9).\n",
    "- So images have border, whcih is gray scale and have values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0552a9a-5fe3-48fa-b44f-029f596658e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88644570-0e09-414b-a747-e1ff1d719293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beforem we import data we whave to transform the data. We need tensor of 4 dimension in order to do things. \n",
    "# to keep track of number of images, height, widhtm and colour\n",
    "\n",
    "# convert MNIST image files into tensor of 4-dimensions (# images, height, Width, Color)\n",
    "transform = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea39d38-f022-4745-a49f-ab088941b5a4",
   "metadata": {},
   "source": [
    "# 1. Download MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afac3fb5-d7f9-42f2-ad7f-e7d8c79e795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the MNIST data save the training and test data\n",
    "# Train Data\n",
    "train_data = datasets.MNIST(root='cnn_data', train=True, download=True, transform=transform) # save locally so give any directory name\n",
    "print(train_data)\n",
    "\n",
    "# test data. just train=False\n",
    "test_data = datasets.MNIST(root='cnn_data', train=False, download=True, transform=transform) \n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c41100-aff0-4444-8a46-72ca051b61f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check where the data is \n",
    "pwd # remeber this to come back tho this directory\n",
    "ls # you will not see the cnn data in the directory because it save data one directory behind\n",
    "\n",
    "# -------------------------------------\n",
    "cd ../\n",
    "pwd\n",
    "ls\n",
    "\n",
    "cd cnn_data\n",
    "ls\n",
    "\n",
    "# cd ../ # Come back to original directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4253f8-5d79-47af-bb2c-6287d083afb7",
   "metadata": {},
   "source": [
    "# 2. Data Loader\n",
    "2. Convolutional and Pooling Layers\n",
    "lets look nitty grity of CNN.We want to upoad images in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732b524a-b47d-4d7d-82a7-5f81bdabb785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batch of 10 images\n",
    "train_loader = DataLoader(train_data, batch_size=10, shuffle =True)\n",
    "test_loader = DataLoader(train_data, batch_size=10, shuffle =False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faad27af-fef2-4c12-a97e-44261ee7b533",
   "metadata": {},
   "source": [
    "# Understand the parts of CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b348f2a0-4ccf-4389-b4f0-94db1c7143b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CNN Model\n",
    "# Describe convolutional layer and whats it doing (c convolutional layer)\n",
    "\n",
    "# lets do it only for 1 image, 3 filters, 3 kernel size and stride =1. We can set padding to make size same\n",
    "conv1 = nn.Conv2d(1, 6, 3, 1) # the output of this layer is 6 neurons, whcih we will pass as input to send con layer\n",
    "conv2 = nn.Conv2d(6, 16, 3, 1)\n",
    "\n",
    "# grab `1` MNIST image\n",
    "for i, (X_Train, y_train) in enumerate(train_data):\n",
    "    break\n",
    "\n",
    "# Check the shape of image. \n",
    "print('X_Train Shape = ',X_Train.shape)\n",
    "\n",
    "# convert image to 4D. 1 batch, 1 image, height, width\n",
    "x = X_Train.view(1, 1, 28, 28)\n",
    "\n",
    "#perform our first Convolution\n",
    "x = F.relu(conv1(x)) \n",
    "print('Shape of x after Conv1 = ', x.shape)# con1 is applied to image. \n",
    "# >>torch.Size([1,6,26,26]) 1 image, 6 filters, after convolution the size is 26x26\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62772314-b318-4ae6-b039-956cde03154b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass pooling layer of kernelsize =2, stride =2\n",
    "x = F.max_pool2d(x,2,2)\n",
    "print('Shape of x after pooling = ', x.shape)\n",
    "# >>torch.Size([1,6,13,13]) 26/2 = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605a4947-e83c-44e7-acda-2139fa43bdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do second conolutional layer and poling layer\n",
    "x = F.relu(conv2(x)) \n",
    "print('Shape of x after Conv2 = ', x.shape)\n",
    "\n",
    "# Apply poling\n",
    "x = F.max_pool2d(x,2,2)\n",
    "print('Shape of x after pooling = ', x.shape) # the size is round down instead of round up. ((28-2)/2 -2)/2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebddda44-6390-4358-85eb-d332f94b42ff",
   "metadata": {},
   "source": [
    "# Model setup to do all the studd automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dc4af1-1c15-4297-8441-7aa2a2fc9922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model class\n",
    "class ConvolutionalNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.conv2D(1,6,3,1)\n",
    "        self.conv2 = nn.conv2D(6,16,3,1)\n",
    "        # Fully connected layer. 3 Fullly connected layer\n",
    "        self.fc1 = nn.Linear(5*5&16, 120) # 120 arbitrary neurons\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10) # output is 10 or 10 classes. So make sure this is correct\n",
    "\n",
    "    def forward(self, X):\n",
    "        # First pass\n",
    "        X = F.relu(conv1(X))\n",
    "        X = F.max_pool2d(X,2,2) # 2x2 kernel and strid=2\n",
    "        # second pass\n",
    "        X = F.relu(conv2(X))\n",
    "        X = F.max_pool2d(X,2,2)  # output of this is of size 16*5*5\n",
    "\n",
    "        # Flatten the output\n",
    "        X = X.view(-1, 16*5*5) # -1 so we can vary the batch size\n",
    "\n",
    "        # Fully connected layer\n",
    "        X = F.relu(self.fc1(X))\n",
    "        X = F.relu(self.fc2(X))\n",
    "        X = self.fc3(X) # No relu on the lst layer\n",
    "\n",
    "        return F.log_softmax(X, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f5fc05-2b4a-4ee0-bd38-8e75431d91b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of model\n",
    "torch.manual_seed(41)\n",
    "model = ConvolutionalNetwork()\n",
    "\n",
    "# Print model to check the architecture\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4bcc9b-6831-43df-9835-00fab87aa801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "criterion = nn.crossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaf1fda-5e47-4509-9e5f-15d7b8aa7946",
   "metadata": {},
   "source": [
    "# Train and Test CNN Model\n",
    "https://www.youtube.com/watch?v=dGLPvNhjs4U&list=PLCC34OHNcOtpcgR9LEYSdi9r7XIbpkpK1&index=17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754d2ba7-6723-4265-ac6c-b77478d64c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Create Variables To Tracks Things\n",
    "epochs = 5\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_correct = []\n",
    "test_correct = []\n",
    "\n",
    "# For Loop of Epochs\n",
    "for i in range(epochs):\n",
    "  trn_corr = 0\n",
    "  tst_corr = 0\n",
    "\n",
    "  # Train\n",
    "  for b,(X_train, y_train) in enumerate(train_loader):\n",
    "    b+=1 # start our batches at 1\n",
    "    y_pred = model(X_train) # get predicted values from the training set. Not flattened 2D\n",
    "    loss = criterion(y_pred, y_train) # how off are we? Compare the predictions to correct answers in y_train\n",
    "\n",
    "    predicted = torch.max(y_pred.data, 1)[1] # add up the number of correct predictions. Indexed off the first point\n",
    "    batch_corr = (predicted == y_train).sum() # how many we got correct from this batch. True = 1, False=0, sum those up\n",
    "    trn_corr += batch_corr # keep track as we go along in training.\n",
    "\n",
    "    # Update our parameters\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    # Print out some results\n",
    "    if b%600 == 0:\n",
    "      print(f'Epoch: {i}  Batch: {b}  Loss: {loss.item()}')\n",
    "\n",
    "  train_losses.append(loss)\n",
    "  train_correct.append(trn_corr)\n",
    "\n",
    "\n",
    "  # Test\n",
    "  with torch.no_grad(): #No gradient so we don't update our weights and biases with test data\n",
    "    for b,(X_test, y_test) in enumerate(test_loader):\n",
    "      y_val = model(X_test)\n",
    "      predicted = torch.max(y_val.data, 1)[1] # Adding up correct predictions\n",
    "      tst_corr += (predicted == y_test).sum() # T=1 F=0 and sum away\n",
    "\n",
    "\n",
    "  loss = criterion(y_val, y_test)\n",
    "  test_losses.append(loss)\n",
    "  test_correct.append(tst_corr)\n",
    "\n",
    "\n",
    "\n",
    "current_time = time.time()\n",
    "total = current_time - start_time\n",
    "print(f'Training Took: {total/60} minutes!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ccabb0-2988-4022-a1f1-0bda8b7803b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the loss at epoch\n",
    "train_losses = [tl.item() for tl in train_losses]  #First try without this and see error. convet tensor to python list\n",
    "plt.plot(train_losses, label=\"Training Loss\")\n",
    "plt.plot(test_losses, label=\"Validation Loss\")\n",
    "plt.title(\"Loss at Epoch\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cbeaba-c097-4d22-ac82-49c7ff5362fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy at the end of each epoch\n",
    "plt.plot([t/600 for t in train_correct], label=\"Training Accuracy\")\n",
    "plt.plot([t/100 for t in test_correct], label=\"Validation Accuracy\")\n",
    "plt.title(\"Accuracy at the end of each Epoch\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aacb9a2-b7fa-444f-871d-058e378702a3",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d98c650-324c-4618-97a9-c0f36cc08727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test data\n",
    "test_load_everything = DataLoader(test_data, batch_size=10000, shuffle=False) # load all the test images\n",
    "\n",
    "with torch.no_grad():\n",
    "  correct = 0\n",
    "  for X_test, y_test in test_load_everything:\n",
    "    y_val = model(X_test)\n",
    "    predicted = torch.max(y_val, 1)[1]\n",
    "    correct += (predicted == y_test).sum()\n",
    "\n",
    "# Did for correct\n",
    "print('Total Correct predictions out of',str(len(test_data)) + correct.item())\n",
    "print('%age correct prediction', correct.item()/len(test_data)*100)\n",
    "correct.item()/len(test_data)*100\n",
    "# >> 9873 correct out of 10000 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313befaa-b14a-4939-8ebd-51b365bf2b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab an image: to check both image and label\n",
    "test_data[4143] # Tensor with an image in it...at end, it shows the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01595ff3-a642-4c13-823f-b310f5f4d0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab just the data: we don't need the label\n",
    "test_data[4143][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b845c71-8a2f-459d-ac15-e09bb5788641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data\n",
    "test_data[4143][0].reshape(28,28)\n",
    "\n",
    "# Show the image\n",
    "plt.imshow(test_data[1978][0].reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9e13cf-30a7-41cc-a12a-28845d25144c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Prediction the image with out trained model. Pass the image thru our model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  new_prediction = model(test_data[1978][0].view(1,1,28,28)) # batch size of 1, 1 color channel, 28x28 image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f44693a-88d3-4df2-9d4c-9dacde011b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the new prediction...get probabilities\n",
    "new_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ef3ceb-32b1-49f3-9fd6-e179956c668a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prediction.argmax()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
