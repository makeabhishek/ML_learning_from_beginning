{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cea9b52f-ecf7-45d3-975e-23ae7e8c9f4c",
   "metadata": {},
   "source": [
    "Cross-entropy loss is commonly used in Convolutional Neural Networks (CNNs) when solving classification problems. Here’s when you should use it:\n",
    "\n",
    "1. Multi-Class Classification\n",
    "When your CNN is used for multi-class classification (e.g., classifying images into one of several categories).\n",
    "The output layer should have a softmax activation function to produce probabilities for each class.\n",
    "Example: Classifying handwritten digits (0–9) in the MNIST dataset.\n",
    "\n",
    "2. Binary Classification\n",
    "When your CNN is used for binary classification (e.g., cat vs. dog).\n",
    "The output layer should have a sigmoid activation function to output a probability between 0 and 1.\n",
    "In this case, binary cross-entropy (log loss) is used instead of categorical cross-entropy.\n",
    "Example: Detecting if an X-ray image shows pneumonia (yes/no).\n",
    "\n",
    "3. Multi-Label Classification\n",
    "When an image can belong to multiple categories at the same time.\n",
    "The output layer should have sigmoid activation for each class instead of softmax.\n",
    "Example: An image of a scene containing both a dog and a car—both labels should be predicted.\n",
    "\n",
    "__Mathematical Formulation__\n",
    "\n",
    "- Categorical Cross-Entropy (Multi-Class)\n",
    "\n",
    "$$L=-\\sum_{i=1}^{N} y_i log(\\hat{y_i})$$\n",
    "where $y_i$  is the true label (one-hot encoded), and \\hat{y_i}is the predicted probability for class \n",
    "\n",
    "- Binary Cross-Entropy (Binary & Multi-Label)\n",
    "**Formula:**\n",
    "$$\n",
    "L = - \\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ y_i $ is the true label (0 or 1).\n",
    "- $ \\hat{y}_i $ is the predicted probability.\n",
    "\n",
    "### Why Use the Logarithm?\n",
    "1. Probability-Based Interpretation\n",
    "In classification, we model the likelihood of the correct class. The log function is used because multiplying small probabilities leads to numerical underflow. Taking the logarithm transforms these probabilities into a summation, making computation stable.\n",
    "\n",
    "2. Penalizing Confident Wrong Predictions More Heavily\n",
    "\n",
    "If the true label is \\( y = 1 \\) and the predicted probability \\( \\hat{y} \\) is close to 0, the loss becomes very large:\n",
    "\n",
    "$$\n",
    "L = -\\log(\\hat{y}) \\rightarrow \\text{large penalty when } \\hat{y} \\approx 0\n",
    "$$\n",
    "\n",
    "- Conversely, if \\( y = 0 \\) and \\( \\hat{y} \\approx 1 \\), the term \\( \\log(1 - \\hat{y}) \\) also results in a large penalty:\n",
    "- This prevents the model from confidently making incorrect predictions.\n",
    "\n",
    "3. Ensuring Convexity for Optimization\n",
    "    - The log function ensures the loss function is convex, which helps gradient-based optimization methods like stochastic gradient descent (SGD) converge efficiently.\n",
    "4. Connection to Maximum Likelihood Estimation (MLE)\n",
    "    - The Binary Cross-Entropy loss is equivalent to the negative log-likelihood of a Bernoulli-distributed variable.\n",
    "    - Minimizing BCE is equivalent to maximizing the likelihood of correct classifications.\n",
    "\n",
    "Binary Cross-Entropy (BCE) has a log loss because it is derived from the logarithm of probabilities in probabilistic modeling. This ensures that the loss function properly penalizes incorrect predictions while being convex and differentiable.\n",
    "- It penalizes incorrect confident predictions heavily.\n",
    "- It makes the function convex and differentiable for optimization.\n",
    "- It aligns with the principle of Maximum Likelihood Estimation (MLE).\n",
    "\n",
    "### When Not to Use Cross-Entropy\n",
    "- Regression Problems: Use Mean Squared Error (MSE) or Mean Absolute Error (MAE) instead.\n",
    "- Unsupervised Learning: Cross-entropy loss is not directly applicable in clustering or autoencoders unless using a classification component.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6085d76-a806-4dca-8913-90589d6db44a",
   "metadata": {},
   "source": [
    "# Alternative Loss Functions to Binary Cross-Entropy (BCE) in PyTorch\n",
    "\n",
    "## 1. Mean Squared Error (MSE) Loss\n",
    "**Formula**:\n",
    "$$\n",
    "L = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "**When to use**:\n",
    "- Used for regression tasks but sometimes used for classification.\n",
    "- Less sensitive to outliers.\n",
    "\n",
    "**PyTorch Example**:\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "y_true = torch.tensor([1.0, 0.0, 1.0])\n",
    "y_pred = torch.tensor([0.9, 0.1, 0.8])\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "loss = mse_loss(y_pred, y_true)\n",
    "print(\"MSE Loss:\", loss.item())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Hinge Loss\n",
    "**Formula**:\n",
    "$$\n",
    "L = \\sum_{i=1}^{N} \\max(0, 1 - y_i \\hat{y}_i)\n",
    "$$\n",
    "\n",
    "**When to use**:\n",
    "- Used in SVMs and margin-based classification.\n",
    "- Requires labels in {-1, +1} format.\n",
    "\n",
    "**PyTorch Example**:\n",
    "```python\n",
    "y_true = torch.tensor([1, -1, 1], dtype=torch.float32)\n",
    "y_pred = torch.tensor([0.8, -0.5, 0.6], dtype=torch.float32)\n",
    "\n",
    "hinge_loss = nn.HingeEmbeddingLoss()\n",
    "loss = hinge_loss(y_pred, y_true)\n",
    "print(\"Hinge Loss:\", loss.item())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Focal Loss\n",
    "**Formula**:\n",
    "$$\n",
    "L = - \\alpha (1 - \\hat{y})^\\gamma y \\log(\\hat{y}) - (1 - \\alpha) \\hat{y}^\\gamma (1 - y) \\log(1 - \\hat{y})\n",
    "$$\n",
    "\n",
    "**When to use**:\n",
    "- For handling imbalanced datasets by down-weighting easy examples.\n",
    "\n",
    "**PyTorch Example**:\n",
    "```python\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        probas = torch.sigmoid(inputs)\n",
    "        loss = self.alpha * (1 - probas) ** self.gamma * bce_loss\n",
    "        return loss.mean()\n",
    "\n",
    "y_true = torch.tensor([1.0, 0.0, 1.0])\n",
    "y_pred = torch.tensor([0.9, 0.1, 0.8])\n",
    "\n",
    "focal_loss = FocalLoss()\n",
    "loss = focal_loss(y_pred, y_true)\n",
    "print(\"Focal Loss:\", loss.item())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Log-Cosh Loss\n",
    "**Formula**:\n",
    "$$\n",
    "L = \\sum_{i=1}^{N} \\log(\\cosh(y_i - \\hat{y}_i))\n",
    "$$\n",
    "\n",
    "**When to use**:\n",
    "- A robust alternative to MSE that is less sensitive to outliers.\n",
    "\n",
    "**PyTorch Example**:\n",
    "```python\n",
    "class LogCoshLoss(nn.Module):\n",
    "    def forward(self, inputs, targets):\n",
    "        return torch.mean(torch.log(torch.cosh(inputs - targets)))\n",
    "\n",
    "y_true = torch.tensor([1.0, 0.0, 1.0])\n",
    "y_pred = torch.tensor([0.9, 0.1, 0.8])\n",
    "\n",
    "log_cosh_loss = LogCoshLoss()\n",
    "loss = log_cosh_loss(y_pred, y_true)\n",
    "print(\"Log-Cosh Loss:\", loss.item())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Kullback-Leibler (KL) Divergence Loss\n",
    "**Formula**:\n",
    "$$\n",
    "L = \\sum_{i=1}^{N} y_i \\log \\frac{y_i}{\\hat{y}_i}\n",
    "$$\n",
    "\n",
    "**When to use**:\n",
    "- For comparing probability distributions.\n",
    "\n",
    "**PyTorch Example**:\n",
    "```python\n",
    "y_true = torch.tensor([0.8, 0.1, 0.1])\n",
    "y_pred = torch.tensor([0.7, 0.2, 0.1])\n",
    "\n",
    "kl_loss = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "loss = kl_loss(torch.log(y_pred), y_true)\n",
    "print(\"KL Divergence Loss:\", loss.item())\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0b221d1-61c2-4227-8d04-f4c369761e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss: 0.020000001415610313\n",
      "Hinge Loss: 0.9666666984558105\n",
      "Focal Loss: 0.019345112144947052\n",
      "Log-Cosh Loss: 0.009950477629899979\n",
      "KL Divergence Loss: 0.012503479607403278\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "y_true = torch.tensor([1.0, 0.0, 1.0], dtype=torch.float32)\n",
    "y_pred = torch.tensor([0.9, 0.1, 0.8], dtype=torch.float32)\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "loss = mse_loss(y_pred, y_true)\n",
    "print(\"MSE Loss:\", loss.item())\n",
    "\n",
    "#----------------------------------------------------------#\n",
    "# Requires labels in {-1, +1} format.\n",
    "y_true = torch.tensor([1, -1, 1], dtype=torch.float32)\n",
    "y_pred = torch.tensor([0.8, -0.5, 0.6], dtype=torch.float32)\n",
    "\n",
    "hinge_loss = nn.HingeEmbeddingLoss()\n",
    "loss = hinge_loss(y_pred, y_true)\n",
    "print(\"Hinge Loss:\", loss.item())\n",
    "\n",
    "#----------------------------------------------------------#\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        probas = torch.sigmoid(inputs)\n",
    "        loss = self.alpha * (1 - probas) ** self.gamma * bce_loss\n",
    "        return loss.mean()\n",
    "\n",
    "y_true = torch.tensor([1.0, 0.0, 1.0])\n",
    "y_pred = torch.tensor([0.9, 0.1, 0.8])\n",
    "\n",
    "focal_loss = FocalLoss()\n",
    "loss = focal_loss(y_pred, y_true)\n",
    "print(\"Focal Loss:\", loss.item())\n",
    "\n",
    "#----------------------------------------------------------#.\n",
    "\n",
    "class LogCoshLoss(nn.Module):\n",
    "    def forward(self, inputs, targets):\n",
    "        return torch.mean(torch.log(torch.cosh(inputs - targets)))\n",
    "\n",
    "y_true = torch.tensor([1.0, 0.0, 1.0])\n",
    "y_pred = torch.tensor([0.9, 0.1, 0.8])\n",
    "\n",
    "log_cosh_loss = LogCoshLoss()\n",
    "loss = log_cosh_loss(y_pred, y_true)\n",
    "print(\"Log-Cosh Loss:\", loss.item())\n",
    "\n",
    "\n",
    "#----------------------------------------------------------#\n",
    "\n",
    "y_true = torch.tensor([0.8, 0.1, 0.1])\n",
    "y_pred = torch.tensor([0.7, 0.2, 0.1])\n",
    "\n",
    "kl_loss = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "loss = kl_loss(torch.log(y_pred), y_true)\n",
    "print(\"KL Divergence Loss:\", loss.item())\n",
    "\n",
    "\n",
    "#----------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9477065-14ab-40f4-a8f5-f8115945db36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
