{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d595a2e-2e53-457d-a282-dabc43b59567",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    " transfer learning and how this can be implemented in PyTorch.\n",
    "\n",
    "We will Implement:\n",
    "- What is Transfer Learning\n",
    "- Use the pretrained ResNet-18 model\n",
    "- Apply transfer learning to classify ants and bees\n",
    "- Exchange the last fully connected layer\n",
    "- Try 2 methods: Finetune the whole network or train only the last layer\n",
    "- Evaluate the results\n",
    "\n",
    "Transfer learning is a ML method where a model developed for the first task is reused as the starting point for the second task. For example we can create a model to classify birds, dogs and than use the same maodel modifield a littlebit at the last layer . Than use the new model to classify bees, ants. this is a popular approach in deeplearnign that allows rapid generation of new models. Important because trainign a completely new model is expensive. If we use a pretrained model we can exchange only the last layer and do not need to train the model again.\n",
    "\n",
    "<center><img src='./images/transferLearn.PNG' width=800px></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161e3dab-9615-44c3-8687-d3b3fab7ca44",
   "metadata": {},
   "source": [
    "Example: \n",
    "- Here we are usign pretrained ResNet18 CNN.  This is a model which was trianed on millions of images on imagenet database. \n",
    "- This modela is 18 layers deep and can classify images into 100 object categories.\n",
    "- In our example we only have two calss. So we only want to detect bees and ants\n",
    "\n",
    "We will also see how can we use\n",
    "- ImageFolder: Datasets\n",
    "- Scheduler: to change the learnign rate\n",
    "- Transfer Learning: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8a3421-b3ef-445b-ba15-1bcff40abf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2968ee-1e75-405f-a38b-8b2788d282b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the data: \n",
    "mean = np.array([0.5, 0.5, 0.5])\n",
    "std = np.array([0.25, 0.25, 0.25])\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Here we have saved our data in an imageFolder. We have a folder. Than subfolder train and val. In each sub folder \n",
    "# we have folders for bees and ants whcih contains bees and ants images.\n",
    "data_dir = 'data/hymenoptera_data'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                             shuffle=True, num_workers=0)\n",
    "              for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(class_names)\n",
    "\n",
    "\n",
    "def imshow(inp, title):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "imshow(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bd05e6-5d5d-4881-94f4-662889e53bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainign Loop\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaae308c-0457-479d-bd66-c850c29f5788",
   "metadata": {},
   "source": [
    "# transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03735f26-25ec-469a-9403-e45a65b0f332",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Finetuning the convnet ####\n",
    "# Load a pretrained model and reset final fully connected layer.\n",
    "\n",
    "model = models.resnet18(pretrained=True) # Avalailable in torchvision.models module\n",
    "\n",
    "# Excahnge the last fully connected layers. Lets take number of input featerues from last layer.\n",
    "num_ftrs = model.fc.in_features\n",
    "# Here the size of each output sample is set to 2.\n",
    "\n",
    "# Create a new layer and assign values\n",
    "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "model.fc = nn.Linear(num_ftrs, 2) # number of outous are 2 for our case. ants or bees.\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2458c376-a5ae-4737-8678-dd1027f8e916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and loss fucniton\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2a485b-3ada-4800-b59a-353de22c76aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scheduler: to update learnign rate\n",
    "step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1) # every 7 epocs our learning rate is multiplied by 0.1 that is 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ac4c5c-dc45-48e3-8201-6862d5bd74e9",
   "metadata": {},
   "source": [
    "This is how can we use transfer learning. \n",
    "In first case we use a technique called 'fine tuning' because here we train the whole model again but only a little bit. That means we are fine tunig the weights with new data and with new last layer\n",
    "\n",
    "Second option is to freeze all the layers in the begineeing and only train the last layer. So we ahve to loop until we get parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4871cc6-1b7e-435f-a10a-3bece55da868",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------#\n",
    "# StepLR Decays the learning rate of each parameter group by gamma every step_size epochs\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "# Learning rate scheduling should be applied after optimizerâ€™s update\n",
    "# e.g., you should write your code this way:\n",
    "\n",
    "# for epoch in range(100):\n",
    "#     train(...)\n",
    "#     validate(...) or evaluate\n",
    "#     scheduler.step()\n",
    "#---------------------------------------------------------------------------------------#\n",
    "\n",
    "#this is we have create above\n",
    "model = train_model(model, criterion, optimizer, step_lr_scheduler, num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3296e984-3ef1-4c39-9b63-86dc096f3eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ConvNet as fixed feature extractor ####\n",
    "# Here, we need to freeze all the network except the final layer.\n",
    "# We need to set requires_grad == False to freeze the parameters so that the gradients are not computed in backward()\n",
    "model_conv = torchvision.models.resnet18(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 2) # setup new last layer\n",
    "\n",
    "model_conv = model_conv.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c008bf3-57d7-4ef9-a4ec-ad63160369d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as\n",
    "# opposed to before.\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n",
    "\n",
    "model_conv = train_model(model_conv, criterion, optimizer_conv,\n",
    "                         exp_lr_scheduler, num_epochs=25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
