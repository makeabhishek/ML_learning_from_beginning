{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af3f8239-9c08-4d6d-8a03-29d63f30b5c5",
   "metadata": {},
   "source": [
    "## Recurrent Neural Net (RNN)\n",
    "https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html\n",
    "RNN's are a class of NN that allows previous outputs to be used as inputs while having hidden states\n",
    "\n",
    "<center><img src='./images/rnn.PNG' width=450px></center> \n",
    "\n",
    "- Here we have an image of simple RNN. We have some inputs tha it do some operations and get hidden states. We take those hidden state and use them in next stage. So we can use our previous knowledge to get new hidden state and than get output.\n",
    "- Let's unfold the graph and see what is happening. basically we are doing sequence of perations\n",
    "- Example: we have a sentence and we might  use every single work as an input.\n",
    "<center><img src='./images/rnn_1.PNG' width=450px></center> \n",
    "\n",
    "### Why RNN's are important?\n",
    "- RNN's allow us to operate on sequences of vectors.\n",
    "- With traditional NN we have one to one relationship (example image classification), but with RNN's we can work with sequences. There are differnt types. We can have squenceu in input and output. \n",
    "The Unreasonable Effectiveness of Recurrent Neural Networks: https://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "Recurrent Neural Networks cheatsheet: https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks#architecture\n",
    "\n",
    "<center><img src='./images/rnn_2.PNG' width=450px></center> \n",
    "\n",
    "Advantages\n",
    "- Possibility of processing input of any length\n",
    "- Model size not increasing with size of input\n",
    "- Computation takes into account historical information\n",
    "- Weights are shared across time\n",
    "\n",
    "Drawbacks\n",
    "- Computation being slow\n",
    "- Difficulty of accessing information from a long time ago\n",
    "- Cannot consider any future input for the current state\n",
    "\n",
    "## Dataset\n",
    "### Name Classification Using A Recurrent Neural Net\n",
    "- We have dataset with differnt filles contain names. We have differnt last names from differn countries. We have 18 differnt countries.\n",
    "- We want to classigfy this and detect whcih country the name is?\n",
    "- We take the whole name as a sequence and than put each single letter in RNN as an input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db71e6a-126e-47ac-b462-cb063d3541cd",
   "metadata": {},
   "source": [
    "## utility funcitons to process characters names.\n",
    "Lets create a helper function to take the whole name as a sequence and than put each single letter in RNN as an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97685835-3232-4bd2-a124-3c21ca58eba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data: https://download.pytorch.org/tutorial/data.zip\n",
    "import io\n",
    "import os\n",
    "import unicodedata\n",
    "import string\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# alphabet small + capital letters + \" .,;'\"\n",
    "ALL_LETTERS = string.ascii_letters + \" .,;'\"\n",
    "N_LETTERS = len(ALL_LETTERS)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in ALL_LETTERS\n",
    "    )\n",
    "# For example remove any special character from the name and only have ASCII characters. `print(unicode_to_ascii('Ślusàrski'))`\n",
    "\n",
    "# helper function to load data. Load all the files and all the names.\n",
    "def load_data():\n",
    "    # Build the category_lines dictionary, a list of names per language\n",
    "    category_lines = {}\n",
    "    all_categories = []\n",
    "    \n",
    "    def find_files(path):\n",
    "        return glob.glob(path)\n",
    "    \n",
    "    # Read a file and split into lines\n",
    "    def read_lines(filename):\n",
    "        lines = io.open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "        return [unicode_to_ascii(line) for line in lines]\n",
    "    \n",
    "    for filename in find_files('data/data_name_classification/names/*.txt'):\n",
    "        category = os.path.splitext(os.path.basename(filename))[0]\n",
    "        all_categories.append(category)\n",
    "        \n",
    "        lines = read_lines(filename)\n",
    "        category_lines[category] = lines\n",
    "        \n",
    "    return category_lines, all_categories\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "To represent a single letter, we use a “one-hot vector” of \n",
    "size <1 x n_letters>. A one-hot vector is filled with 0s\n",
    "except for a 1 at index of the current letter, e.g. \"b\" = <0 1 0 0 0 ...>.\n",
    "\n",
    "To make a word we join a bunch of those into a\n",
    "2D matrix <line_length x 1 x n_letters>.\n",
    "\n",
    "That extra 1 dimension is because PyTorch assumes\n",
    "everything is in batches - we’re just using a batch size of 1 here.\n",
    "\"\"\"\n",
    "\n",
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letter_to_index(letter):\n",
    "    return ALL_LETTERS.find(letter)\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letter_to_tensor(letter):\n",
    "    tensor = torch.zeros(1, N_LETTERS)\n",
    "    tensor[0][letter_to_index(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# one hot encoding. We need a way to display our data that can be used for training. One hot encoding fill all the values with zeros except for the index of one letter.\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def line_to_tensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, N_LETTERS)\n",
    "    for i, letter in enumerate(line):\n",
    "        tensor[i][0][letter_to_index(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def random_training_example(category_lines, all_categories):\n",
    "    \n",
    "    def random_choice(a):\n",
    "        random_idx = random.randint(0, len(a) - 1)\n",
    "        return a[random_idx]\n",
    "    \n",
    "    category = random_choice(all_categories)\n",
    "    line = random_choice(category_lines[category])\n",
    "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "    line_tensor = line_to_tensor(line)\n",
    "    return category, line, category_tensor, line_tensor\n",
    "\n",
    "\"\"\"\n",
    "# If writing in .py file\n",
    "if __name__ == '__main__':\n",
    "    print(ALL_LETTERS)\n",
    "    print(unicode_to_ascii('Ślusàrski'))\n",
    "    \n",
    "    category_lines, all_categories = load_data()\n",
    "    print(category_lines['Italian'][:5])\n",
    "    \n",
    "    print(letter_to_tensor('J')) # [1, 57]\n",
    "    print(line_to_tensor('Jones').size()) # [5, 1, 57]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a28dc4-bf93-453f-83d7-4d4b52ca40ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(ALL_LETTERS)\n",
    "print(unicode_to_ascii('Ślusàrski'))\n",
    "\n",
    "category_lines, all_categories = load_data()\n",
    "print(category_lines['Italian'][:5]) # return a dictionary with countary as key and the corresponding names as values\n",
    "\n",
    "print(letter_to_tensor('J')) # [1, 57]\n",
    "print(line_to_tensor('Jones').size()) # [5, 1, 57] 5: number of characters, 57: number of all differnt characters, 1: becuase pytorch expect this form of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df85250-7b18-4208-8794-67d43489a41a",
   "metadata": {},
   "source": [
    "### Implement RNN\n",
    "RNN for name classification. We have input and hidden state than internally we combined tensor and apply two differnt hidden layer. input to output and input to hidden layers. They are two differnt linear layers. Than have one hidden output from i2h whcih we use for the next input. We also get a output from i2o, since we are doing a multiclass classification we apply softmax layer and get output.\n",
    "<center><img src='./images/rnn_architect.PNG' width=450px></center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c818e931-308e-4c65-9d0f-9ca653b62048",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# from utils import ALL_LETTERS, N_LETTERS\n",
    "# from utils import load_data, letter_to_tensor, line_to_tensor, random_training_example\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    # implement RNN from scratch rather than using nn.RNN\n",
    "    def __init__(self, input_size, hidden_size, output_size): # hyper parameter: hidden size,\n",
    "        # in the init method lets first create super \n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size) # because we have combined so input_size + hidden_size\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1) #1,57 because our input is of shape 1x57 and we need second dimension\n",
    "        \n",
    "    def forward(self, input_tensor, hidden_tensor):\n",
    "        combined = torch.cat((input_tensor, hidden_tensor), 1)\n",
    "        \n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self): # helper function to get initial hidden state in the starting.\n",
    "        return torch.zeros(1, self.hidden_size) # we create empty zero tensor ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5581ed0e-10cd-46ed-bde1-06bb555df166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "category_lines, all_categories = load_data() # disctionalr with country as key and names as vlaues\n",
    "n_categories = len(all_categories)\n",
    "print(n_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098a60fc-a5c4-48c5-9d96-de0ac5e4bdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden size and output size (i.e categories) for RNN\n",
    "n_hidden = 128 # hyperparameter\n",
    "rnn = RNN(N_LETTERS, n_hidden, n_categories)\n",
    "\n",
    "# one step. For an example let's do one single step\n",
    "input_tensor = letter_to_tensor('A')\n",
    "hidden_tensor = rnn.init_hidden()\n",
    "\n",
    "output, next_hidden = rnn(input_tensor, hidden_tensor)\n",
    "print(output.size()) # We get new output and new hidden state of certian size. n_categories\n",
    "print(next_hidden.size()) # we get same size as we deifned n_hidden = 128 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c84445-2196-423c-a094-7bb73728fa48",
   "metadata": {},
   "source": [
    "Now we want to treat the name as one sequence and than each singel character is one single input. So we repetedly apply the RNN for all the characters in the name and than at the very end we take the last output and apply `Softmax` and take the vlaue with the highest probabaility\n",
    " lets do this for one name.\n",
    " \n",
    "<center><img src='./images/rnn_1.PNG' width=450px></center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120549a5-3c84-4aec-97ff-f089f0047697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# whole sequence/name. \n",
    "input_tensor = line_to_tensor('Albert')\n",
    "hidden_tensor = rnn.init_hidden()\n",
    "\n",
    "output, next_hidden = rnn(input_tensor[0], hidden_tensor)\n",
    "print(output.size())\n",
    "print(next_hidden.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a2bf1e-7e32-4b4d-9f65-ef6420602b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to repetedly apply the above prcess\n",
    "def category_from_output(output):\n",
    "    category_idx = torch.argmax(output).item() # we applied softmax, and want to get index of maximum likelihood.\n",
    "    return all_categories[category_idx]\n",
    "\n",
    "print(category_from_output(output))\n",
    "\n",
    "# model is not trained so we may not get correct value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe0d8c7-29a7-4534-b060-52943e418c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the \n",
    "\n",
    "# define loss and optimizer\n",
    "criterion = nn.NLLLoss()\n",
    "learning_rate = 0.005\n",
    "optimizer = torch.optim.SGD(rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define a function. this will be a one step\n",
    "def train(line_tensor, category_tensor):\n",
    "    hidden = rnn.init_hidden()\n",
    "\n",
    "    # we watn to do reptadly\n",
    "    for i in range(line_tensor.size()[0]): # length of the name\n",
    "        output, hidden = rnn(line_tensor[i], hidden) # current character and previous hidden state\n",
    "\n",
    "    # we et final output and thatn calculate the loss\n",
    "    loss = criterion(output, category_tensor)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8e03fd-9316-44c2-b811-de4ce007d286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop.\n",
    "# Lets track some values for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "plot_steps, print_steps = 1000, 5000\n",
    "\n",
    "n_iters = 100000\n",
    "\n",
    "for i in range(n_iters):\n",
    "    # get random training samples, line means actual name\n",
    "    category, line, category_tensor, line_tensor = random_training_example(category_lines, all_categories)\n",
    "\n",
    "    # Call training funciton\n",
    "    output, loss = train(line_tensor, category_tensor)\n",
    "    current_loss += loss  # add loss to current loss\n",
    "\n",
    "    # print some information after every plot_steps\n",
    "    if (i+1) % plot_steps == 0:\n",
    "        all_losses.append(current_loss / plot_steps)\n",
    "        current_loss = 0\n",
    "        \n",
    "    if (i+1) % print_steps == 0:\n",
    "        guess = category_from_output(output)\n",
    "        correct = \"CORRECT\" if guess == category else f\"WRONG ({category})\"\n",
    "        print(f\"{i+1} {(i+1)/n_iters*100} {loss:.4f} {line} / {guess} {correct}\")\n",
    "        \n",
    "# plot the losses\n",
    "plt.figure()\n",
    "plt.plot(all_losses)\n",
    "plt.show()\n",
    "\n",
    "# We can save the model here and use it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cefb6b4-a2b8-481f-9057-962c9d27a45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def predict(input_line):\n",
    "    print(f\"\\n> {input_line}\") # raw \n",
    "    with torch.no_grad(): # turn off the gradients\n",
    "        line_tensor = line_to_tensor(input_line) # raw to tensor\n",
    "        \n",
    "        hidden = rnn.init_hidden()\n",
    "    \n",
    "        for i in range(line_tensor.size()[0]): \n",
    "            output, hidden = rnn(line_tensor[i], hidden) # new output and hidden state by applying RNN.\n",
    "        \n",
    "        guess = category_from_output(output)\n",
    "        print(guess) # if its correct or not\n",
    "\n",
    "        # you can print accuracy.\n",
    "        \n",
    "\n",
    "\n",
    "while True:\n",
    "    sentence = input(\"Input:\")\n",
    "    if sentence == \"quit\":\n",
    "        break\n",
    "    \n",
    "    predict(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
