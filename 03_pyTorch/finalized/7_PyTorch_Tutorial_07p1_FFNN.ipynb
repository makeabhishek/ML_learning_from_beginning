{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "16b919e8-9c65-47bd-b7eb-b83927eb86a3",
   "metadata": {},
   "source": [
    "# Feed-Forward Neural Network\n",
    "we will implement our first multilayer neural network that can do digit classification based on the famous MNIST dataset.\n",
    "\n",
    "We put all the things from the last tutorials together:\n",
    "- Dataset: MNISt for digit classification\n",
    "- DataLoader, TransfomramtionUse the DataLoader to load our dataset and apply a transform to the dataset\n",
    "- Multilayer Neural Net: Implement a feed-forward neural net with input layer, hidden layer, and output layer\n",
    "- Activation funciton: Apply activation functions.\n",
    "- loss and optimizer: Set up loss and optimizer\n",
    "- Training loop: that can use batch training.\n",
    "- Model Evaluation: Evaluate our model and calculate the accuracy.\n",
    "-GPU Support:  Additionally, we will make sure that our whole code can also run on the gpu if we have gpu support.\n",
    "\n",
    "### MNIST Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a156b31-fbd6-4760-b6bd-0663d20c08e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision # for datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters \n",
    "input_size = 784 # 28x28 (Image Size) --> than we flatten \n",
    "hidden_size = 100 \n",
    "num_classes = 10 # we have 10 digits from 0 to 9. 10 classes\n",
    "num_epochs = 2\n",
    "batch_size = 100 \n",
    "learning_rate = 0.001\n",
    "\n",
    "# 1. Download train and test data \n",
    "# 2. Load train and test data\n",
    "\n",
    "# 1. MNIST dataset: Import dataset from PyTorch library `torchvision.datasets`\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data',   # create a folder data\n",
    "                                           train=True,      # Training data \n",
    "                                           transform=transforms.ToTensor(),  # transform to tensor\n",
    "                                           download=True)   # Download if its not available\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor()) # Now dont need to download anymore\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False) # shuffle flase because it doen't matter for evaluation\n",
    "\n",
    "# Load the data: Lets look one batch of data\n",
    "examples = iter(train_loader)\n",
    "samples, labels = examples.next()\n",
    "print(samples.shape, labels.shape)\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.imshow(samples[i][0], cmap='gray') # [i][0] 0 because we want to extract 1st channel\n",
    "plt.show()\n",
    "\n",
    "\n",
    "examples = iter(test_loader)\n",
    "# unpack \n",
    "# samples, lables = examples.next()\n",
    "# print(samples.shape, lables.shape)\n",
    "example_data, example_targets = next(examples)\n",
    "print(example_data.shape, example_targets.shape)\n",
    "\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.imshow(example_data[i][0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54023034-73db-4e9d-b30c-9127765a205d",
   "metadata": {},
   "source": [
    "### Create a  Fully connected neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb81414-a6d4-42cd-a057-197f9a4821dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected neural network with one hidden layer\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes): # output_size = num_classes\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        # Create the layers\n",
    "        self.l1 = nn.Linear(input_size, hidden_size) \n",
    "        # After layer apply activation function\n",
    "        self.relu = nn.ReLU()\n",
    "        # than we have another linear layer. Input size = hidden size and output size = output classes\n",
    "        self.l2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x): # `x` is one sample\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)   # no activation and no softmax at the end. We will apply cross-entropy whcih will aplly softmax\n",
    "        return out\n",
    "\n",
    "# model = NeuralNet(input_size, hidden_size, num_classes)\n",
    "model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3fbcdc-074e-43f5-9ea2-ab206994a1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss() # this will apply SoftMax for us automatically\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50428272-7fc9-48f0-91d2-34d99cceea57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop: Train the model\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    # run over all batches\n",
    "    for i, (images, labels) in enumerate(train_loader):  # enumerate function will give the actual index and the data. The data here is the tuples of (images, labels)\n",
    "        # Reshape the images: \n",
    "        # origin shape: [100, 1, 28, 28]\n",
    "        # resized: [100, 784], number of batches and flatten image\n",
    "        images = images.reshape(-1, 28*28).to(device) # -1 tensor will automatically find the dimension for us\n",
    "        labels = labels.to(device) \n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad() # to empty the values gradeitns attribute\n",
    "        loss.backward()\n",
    "        optimizer.step() # update step to update parameters for us\n",
    "\n",
    "        # Print the information\n",
    "        if (i+1) % 100 == 0: # every 100 step\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b99563-ae5e-486d-a2b4-f16eea514457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing and evaluation of the model: for this we dont need to calcualte gradeints that all we do\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency). Wrap this using `with`\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    # loop over all the abtches \n",
    "    for images, labels in test_loader: # same as we did for training\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        # calcualte prediction. model is the trained model\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # torch.max returns (value ,index)\n",
    "        _, predicted = torch.max(outputs.data, 1) # we dont need first actual value whcih are classes\n",
    "        n_samples += labels.size(0) # number of samples in cuurent batch\n",
    "        n_correct += (predicted == labels).sum().item() # for each correct prediction we add one\n",
    "\n",
    "    # Calcualte total accuracy in percent %\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network on the 10000 test images: {acc} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
