{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf81c5e6-fa4e-4df8-a836-37059d176c96",
   "metadata": {},
   "source": [
    "# Logistic/Softmax regression and Cross Entropy Loss with PyTorch\n",
    "\n",
    "https://www.youtube.com/watch?v=1fXB0Lc9RMI&list=PLLeO8f6PhlKb_FAC7qxOBtxT9-8EPDAqk&index=4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9fed23-2ff2-4573-b1ac-4e8a8a09fbd4",
   "metadata": {},
   "source": [
    "## **Logistic Regression: A Classification Algorithm**\n",
    "Logistic Regression is a **supervised learning algorithm** used for **classification tasks**. Despite its name, it is **not a regression algorithm** but rather a method for predicting **categorical outcomes** (e.g., \"yes\" or \"no\", \"spam\" or \"not spam\", etc.).\n",
    "\n",
    "---\n",
    "\n",
    "### **How Logistic Regression Works**\n",
    "Logistic Regression works by applying the **sigmoid (logistic) function** to a **linear equation**. This converts continuous values into probabilities between **0 and 1**.\n",
    "\n",
    "#### **Mathematical Representation**\n",
    "1. **Linear Combination of Features**  \n",
    "   Given input features \\(X\\), the model computes a weighted sum:\n",
    "\n",
    "   $$\n",
    "   z = W X + b\n",
    "   $$\n",
    "\n",
    "   Where:\n",
    "   - \\(X\\) = input features\n",
    "   - \\(W\\) = weights (learned during training)\n",
    "   - \\(b\\) = bias term\n",
    "\n",
    "2. **Applying the Sigmoid Function**  \n",
    "   The output \\(z\\) is passed through the **sigmoid function**:\n",
    "\n",
    "   $$\n",
    "   \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "   $$\n",
    "\n",
    "   This ensures the output is a probability between 0 and 1.\n",
    "\n",
    "3. **Decision Rule**  \n",
    "   - If \\( \\sigma(z) > 0.5 \\), classify as **1** (positive class).\n",
    "   - If \\( \\sigma(z) \\leq 0.5 \\), classify as **0** (negative class).\n",
    "\n",
    "---\n",
    "\n",
    "### **Types of Logistic Regression**\n",
    "1. **Binary Logistic Regression**  \n",
    "   - Used when there are **two** classes (e.g., spam or not spam).\n",
    "   - Example: Predicting if an email is **spam (1) or not spam (0)**.\n",
    "\n",
    "2. **Multiclass Logistic Regression (Softmax Regression)**  \n",
    "   - Used when there are **more than two** classes.\n",
    "   - Uses the **softmax function** instead of the sigmoid function.\n",
    "   - Example: Predicting if an image contains a **cat (class 0), dog (class 1), or bird (class 2)**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Loss Function: Binary Cross-Entropy**\n",
    "To measure how well the model is performing, we use the **Binary Cross-Entropy (Log Loss)**:\n",
    "\n",
    "$$\n",
    "Loss = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y_i \\log(\\hat{y_i}) + (1 - y_i) \\log(1 - \\hat{y_i}) \\right]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\( y_i \\) = actual label (0 or 1)\n",
    "- \\( \\hat{y_i} \\) = predicted probability\n",
    "- \\( m \\) = number of samples\n",
    "\n",
    "This function **penalizes incorrect predictions more heavily** when the model is confident but wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01019181-60bc-48e1-844d-1e2d24a6ed46",
   "metadata": {},
   "source": [
    "# **Example: Logistic Regression in Python**\n",
    "Using **scikit-learn** to classify whether a tumor is malignant (1) or benign (0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7046a302-fcfa-43b4-ac66-54e5c365d053",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate synthetic classification data\n",
    "X, y = make_classification(n_samples=1000, n_features=5, random_state=42)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ea0b93-039e-496a-b1ef-05b4749e8b7e",
   "metadata": {},
   "source": [
    "## We follow the same steps as Linear regression\n",
    "0. **Prepare Data**\n",
    "1. **Design Model** (input size, output size, forward pass)\n",
    "2. **Construct Loss and optimizer**\n",
    "3. **Training Loop**\n",
    "    - Forward Pass: Compute Prediction and Loss\n",
    "    - Backward Pass: Gradients\n",
    "    - Update Weights\n",
    "We have to make slight modification to linear regression. We usaulay add one more layer to our model, and select a different loss function\n",
    "\n",
    "\n",
    "## Summary of Steps:\n",
    "1. **Import Libraries**\n",
    "2. **Load & Prepare Data**\n",
    "3. **Split into Training & Testing Sets**\n",
    "4. **Preprocess Data**\n",
    "5. **Choose a Model**\n",
    "6. **Train the Model**\n",
    "7. **Make Predictions**\n",
    "8. **Evaluate Performance**\n",
    "9. **Hyperparameter Tuning (Optional)**\n",
    "10. **Deploy the Model (Optional)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23aad56c-f1c1-4fc8-ae33-9ce78db8373e",
   "metadata": {},
   "source": [
    "### Code Understanding\n",
    "\n",
    "- The model inherits from `nn.Module`, PyTorch’s base class for all neural networks.\n",
    "`self.linear = nn.Linear(n_input_features, 1):`\n",
    "\n",
    "- A single-layer model with: `n_input_features` (30 features from the dataset). 1 `output neuron` (since it’s binary classification).\n",
    "`forward(self, x):` \n",
    "- Applies linear transformation:\n",
    "$$ y=Wx+b $$\n",
    "- Applies sigmoid activation:\n",
    "$$\\sigma(y)=\\frac{1}{1+e^{-y}}$$\n",
    "- Converts raw scores to probabilities (0 to 1).\n",
    "\n",
    "`nn.BCELoss():`\n",
    "- Standard loss function for binary classification.\n",
    "- Measures the difference between predicted probabilities and actual labels.\n",
    "\n",
    "Optimizer: `torch.optim.SGD` (__Stochastic Gradient Descent)__:\n",
    "- `model.parameters():` Optimizes model weights.\n",
    "- `lr=0.01:` Learning rate controls step size in gradient updates.\n",
    "\n",
    "#### Why Do We Need to Perform Feature Scaling in Logistic Regression?\n",
    "Feature scaling is crucial for machine learning models, especially for gradient-based optimization algorithms like logistic regression. \n",
    "1. Logistic Regression Uses Gradient Descent\n",
    "    - Logistic Regression learns model parameters using Gradient Descent, which updates weights by computing gradients.\n",
    "    - If features have different scales, gradient updates will be uneven, leading to slow convergence or even failure to find the optimal solution.\n",
    "2. Helps with Convergence Speed \\\n",
    "3. Prevents Numerical Instability \\\n",
    "    - Some features might have very large values, causing issues like overflow or underflow during computation.\n",
    "    - This is especially critical when using sigmoid activation in logistic regression $\\sigma(x)=\\frac{1}{1+e^{-x}}$\n",
    "    - If x is too large, $^(-x$ approaches 0, causing numerical instability.\n",
    "4. Improves Model Performance \\\n",
    "    - Proper scaling ensures that all features contribute equally to the learning process.\n",
    "    - This can improve accuracy and generalization on new data.\n",
    "5. Required for Many Regularization Methods \\\n",
    "    - If you use $L1$ (Lasso) or $L2$ (Ridge) regularization, scaling ensures that penalization applies equally to all features.\n",
    "\n",
    "  \n",
    "- Example:\n",
    "| Feature | Range               |\n",
    "|---------|---------------------|\n",
    "| Age     | 18 - 80            |\n",
    "| Salary  | 20,000 - 200,000   |\n",
    "\n",
    "- Since \"Salary\" has much larger values, the model will be dominated by it, and \"Age\" will be ignored.\n",
    "- Without Scaling:\n",
    "    - Large values (like 200,000) will result in large weight updates.\n",
    "    - Small values (like 18) will result in tiny updates.\n",
    "    - This leads to imbalanced learning and poor optimization.\n",
    "\n",
    "### Which Scaling Method Should You Use?\n",
    "\n",
    "#### 1. Standardization (Recommended for Logistic Regression)\n",
    "$$\n",
    "X_{\\text{scaled}} = \\frac{X - \\mu}{\\sigma}\n",
    "$$\n",
    "- **Zero mean, unit variance** (centered around 0).\n",
    "- Works well for **gradient-based** models like Logistic Regression, SVM, and Neural Networks.\n",
    "- **Used in below code:**\n",
    "\n",
    "```\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "```\n",
    "\n",
    "#### Min-Max Scaling (Alternative)\n",
    "$$\n",
    "X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}}\n",
    "$$\n",
    "- Scales values between **0 and 1**.\n",
    "- Used for models that need **bounded inputs** (e.g., Neural Networks).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5ea3082-b0bf-4444-ab8c-a6979da7fbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "569 30\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ----\n",
    "# 0) Prepare data : Import Breast Cancer data from `sklearn.datasets`. It is a binary classification dataset.\n",
    "# X contains features (independent variables). y contains the target labels (0 for benign, 1 for malignant).\n",
    "bc = datasets.load_breast_cancer()\n",
    "X, y = bc.data, bc.target\n",
    "\n",
    "n_samples, n_feateures = X.shape\n",
    "print(n_samples, n_feateures) # 569 samples and 30 differnt features (quite a lot)\n",
    "\n",
    "\n",
    "# Splitting and Scaling the Data: \n",
    "# Splits 80% training and 20% testing data; random_state=1234 ensures reproducibility.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
    "\n",
    "# Feature Scaling (Standardization): Alwyas recommended when doing logistic regression\n",
    "# `StandardScaler()` scales each feature to zero mean and unit variance.\n",
    "# `fit_transform(X_train)`: Learns and applies scaling on training data.\n",
    "# `transform(X_test)`: Uses the same scaling on test data.\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# Convert to tensor\n",
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "\n",
    "# Reshape the y tensor: right now y has only one row and we want to make it as a column vector to put each value in one row\n",
    "y_train = y_train.view(y_train.shape[0], 1)\n",
    "y_test = y_test.view(y_test.shape[0], 1)\n",
    "\n",
    "# ----\n",
    "# 1) model\n",
    "# f = wx + b, sigmoid function: return the value 0 or 1.\n",
    "# Lets create our own class. call this LogisticRegression and this must be derived from nn.module. It will have __init__, whcih has self and other arguments\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, n_input_features):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        # it only has one layer, with n_input_features and output size = 1, i.e, only one class label at the end.\n",
    "        self.linear = nn.Linear(n_input_features, 1)\n",
    "    # implement the forward pass whcih has self and data\n",
    "        def forward(self, x):\n",
    "            y_predicted = torch.sigmoid(self.linear(x))\n",
    "            return y_predicted\n",
    "\n",
    "model = LogisticRegression(n_features)\n",
    "\n",
    "# ----\n",
    "# 2) Loss and Optimizer\n",
    "learning_rate = 0.01\n",
    "criterion = nn.BCELoss() # binary cross entropy loss\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "# ----\n",
    "# 3) Training Loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    # 1) Forward Pass and Loss: Computes predictions (y_predicted) and Computes loss (loss).\n",
    "    y_predicted = model(X_train)\n",
    "    loss = criterion(y_predicted, y_train)\n",
    "        \n",
    "    # 2) backward Pass: calculates gradients for all weights.\n",
    "    loss.backward()\n",
    "\n",
    "    # 3) Update Weights: updates the model's weights using the gradients.\n",
    "    optimizer.step()\n",
    "\n",
    "    # Zero the gradeints:resets gradients (otherwise, they accumulate).\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if (epoch+1) % 10 ==0: # Print loss every 10 epochs.\n",
    "        print(f'epoch: {epoch+1}, loss={loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be60a92-576e-4cd7-b6b8-dd78e499c8cd",
   "metadata": {},
   "source": [
    "### Testing or Evalaution\n",
    "#### Understand the code structure for evalauting the model\n",
    "- During training, PyTorch tracks all operations on tensors to compute gradients for backpropagation. However, in evaluation, we do not need to compute gradients, which saves memory and speeds up computations.\n",
    "- `with torch.no_grad():` disables gradient tracking for the following code block\n",
    "- `y_predicted = model(X_test)`: The test data `X_test` is passed to the model to get predictions. Since `model(X_test)` is using the `sigmoid activation function`, the output will be a probability between $0$ and $1$.\n",
    "- `y_predicted_cls = y_predicted.round()`:  The output of the model is a probability (e.g., 0.7, 0.3, 0.9, etc.). To get class labels (0 or 1), we use `.round()`, which converts: Values $\\geq 0.5$ to $1$ (positive class) and Values $< 0.5$ $0$ (negative class).\n",
    "- `acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])`:\n",
    "    - `y_predicted_cls.eq(y_test)` creates a Boolean tensor where:\n",
    "    - True $(1)$ means the prediction is correct.\n",
    "    - False $(0)$ means the prediction is incorrect.\n",
    "    - `.sum()` counts the number of correct predictions.\n",
    "    - `float(y_test.shape[0])` gets the total number of test samples.\n",
    "    - Final calculation: $$ Accuray = \\frac{correct prediction}{Tortal Samples}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad6eeff-7fe4-4cd3-9ba3-67b6699f5bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model, whcih is not the part of CG. Dont want to tract gradient\n",
    "with torch.no_grad(): #disables gradient tracking for the following code block.\n",
    "    # Get the accuracy, get all the predicted classe3s from test samples\n",
    "    y_predicted = model(X_test)\n",
    "    y_predicted_cls = y_predicted.round()\n",
    "    # Calculate accuracy\n",
    "    acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "    print(f'accuracy = {acc:.4f}') # ensures the accuracy is displayed with 4 decimal places."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7534b8ce-240a-4194-8f13-5a87e8de901e",
   "metadata": {},
   "source": [
    "### Exercise \n",
    "If accuracy is not good, play with `num_epochs`, `learning_rate` and differnt `optimizer`.\n",
    "\n",
    "Possible Improvements:\n",
    "- Try Adam (Adaptive Moment Estimation) optimizer instead of SGD (`torch.optim.Adam`).\n",
    "    - Instead of: `optimizer = torch.optim.SGD(model.parameters(), lr=0.01)`\n",
    "    - Use: `optimizer = torch.optim.Adam(model.parameters(), lr=0.001)`\n",
    "- Tune learning rate and number of epochs for better accuracy\n",
    "\n",
    "### Why Use Adam?\n",
    "- Faster Convergence\n",
    "    - Adam updates weights using adaptive learning rates, so it converges faster than vanilla SGD.\n",
    "    - It is especially useful when dealing with large datasets or high-dimensional spaces.\n",
    "- Adaptive Learning Rates (Automatic Learning Rate Adjustment)\n",
    "    - In SGD, we use a fixed learning rate, which may require manual tuning.\n",
    "    - Adam adjusts learning rates dynamically for each parameter based on the magnitude of past gradients, reducing the need for manual tuning.)\n",
    "- Less Hyperparameter Tuning Required (More Robust to Hyperparameters) \\\n",
    "    - SGD requires careful tuning of the learning rate, momentum, etc.\n",
    "    - Adam is less sensitive to hyperparameter choices and often works well with default setting\n",
    "- Better for Deep Learning and Large Datasets \n",
    "- Handles Noisy and Sparse Gradients Well\n",
    "    - SGD struggles with sparse gradients (i.e., when some features rarely update).\n",
    "    - Adam uses adaptive scaling, making it more efficient for sparse datasets and less sensitive to noisy gradients.\n",
    "- Works Well for Non-Convex Problems\n",
    "\n",
    "### Comparison Table: Adam vs. SGD\n",
    "\n",
    "| Optimizer  | Learning Rate | Convergence Speed | Handles Noisy Gradients | Works Well for Deep Learning |\n",
    "|------------|--------------|-------------------|------------------------|------------------------------|\n",
    "| **SGD**    | Fixed        | Slower            | Struggles              | Sometimes                    |\n",
    "| **Adam**   | Adaptive     | Faster            | Better                 | Yes                          |\n",
    "\n",
    "\n",
    "### When to Use Adam vs. SGD?\n",
    "| Scenario                                      | Use Adam? | Use SGD? |\n",
    "|----------------------------------------------|---------|---------|\n",
    "| **Default choice for most models**           | ✅      | ❌      |\n",
    "| **Deep learning (CNNs, RNNs, Transformers, etc.)** | ✅      | ❌      |\n",
    "| **Small datasets, simple models**            | ❌      | ✅      |\n",
    "| **Training speed is important**              | ✅      | ❌      |\n",
    "| **Fine-tuning with pre-trained models**      | ✅      | ❌      |\n",
    "| **Best final accuracy (with careful tuning)** | ❌      | ✅      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9faf729-0db0-4cef-99fc-128ecc461806",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
