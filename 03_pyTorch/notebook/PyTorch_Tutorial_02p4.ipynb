{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d33d8035-311d-4fb4-855c-8a2501fa25c6",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-size: 32px; font-weight: bold;\">\n",
    "    PyTorch Tutorial 02 - Advanced Matrix Algebra\n",
    "</div>\n",
    "\n",
    "## Matrix Operations:  Determinant & Inverse\n",
    "\n",
    "### (1) Matrix Multiplication\n",
    "In PyTorch, you can perform matrix multiplication using `torch.mm()`, `torch.matmul()`, and the `@` operator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d81418-92c0-47ea-8264-9c09f305cc2b",
   "metadata": {},
   "source": [
    "## Advanced Matrix Operations in PyTorch\n",
    "- Tensor Factorizations (CP Decomposition, Tucker Decomposition)\n",
    "- Graph-based Matrix Operations\n",
    "- Sparse Matrix Computations (PyTorch supports sparse tensors)\n",
    "\n",
    "| **Operation** | **PyTorch Function** | **Use Case** |\n",
    "|--------------|----------------------|-------------|\n",
    "| **CP Decomposition** | `parafac()` | Tensor Factorization |\n",
    "| **Tucker Decomposition** | `tucker()` | Dimensionality Reduction |\n",
    "| **Sparse Matrix Representation** | `tensor.to_sparse()` | GNNs, Large Data |\n",
    "| **Sparse Matrix Multiplication** | `torch.sparse.mm()` | High-Speed Computation |\n",
    "| **Adjacency Matrix** | `torch.tensor([[..]])` | Graph Theory |\n",
    "| **Degree Matrix** | `torch.diag(adj_matrix.sum(dim=1))` | Node Connectivity |\n",
    "| **Graph Laplacian** | `degree_matrix - adj_matrix` | Graph Signal Processing |\n",
    "| **PageRank Transition Matrix** | `torch.linalg.inv(D) @ A` | Google‚Äôs Algorithm |\n",
    "\n",
    "**These operations are essential in AI, Data Science, and Graph Analytics!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d15bf61-9aa5-45b1-bf2b-5c9b739d7a34",
   "metadata": {},
   "source": [
    "### Tensor Decompositions (Factorizing High-Dimensional Data)\n",
    "Tensor decompositions break down multi-dimensional arrays (tensors) into simpler components. This is useful in deep learning, dimensionality reduction, and recommendation systems. \\\n",
    "CP decomposition expresses a tensor as a sum of rank-1 tensors: \\\n",
    "Used in: Deep learning, multi-way data analysis, recommender systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5754ffd3-52dd-409f-ad80-9eff476d19bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorly as tl\n",
    "from tensorly.decomposition import parafac\n",
    "\n",
    "# Create a random 3D tensor\n",
    "T = torch.randn(4, 4, 4)\n",
    "\n",
    "# Perform CP decomposition (rank=2)\n",
    "factors = parafac(T.numpy(), rank=2)\n",
    "\n",
    "# Print factorized matrices\n",
    "for i, factor in enumerate(factors):\n",
    "    print(f\"Factor {i+1}:\\n\", torch.tensor(factor))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5424ed-84b3-45cf-a106-f374007c36cd",
   "metadata": {},
   "source": [
    "### Tucker Decomposition\n",
    "Tucker decomposition factorizes a tensor into a core tensor and factor matrices. \\\n",
    "Used in: Compression of deep learning models, NLP, and image processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b8f1c5-3a23-43e0-8323-9bde547292de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorly.decomposition import tucker\n",
    "\n",
    "# Perform Tucker decomposition (core + factor matrices)\n",
    "core, factors = tucker(T.numpy(), ranks=[2, 2, 2])\n",
    "\n",
    "print(\"Core Tensor:\\n\", torch.tensor(core))\n",
    "for i, factor in enumerate(factors):\n",
    "    print(f\"Factor {i+1}:\\n\", torch.tensor(factor))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d14497e-9045-466b-a202-3173974f6941",
   "metadata": {},
   "source": [
    "### Sparse Matrices (Handling Large-Scale Data)\n",
    "Sparse matrices store only nonzero elements to save memory and speed up computations. PyTorch natively supports sparse tensors. \\\n",
    "Used in: Graph Neural Networks (GNNs), Large-Scale Data Processing, Recommender Systems.\n",
    "\n",
    "\n",
    "#### Sparse Matrix Multiplication\n",
    "Why use Sparse Matrices? ‚úî Reduces memory usage\n",
    "- Speeds up computations on large-scale data\n",
    "- Essential for Graph-based models (GNNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66449a7c-065b-4d6b-b049-88c38d06c754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dense matrix\n",
    "dense_matrix = torch.tensor([[0, 0, 3], [4, 0, 0], [0, 5, 0]])\n",
    "\n",
    "# Convert to sparse tensor\n",
    "sparse_matrix = dense_matrix.to_sparse()\n",
    "\n",
    "print(\"Sparse Matrix:\\n\", sparse_matrix)\n",
    "\n",
    "# Sparse Matrix Multiplication\n",
    "# Multiply sparse matrices\n",
    "result = torch.sparse.mm(sparse_matrix, dense_matrix)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee4f218-3f9a-4ae4-b148-fb350fbda6f1",
   "metadata": {},
   "source": [
    "### Graph-Based Matrix Computations\n",
    "Graphs can be represented as adjacency matrices, which allow us to use matrix operations for graph algorithms. \\\n",
    "Used in: Graph Neural Networks (GNNs), Social Networks, Knowledge Graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4548c6-deba-4ef0-b4f9-bca73b8de8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph adjacency matrix\n",
    "adj_matrix = torch.tensor([\n",
    "    [0, 1, 0, 0],\n",
    "    [0, 0, 1, 0],\n",
    "    [0, 0, 0, 1],\n",
    "    [1, 0, 0, 0]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "print(\"Adjacency Matrix:\\n\", adj_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af50fdc-3b3d-43d1-92eb-6f914db94a0c",
   "metadata": {},
   "source": [
    "### Computing Node Connectivity (Degree Matrix)\n",
    "The degree matrix counts how many edges are connected to each node. \\\n",
    "Why Compute the Degree Matrix? ‚úî Used for Laplacian Matrices (graph signal processing)\n",
    "- Essential in PageRank, Spectral Clustering, GNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df080235-2625-40f1-b7dd-67b27925b95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Degree Matrix\n",
    "degree_matrix = torch.diag(adj_matrix.sum(dim=1))\n",
    "print(\"Degree Matrix:\\n\", degree_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1124db67-4bdd-4e07-baeb-81b577861e94",
   "metadata": {},
   "source": [
    "### Laplacian Matrix (Graph Signal Processing)\n",
    "The Laplacian matrix is computed as: \\\n",
    "$$ ùêø = ùê∑ - ùê¥ $$\n",
    "where D is the degree matrix and A is the adjacency matrix. \\\n",
    "Used in: Graph Clustering, GNNs, Diffusion Networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5af9e41-68ea-4bd6-a256-fb39d22c698f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Graph Laplacian\n",
    "laplacian_matrix = degree_matrix - adj_matrix\n",
    "print(\"Laplacian Matrix:\\n\", laplacian_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f031dc45-72e9-44e9-8b8b-f45cd08bea40",
   "metadata": {},
   "source": [
    "### Markov Chains & PageRank (Google‚Äôs Algorithm)\n",
    "PageRank is a Markov Chain where the transition matrix is computed as:\n",
    "$$ P=D^{-1}A$$\n",
    "Used in: Web Search, Network Analysis, AI Ranking Algorithms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef367d0-510b-4873-ae06-3fa0281d1c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute transition probability matrix for PageRank\n",
    "transition_matrix = torch.linalg.inv(degree_matrix) @ adj_matrix\n",
    "print(\"PageRank Transition Matrix:\\n\", transition_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf730e8f-0b6b-4e03-9b65-b29f2379171e",
   "metadata": {},
   "source": [
    "### Whats Next: Deep Learning Matrix Computations in PyTorch\n",
    "\n",
    "| **Operation** | **PyTorch Function** | **Use Case** |\n",
    "|--------------|----------------------|-------------|\n",
    "| **Linear Transformation (Y = WX + b)** | `X @ W.T + b` | Neural Networks |\n",
    "| **Activation Functions (ReLU, Sigmoid, Tanh)** | `torch.relu(x), torch.sigmoid(x)` | Deep Learning Models |\n",
    "| **Softmax (Converts Scores to Probabilities)** | `torch.nn.functional.softmax(x)` | Classification |\n",
    "| **Gradient Computation (Backpropagation)** | `tensor.backward()` | Training Models |\n",
    "| **Loss Function (MSE, Cross Entropy)** | `torch.nn.functional.mse_loss(y_pred, y_true)` | Training Optimization |\n",
    "| **CNN Convolution** | `torch.nn.functional.conv2d()` | Feature Extraction |\n",
    "| **Singular Value Decomposition (SVD)** | `torch.svd(A)` | Dimensionality Reduction |\n",
    "\n",
    "**These matrix computations power modern Deep Learning models!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785f8dfa-7067-4522-930b-368a5128e854",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
