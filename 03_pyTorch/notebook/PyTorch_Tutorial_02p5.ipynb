{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46f9463a-f2b4-45df-9125-158c2eec9394",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-size: 32px; font-weight: bold;\">\n",
    "    Deep Learning Matrix Computations in PyTorch\n",
    "</div>\n",
    "\n",
    "Deep Learning heavily relies on matrix operations for tasks like neural network training, backpropagation, and optimization. Let's explore the core matrix computations used in deep learning and how they are implemented in PyTorch.\n",
    "\n",
    "| **Operation** | **PyTorch Function** | **Use Case** |\n",
    "|--------------|----------------------|-------------|\n",
    "| **Linear Transformation (Y = WX + b)** | `X @ W.T + b` | Neural Networks |\n",
    "| **Activation Functions (ReLU, Sigmoid, Tanh)** | `torch.relu(x), torch.sigmoid(x)` | Deep Learning Models |\n",
    "| **Softmax (Converts Scores to Probabilities)** | `torch.nn.functional.softmax(x)` | Classification |\n",
    "| **Gradient Computation (Backpropagation)** | `tensor.backward()` | Training Models |\n",
    "| **Loss Function (MSE, Cross Entropy)** | `torch.nn.functional.mse_loss(y_pred, y_true)` | Training Optimization |\n",
    "| **CNN Convolution** | `torch.nn.functional.conv2d()` | Feature Extraction |\n",
    "| **Singular Value Decomposition (SVD)** | `torch.svd(A)` | Dimensionality Reduction |\n",
    "\n",
    "\n",
    "### Essential Matrix Operations in Deep Learning\n",
    "Deep learning models work with tensors (multi-dimensional matrices) and use the following operations:\n",
    "\n",
    "- Matrix Multiplication $\\rightarrow$ Used in linear layers.\n",
    "- Element-wise Operations $\\rightarrow$ Used in activation functions.\n",
    "- Gradient Computation $\\rightarrow$ Used in backpropagation.\n",
    "- Batch Operations $\\rightarrow$ Used in mini-batch training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1168e8-3351-4c33-b9b6-b2940481c8a6",
   "metadata": {},
   "source": [
    "### 1. Linear Transformations (Fully Connected Layers)\n",
    "Neural networks consist of linear transformations where:\n",
    "$$Y=WX+b$$\n",
    "- X = Input Matrix (features)\n",
    "- W = Weight Matrix (learnable parameters)\n",
    "- b = Bias Vector (learnable parameters)\n",
    "\n",
    "Used in: Fully Connected (Dense) Layers, Convolutional Layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c26a54d-aae7-4f5f-b2a8-8d562ff7a76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define input tensor (features)\n",
    "X = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "\n",
    "# Define weight matrix\n",
    "W = torch.tensor([[0.5, -1.0], [1.5, 2.0]])\n",
    "\n",
    "# Define bias vector\n",
    "b = torch.tensor([0.1, -0.2])\n",
    "\n",
    "# Compute the linear transformation Y = WX + b\n",
    "Y = X @ W.T + b  # @ is matrix multiplication\n",
    "print(\"Linear Transformation Output:\\n\", Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afdf203-dedd-4a1c-ac42-e9927929a67e",
   "metadata": {},
   "source": [
    "### Activation Functions (Element-wise Computation)\n",
    "Non-linear activations are applied element-wise to introduce non-linearity.\n",
    "\n",
    "#### Common Activation Functions\n",
    "\n",
    "1. ReLU (Rectified Linear Unit)\n",
    "$$ùëì(x)=max(0,x)$$\n",
    "\n",
    "2. Sigmoid\n",
    "$$f(x) = \\frac{1}{1+e^x}$$\n",
    "\n",
    "\n",
    "4. Tanh\n",
    "$$f(x) = \\frac{e^{x}-e^{-x}}{e^{x} + e^{-x}}$$\n",
    "\n",
    "- ReLU removes negative values.\n",
    "- Sigmoid squashes inputs to (0,1).\n",
    "- Tanh squashes to (-1,1).\n",
    "\n",
    "Used in: Activation layers in deep neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d92355-db5e-4794-bd2e-ec0ab5f05018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a tensor\n",
    "Z = torch.tensor([[1.0, -2.0, 3.0], [-1.0, 0.5, -0.5]])\n",
    "\n",
    "# Apply activation functions\n",
    "relu_output = torch.relu(Z)\n",
    "sigmoid_output = torch.sigmoid(Z)\n",
    "tanh_output = torch.tanh(Z)\n",
    "\n",
    "print(\"ReLU Output:\\n\", relu_output)\n",
    "print(\"Sigmoid Output:\\n\", sigmoid_output)\n",
    "print(\"Tanh Output:\\n\", tanh_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a9d783-c011-4fca-b37a-36487039dcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Activation Functions\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Generate values from -5 to 5\n",
    "x = torch.linspace(-5, 5, 100)\n",
    "\n",
    "# Compute activations\n",
    "relu_y = torch.relu(x)\n",
    "sigmoid_y = torch.sigmoid(x)\n",
    "tanh_y = torch.tanh(x)\n",
    "\n",
    "# Plot activation functions\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(x, relu_y, label=\"ReLU\", linewidth=2)\n",
    "plt.plot(x, sigmoid_y, label=\"Sigmoid\", linewidth=2)\n",
    "plt.plot(x, tanh_y, label=\"Tanh\", linewidth=2)\n",
    "\n",
    "plt.axhline(0, color='black', linewidth=0.5, linestyle=\"dashed\")\n",
    "plt.axvline(0, color='black', linewidth=0.5, linestyle=\"dashed\")\n",
    "\n",
    "plt.xlabel(\"Input\")\n",
    "plt.ylabel(\"Output\")\n",
    "plt.title(\"Activation Functions\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd2854d-cf7c-41ef-9700-e40920f6430e",
   "metadata": {},
   "source": [
    "### Softmax Function (Probability Distribution)\n",
    "Softmax converts raw scores into probabilities: \n",
    "$$ Softmax(z_i) = \\frac{e^{z_i}}{\\sum e^{z_i}}$$\n",
    "\n",
    "- Turns raw scores into probabilities.\n",
    "- Helps in classification (e.g., ImageNet).\n",
    "  \n",
    "Used in: Classification Problems (e.g., ImageNet, NLP Models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2060f60a-d6f3-4c0e-8d6e-1ef62023aad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define logits (raw scores)\n",
    "logits = torch.tensor([2.0, 1.0, 0.1])\n",
    "\n",
    "# Apply softmax\n",
    "softmax_output = torch.nn.functional.softmax(logits, dim=0)\n",
    "print(\"Softmax Output:\\n\", softmax_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8306f7-34f6-410c-8355-8df0d143682d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define logits (raw scores)\n",
    "logits = torch.tensor([2.0, 1.0, 0.1])\n",
    "softmax_output = torch.nn.functional.softmax(logits, dim=0)\n",
    "\n",
    "# Convert to numpy for plotting\n",
    "probabilities = softmax_output.numpy()\n",
    "\n",
    "# Plot softmax output\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar([\"Class 1\", \"Class 2\", \"Class 3\"], probabilities, color=[\"blue\", \"green\", \"red\"])\n",
    "plt.xlabel(\"Classes\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.title(\"Softmax Function Output\")\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940b842a-58ec-4a53-a185-294bf306a95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Backpropagation (Computing Gradients with Autograd)\n",
    "In deep learning, we compute gradients using automatic differentiation. \\\n",
    "Used in: Training Neural Networks (Gradient Descent, Backpropagation).\n",
    "\n",
    "- Shows how gradient descent moves towards the minimum loss.\n",
    "- Key for training neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1890bad9-0575-4f56-9e1a-156a8e5c8082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a tensor with requires_grad=True (track gradients)\n",
    "X = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# Define a simple function: Y = X^2 + 3X\n",
    "Y = X**2 + 3*X\n",
    "\n",
    "# Compute gradients (backpropagation)\n",
    "Y.sum().backward()\n",
    "\n",
    "# Print gradients (dY/dX)\n",
    "print(\"Gradients:\\n\", X.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50ec075-94a3-4adf-9fe2-6e34be76058f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize how gradient descent updates a loss function.\n",
    "# Define a simple quadratic loss function: L(x) = (x-3)^2\n",
    "def loss_function(x):\n",
    "    return (x - 3) ** 2\n",
    "\n",
    "# Gradient of loss function\n",
    "def gradient(x):\n",
    "    return 2 * (x - 3)\n",
    "\n",
    "# Gradient Descent Parameters\n",
    "x = torch.tensor(10.0, requires_grad=True)  # Initial value\n",
    "learning_rate = 0.1\n",
    "history = []\n",
    "\n",
    "# Perform Gradient Descent\n",
    "for i in range(20):\n",
    "    loss = loss_function(x)\n",
    "    history.append(x.item())  # Store x values\n",
    "    loss.backward()  # Compute gradients\n",
    "    with torch.no_grad():  # Update x\n",
    "        x -= learning_rate * x.grad\n",
    "        x.grad.zero_()  # Reset gradients\n",
    "\n",
    "# Plot loss function\n",
    "x_vals = np.linspace(-1, 10, 100)\n",
    "y_vals = loss_function(torch.tensor(x_vals))\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x_vals, y_vals, label=\"Loss Function\", linewidth=2)\n",
    "plt.scatter(history, loss_function(torch.tensor(history)), color=\"red\", label=\"Gradient Descent Steps\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Gradient Descent Optimization\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0829aadb-0606-467b-a4a3-40e256e4a8f6",
   "metadata": {},
   "source": [
    "### Loss Function Computation\n",
    "Deep learning uses loss functions to measure the difference between predictions and actual labels.\n",
    "- Example: Mean Squared Error (MSE)\n",
    "$$ MSE =\\frac{1}{N} \\sum (y_{true} - y_{pred})^2 $$\n",
    "Used in: Regression Tasks, Neural Network Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102a7ca9-8e39-4233-8253-4a3426f15d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define predicted and actual labels\n",
    "y_pred = torch.tensor([3.0, 5.0, 7.0])\n",
    "y_true = torch.tensor([2.5, 5.0, 8.0])\n",
    "\n",
    "# Compute Mean Squared Error (MSE)\n",
    "mse_loss = torch.nn.functional.mse_loss(y_pred, y_true)\n",
    "print(\"MSE Loss:\", mse_loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae80957e-d3f7-4030-aedb-f1f3c6fa1cc7",
   "metadata": {},
   "source": [
    "### Matrix Computation for Convolutional Neural Networks (CNNs)\n",
    "Convolution is implemented using matrix operations.\n",
    "- Convolution as Matrix Multiplication \\\n",
    "CNNs use the convolution operation, where a kernel (filter) slides over an input to extract features. \\\n",
    "Used in: Image Processing, CNNs, Feature Extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893d11c8-7597-42ad-b7b8-e75f6b871ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Define input image (3x3 matrix)\n",
    "image = torch.tensor([[1.0, 2.0, 3.0], \n",
    "                      [4.0, 5.0, 6.0], \n",
    "                      [7.0, 8.0, 9.0]])\n",
    "\n",
    "# Define kernel (filter)\n",
    "kernel = torch.tensor([[0.0, 1.0, 0.0], \n",
    "                       [1.0, -4.0, 1.0], \n",
    "                       [0.0, 1.0, 0.0]])\n",
    "\n",
    "# Perform 2D convolution (cross-correlation)\n",
    "output = F.conv2d(image.unsqueeze(0).unsqueeze(0), kernel.unsqueeze(0).unsqueeze(0), padding=1)\n",
    "print(\"Convolution Output:\\n\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec5c9bb-1583-45bf-8189-91287a0c4f4e",
   "metadata": {},
   "source": [
    "#### Visualizing Convolution in a CNN\n",
    "We‚Äôll apply a convolution filter (edge detection) to an image.\n",
    "\n",
    "- Shows how CNN filters extract features from images.\n",
    "- Used in object detection and image processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3810e9e1-2758-477b-8b05-cb4613712e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Load image and convert to grayscale\n",
    "image = Image.open(\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/2d/Square_200x200.svg/200px-Square_200x200.svg.png\").convert(\"L\")\n",
    "image = np.array(image, dtype=np.float32) / 255.0  # Normalize\n",
    "\n",
    "# Convert to PyTorch tensor and add batch & channel dimensions\n",
    "image_tensor = torch.tensor(image).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "# Define an edge detection filter (Sobel operator)\n",
    "sobel_filter = torch.tensor([[-1, -1, -1], \n",
    "                             [-1,  8, -1], \n",
    "                             [-1, -1, -1]], dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "# Apply convolution\n",
    "edge_detected = F.conv2d(image_tensor, sobel_filter, padding=1)\n",
    "\n",
    "# Convert output tensor to numpy\n",
    "edge_image = edge_detected.squeeze().detach().numpy()\n",
    "\n",
    "# Plot original and edge-detected images\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(edge_image, cmap=\"gray\")\n",
    "plt.title(\"Edge Detection (Sobel Filter)\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab60a2d4-5afb-4040-aa6b-e812cf43747d",
   "metadata": {},
   "source": [
    "### Eigenvalues and Singular Value Decomposition (SVD) in Deep Learning\n",
    "Eigenvalues and SVD are used in dimensionality reduction and neural network compression. \\\n",
    "SVD helps with dimensionality reduction and feature extraction.\n",
    "\n",
    "Used in: PCA, Model Compression, Feature Reduction.\n",
    "\n",
    "- Used in PCA (Principal Component Analysis).\n",
    "- Helps compress neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5affa8f8-7052-4f9c-8dc5-ce9931511f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random matrix\n",
    "A = torch.randn(5, 3)\n",
    "\n",
    "# Compute SVD\n",
    "U, S, V = torch.svd(A)\n",
    "print(\"Singular Values:\\n\", S)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4d0edc-91a2-4119-84c6-42efda5f4922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Singular Value Decomposition (SVD)\n",
    "# Create a random matrix\n",
    "A = torch.randn(10, 10)\n",
    "\n",
    "# Compute SVD\n",
    "U, S, V = torch.svd(A)\n",
    "\n",
    "# Plot singular values\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(S.numpy(), marker=\"o\", linestyle=\"dashed\", color=\"red\", label=\"Singular Values\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Singular Value Decomposition (SVD)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
