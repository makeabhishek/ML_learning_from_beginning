{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "830b71e0-351d-450a-9afd-7205866531f4",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-size: 32px; font-weight: bold;\">\n",
    "    PyTorch Tutorial 02 - Tensor Basics\n",
    "</div>\n",
    "\n",
    "In PyTorch everything is tensor. You may be aware of _arrays_ from `numpy`. Tensor can be of any dimension\n",
    "\n",
    "We should consider to learn\n",
    "\n",
    "1. Basics of PyTorch tensors\n",
    "    - Empty, Random, Zero, one, randn, tensor from list\n",
    "2. Tensor operations/manipulations and computational graphs\n",
    "    - Addition, substraction, multiplication, inplace operation, broadcasting, Concatenating, \n",
    "4. Implement deep learning models with custom data loaders and preprocessing techniques using PyTorch.\n",
    "5. Understand GPU and CPU utilization and PyTorch software infrastructure\n",
    "\n",
    "---------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# 1. Overview\n",
    "Pytorch was developed by __Facebook AI__. But now owned by _PyTorch_ foundation (like _Linux_)\n",
    "\n",
    "We should consider to learn\n",
    "1.\tBasics of _PyTorch_ tensors and __Computational Graphs__\n",
    "2.\tImplement deep learning models with custom data loaders and preprocessing techniques using PyTorch.\n",
    "3.\tUnderstand GPU and CPU utilization and PyTorch software infrastructure\n",
    "\n",
    "### Why to use PyTorch? It's advantages:\n",
    "1.\tEasy to use GPU computational power without going deep into hardware and memory.\n",
    "2.\tðŸš€ __Automatic differentiation (AD)__: \n",
    "    - Compiled computational graph.\n",
    "    -  We use AD, which is a modular way of computing automatic differentiation.\n",
    "    -  We do it either by compiling computational graph or we use dynamical computational graph.\n",
    "    -  PyTorch uses dynamical computational graph, which is faster and dynamically changing the graph instead of using single graph for everything.\n",
    "4.\tPyTorch Ecosystem: ```Tochvision, torchtext, torchaudio```. Parallel computing, model deployment.\n",
    "\n",
    "## 2. Tensor Basics\n",
    "- Tensors can run on GPUâ€™s unlike numPy array.\n",
    "\n",
    "## Tensor Initialization\n",
    "Before we write the code in PyTorch, lets look how can we initialise the tensors. Here I show four different ways to initilise tensor. Tensor can have any dimension i.e, 1D, 2D, nD. We can initilise the tensor in cpu or gpu, we can initlise based on data type, for example `float`, `long`\n",
    "- From Numpy: convert using `torch.Tensor` \n",
    "- Using `torch.Tensor()`. We can create a list or list of list or dictionary of list.\n",
    "- Using random\n",
    "- Using `zeros_like` and `ones_like`\n",
    "\n",
    "- Tensors are a core _PyTorch_ data type similar to a multidimensional array (like in _Numpy_). For example the image shown below can be represented in RGB channel with width and height. The tensore representaion of this image can be written as $[3, 224, 224]$.\n",
    "- Use for representing data in numerical way\n",
    "- Tensors can run on GPUs unlike Numpy's array\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"D:/Data/Wax/pygimli_data_abhishek/pytorch/images/01_tensor_representation.PNG\" alt=\"Tensor Representation\" width=\"600\">\n",
    "</div>\n",
    "\n",
    "**Help in Pytorch**\n",
    "Always good to check the arguments in a function\n",
    "\n",
    "`help(torch.randint)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04833f0-4c4e-468b-bae1-f5494fb070d9",
   "metadata": {},
   "source": [
    "## Empty tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65818172-a310-4135-9554-2419619ef116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import torch\n",
    "\n",
    "# Create an empty tensor of size 1, so it is a scalar. Do not initialise the value.\n",
    "x= torch.empty(1)\n",
    "print(x)\n",
    "\n",
    "#  Create an empty tensor of size 3, so it is a 1D vector.\n",
    "x = torch.empty(3)\n",
    "print(x)\n",
    "\n",
    "#  Create an empty tensor of size (2,3), so it is a 2D matrix.\n",
    "x = torch.empty(2, 3)\n",
    "print(x)\n",
    "\n",
    "# Similarly, Create an empty 3D Tensor\n",
    "x= torch.empty(2,3,3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551b6445-e77b-4efa-bb53-401fef00152d",
   "metadata": {},
   "source": [
    "## Random tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4941b4fb-8ded-4390-9bd1-77949e96aab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random Tensor\n",
    "x= torch.rand(2,2)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b30d5e-ddf5-4283-8b15-5c468a0bc9f5",
   "metadata": {},
   "source": [
    "## Check the shape, size, and length of a tensor \n",
    "Note: .shape and .size() are equivalent in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08b13a2-df24-4473-83d1-795d3a6a0bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random tensor\n",
    "tensor = torch.randn(3, 4, 5)  # A 3D tensor with shape (3, 4, 5)\n",
    "\n",
    "# Check shape\n",
    "print(\"Shape of tensor:\", tensor.shape)\n",
    "\n",
    "# Check the Size of a Tensor\n",
    "print(\"Size of tensor:\", tensor.size())\n",
    "\n",
    "# Check the Length of a Tensor\n",
    "print(\"Length of tensor:\", len(tensor))\n",
    "\n",
    "#  Check the Number of Elements in the Tensor\n",
    "print(\"Total elements in tensor:\", tensor.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe1f7b2-b69e-4361-b9cd-fe06ba2bfe4e",
   "metadata": {},
   "source": [
    "## Zero and One tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0c4e32-0577-453d-b14f-36f23c18ed92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a zero Tensor\n",
    "x= torch.zeros(2,2)\n",
    "print(x)\n",
    "\n",
    "# Create a one Tensor\n",
    "x= torch.ones(2,2)\n",
    "print(x)\n",
    "\n",
    "\n",
    "x = torch.zeros(2,2)\n",
    "print(x)\n",
    "\n",
    "x = torch.ones(2,2)\n",
    "print(x)\n",
    "\n",
    "# Multidimensional tensor. Lets define 3 dimentional tensor. \n",
    "y= torch.zeros((2,3,4))\n",
    "print(y)         # check the paranthesis\n",
    "\n",
    "# access the elements\n",
    "y[0]        # extract element from 1st axis\n",
    "\n",
    "z = torch.ones((2,3,4))\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dd6d7c-3bbe-4fa2-bd68-f322c7bdf974",
   "metadata": {},
   "source": [
    "## Generate numbers or tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea49cf02-217d-497a-bc71-65c0198e9ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x= torch.arange(12)\n",
    "print(x)\n",
    "\n",
    "# Check the type of tensor\n",
    "print(type(x))\n",
    "\n",
    "# Check the dimension of tensor\n",
    "print(x.shape)         # we dont use () because its a property not a function\n",
    "\n",
    "# Check the number off elements in \n",
    "x.numel()        # total number of elements in one axis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48365102-beb4-4bac-b6d7-61dd982de64d",
   "metadata": {},
   "source": [
    "## Check tensor data type and modify it e.g., `float, Int, float32, double`\n",
    "By defauult its a float32 data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3992e2e2-1e66-4279-88a1-5fce3101bb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data type\n",
    "x = torch.ones(2,2)\n",
    "print(x.dtype)\n",
    "# torch.float32  # this is default\n",
    "\n",
    "# Assign data type to tensor\n",
    "x = torch.ones(2,2, dtype=torch.int)\n",
    "print(x.dtype)\n",
    "\n",
    "x = torch.ones(2,2, dtype=torch.double)\n",
    "print(x.dtype)\n",
    "\n",
    "x = torch.ones(2,2, dtype=torch.float16)\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5625936-02d2-407d-b42f-66b9177f9868",
   "metadata": {},
   "source": [
    "## Generate random numbers with noraml distribution, which foloow standard Gaussian Distribution with mean 0 and STD 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b98a9f4-9c2e-40af-bb98-6a9847500dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn(3,4) # or\n",
    "b = torch.randn(size=(128,128))\n",
    "print(b)\n",
    "\n",
    "# Check size, max elemt, min element of b\n",
    "b.size(); print('size of b is:', b)\n",
    "b.max(); print('Maximum value in Tensor b is:', b)\n",
    "b.min(); print('Minimum value in Tensor b is:', b)\n",
    "\n",
    "# define datatyep while generating random number\n",
    "c  = torch.randn(size=(128,128), dtype=torch.float64)\n",
    "c  = torch.randn(size=(128,128), dtype=torch.intt64) #  Why giving error: because randn and int cannot go together to follow the ditribution\n",
    "\n",
    "d =  torch.randint(high=100, size=(128,128), dtype=toch.int64) # high is the maximum value of number in the tensor\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83636d3-df56-42f0-b794-1d3372427b92",
   "metadata": {},
   "source": [
    "## Creating Tensor from Python List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de88bb9-43a9-491c-823e-8541ced497d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensor from data, eg. from list\n",
    "x = torch.tensor([2.5, 0.1])\n",
    "print(x)\n",
    "\n",
    "x = torch.Tensor([0.1, 0.2, 0.1, 0.4, 155])\n",
    "print(x)\n",
    "\n",
    "x = torch.tensor([[2,1,4,3],[1,2,3,4],[4,3,2,1]])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c43b006-ccdf-4954-8750-8fe34370c49e",
   "metadata": {},
   "source": [
    "## Tensor operations / Tensor manipulation\n",
    "Once we initilise the tensor we can perform differnt type of manipulations on tensors. It can be in three different ways,\n",
    "\n",
    "- __1. Element-wise operations:__ if we have two tensor $a$ and $b$ we can multiply them `a*b`\n",
    "- __2. Functional operations:__ we can use pytorch function like `torch.matmul, torch.nn.functional.relu`\n",
    "- __3. Modular operations:__ this is a object oriented way of performing same functional operations. For example, if we want to pass an image as a convolutional filter then we can create a module for conv filter. That module can be used to perform filtering, whcih comes an output of that module.\n",
    "\n",
    "### Why need to worry about tensor manipulation:\n",
    "The reason to consider a particular tensor manipulation is important because of __computational graph.__\n",
    "\n",
    "Often when we talk about deep learning or neural networks, we need to optimize for the __weights__ and __biases__ using __forward__ and __backward propagation__. When we perform forward and backward propagation, we need to get __gradient__. To obtain the gradients we need to store a record of different tensors and different operations that we have performed using __Directed Acyclic Graph (DAG)__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bf62e4-f6d5-4146-9992-e763c83a3174",
   "metadata": {},
   "source": [
    "## Element wise operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18cc4c8-61c3-4183-8904-931f856ab740",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2,2)\n",
    "y = torch.rand(2,2)\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "# element wise addition\n",
    "z = x + y\n",
    "z = torch.add(x,y)\n",
    "print(z)\n",
    "\n",
    "# element wise substracion\n",
    "z = x - y \n",
    "z = torch.sub(x,y)\n",
    "print(z)\n",
    "\n",
    "# element wise multiplication\n",
    "z = x * y \n",
    "z = torch.mul(x,y)\n",
    "print(z)\n",
    "\n",
    "# element wise division\n",
    "z = x / y \n",
    "z = torch.div(x,y)\n",
    "print(z)\n",
    "\n",
    "x = torch.tensor([1.0, 2, 4, 8])\n",
    "y = torch.tensor([2, 2, 2, 2])\n",
    "z = torch.matmul(x,y)\n",
    "torch.exp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a66a83-9884-4015-b20f-076f688e5f55",
   "metadata": {},
   "source": [
    "## Matrix Operations:  Determinant & Inverse\n",
    "\n",
    "### (1) Matrix Multiplication\n",
    "In PyTorch, you can perform matrix multiplication using `torch.mm()`, `torch.matmul()`, and the `@` operator.\n",
    "\n",
    "\n",
    "## <span style=\"color: yellow;\">We will cover Matrix Algebra in separate notbook</span>\n",
    "\n",
    "### (2) Matrix Operations in PyTorch\n",
    "\n",
    "| **Operation** | **PyTorch Function** |\n",
    "|--------------|----------------------|\n",
    "| **Determinant** | `torch.det(A)` |\n",
    "| **Inverse** | `torch.inverse(A)` |\n",
    "| **Transpose** | `A.T` or `torch.transpose(A, 0, 1)` |\n",
    "| **Trace (Sum of Diagonal Elements)** | `torch.trace(A)` |\n",
    "| **Eigenvalues & Eigenvectors** | `torch.linalg.eig(A)` |\n",
    "| **Singular Value Decomposition (SVD)** | `torch.svd(A)` |\n",
    "| **Rank of a Matrix** | `torch.linalg.matrix_rank(A)` |\n",
    "| **Solving Linear System (`Ax = b`)** | `torch.linalg.solve(A, b)` |\n",
    "\n",
    "### (3) Common Matrix Operations for Scientific Research in PyTorch\n",
    "\n",
    "| **Operation** | **PyTorch Function** | **Use Case** |\n",
    "|--------------|----------------------|-------------|\n",
    "| **Determinant** | `torch.det(A)` | Matrix Invertibility |\n",
    "| **Inverse** | `torch.inverse(A)` | Solving Equations |\n",
    "| **Norms** | `torch.norm(A, p)` | Regularization, Optimization |\n",
    "| **Eigenvalues & Eigenvectors** | `torch.linalg.eig(A)` | PCA, Stability Analysis |\n",
    "| **Singular Value Decomposition (SVD)** | `torch.svd(A)` | Dimensionality Reduction |\n",
    "| **Rank** | `torch.linalg.matrix_rank(A)` | Feature Selection, Linear Dependence |\n",
    "| **Condition Number** | `torch.linalg.cond(A)` | Numerical Stability |\n",
    "| **Pseudo Inverse** | `torch.linalg.pinv(A)` | Least Squares Solutions |\n",
    "| **Solving Linear Equations** | `torch.linalg.solve(A, b)` | Scientific Simulations |\n",
    "| **Cholesky Decomposition** | `torch.linalg.cholesky(A)` | Monte Carlo Simulations |\n",
    "| **QR Decomposition** | `torch.linalg.qr(A)` | Orthogonalization |\n",
    "\n",
    "**These matrix operations are essential for research in AI, Physics, and Engineering!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37237582-98ae-4c16-bfe3-23b96ed64e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix Multiplication\n",
    "\n",
    "# Define two matrices\n",
    "A = torch.tensor([[1, 2], [3, 4]])\n",
    "B = torch.tensor([[5, 6], [7, 8]])\n",
    "\n",
    "# Using torch.mm() (Only for 2D Matrices)\n",
    "result = torch.mm(A, B)\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Using torch.matmul() (Generalized)\n",
    "result = torch.matmul(A, B)\n",
    "print(result)\n",
    "\n",
    "# Using @ Operator (Shorthand for matmul)\n",
    "result = A @ B\n",
    "print(result)\n",
    "\n",
    "# Matrix Multiplication for Higher Dimensions\n",
    "A = torch.randn(2, 3, 4)  # Batch of 2 matrices of size (3x4)\n",
    "B = torch.randn(2, 4, 5)  # Batch of 2 matrices of size (4x5)\n",
    "\n",
    "result = torch.matmul(A, B)  # Resulting shape will be (2, 3, 5)\n",
    "print(result.shape)  # Output: torch.Size([2, 3, 5])\n",
    "\n",
    "# Element-wise Multiplication (* Operator). NOTE: Make sure A and B have the same shape! Otherwise, PyTorch will try to broadcast them\n",
    "result = A * B  # Element-wise multiplication (Hadamard Product)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b39685-080b-4a67-b3f8-0f8aa4387c6f",
   "metadata": {},
   "source": [
    "## Inplace operations: \n",
    "This will modify $y$ by adding all the elements of $x$ to $y$ \\\n",
    "In _PyTorch_ everyfunction which has trailing underscore (_) do a inplace operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b71d0c-845e-4a29-beda-141dc9e65d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.add_(x) \n",
    "print(y)\n",
    "\n",
    "y.sub_(x) \n",
    "print(y)\n",
    "\n",
    "y.mul_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e722c09b-1c5f-4ad2-95d1-a07d7d777ded",
   "metadata": {},
   "source": [
    "## Functional operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36275793-3f93-49ea-856a-db6704f5919d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiply two vectors and check the size\n",
    "tensor1 = torch.randn(3)\n",
    "tensor2 = torch.randn(3)\n",
    "torch.matmul(tensor1, tensor2).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a3e92e-7c1e-4309-8b0b-ac57e3eeddc6",
   "metadata": {},
   "source": [
    "## Concatenate tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5637a89b-c206-4bfb-a8e7-a98ec728face",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(12, dtype=torch.flat32).reshape((3,4))\n",
    "y = torch.tensor([[2.0, 1, 4, 3],[1, 2, 3, 4],[4, 3, 2, 1]])\n",
    "x,y\n",
    "X.shape, Y.shape\n",
    "\n",
    "torch.cat((X,Y),dim=0) # Concatenate in y direction or column wise\n",
    "torch.cat((X,Y),dim=1) # Concatenate in x direction or column wise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d51f45b-ed6c-446e-820a-1e756ccdf4e8",
   "metadata": {},
   "source": [
    "## Other operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce54699b-13d1-406f-85c4-208e0b538a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.sum()         # Summing all the elements in the tensor yields a tensor with only one element\n",
    "X.sum().shape        # we will not see any number, this mean\n",
    "X.sum(dim=0)\n",
    "X.sum(dim=0, keepdim=True)\n",
    "X.sum(dim=0).shape\n",
    "X.shum(dim=0, keepdim=True).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d7aa49-e8c6-40ce-a543-ad7588ad1f83",
   "metadata": {},
   "source": [
    "## Perform element-wise operations by invooking the braodcasting mechanism\n",
    "we cannot do element wise operation if the size of tensors are not the same. But we can do this using __broadcasting._\n",
    "\n",
    "In braodcasting we make the replica of row or column to make the two tensors of same shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a780d5-3411-4f8a-9a42-9db5d5b21f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(3).reshape((3,1))\n",
    "b = torch.arange(2).reshape((1,2))\n",
    "a,b\n",
    "\n",
    "\n",
    "a+b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa546b8-431a-4ae4-831f-bc19618baa35",
   "metadata": {},
   "source": [
    "## Indexing and Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410c0651-892a-451c-a22d-acef13cbb565",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.arange(12, dtype=torch.flat32).reshape((3,4))\n",
    "X[0]         # the tensor has two axis. If we just index one number , we are indexing based on 0-axis\n",
    "X[-1]        # index for last set of element\n",
    "# Assignment\n",
    "X[1:3] =12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0514f70-8047-47d6-9370-809863ef0123",
   "metadata": {},
   "source": [
    "## Slicing in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14a4b3b-bb8c-4866-9afb-6889c8239202",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(5,3)\n",
    "print(x)\n",
    "\n",
    "# all the rows but first column only\n",
    "print(x[:, 0]) \n",
    "\n",
    "# 1st row all the column\n",
    "print(x[0, :]) \n",
    "\n",
    "# only ine element\n",
    "print(x[1, 1]) \n",
    "# if tensor has only one element we can also call.item method, whcih give the actual value. Only used if on value in tensor\n",
    "print(x[1, 1].item()) # only one element"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfc290e-07fb-4c38-a0f3-e78e2d6de52e",
   "metadata": {},
   "source": [
    "## Reshaping Tensors\n",
    "NOTE: number of elements must be same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de73deb-9c42-4a7d-b7cc-d1aef0c8f063",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(4,4)\n",
    "y = x.view(16)\n",
    "print(y)\n",
    "# if we don't want to put number of element for diemenison we can also use\n",
    "y = x.view(-1,8)\n",
    "print(y)\n",
    "print(y.size())\n",
    "\n",
    "\n",
    "X = x.reshape(3,4) # number of elements must be same\n",
    "# We can see after reshaping that we have extra [], because of an extra axis\n",
    "print(X.shape)         # first axis has 3 elements and 2nd axis has 4 elements. 0th axis is the column (y direction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49573f24-670d-43c1-988a-5e1487441096",
   "metadata": {},
   "source": [
    "## Convert numpy array to torch tensor and viceversa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16e98e5-2400-44e1-8d80-f192cf0c7eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = torch.ones(5)\n",
    "print(a)\n",
    "\n",
    "b = a.numpy()\n",
    "print(b)\n",
    "print(type(b))\n",
    "\n",
    "#######################################################\n",
    "A =  X.numpy()                # tensor to numpy\n",
    "B = torch.from_numpy(A)       # tensor from numpy\n",
    "type(A), type(B)\n",
    "\n",
    "c = np.arange(100).reshape(25,4)\n",
    "d =  torch.from_numpy(c)\n",
    "print(d.type)\n",
    "print(d.dtype)\n",
    "\n",
    "# convert to long datatype i.e., 64 bit integer\n",
    "e = torch.LongTensor(d)\n",
    "e.dtype\n",
    "\n",
    "# convert to float tensor\n",
    "f = torch.FloatTensor(d+0.1) # adding 0.1 to having the float.\n",
    "f.dtype\n",
    "\n",
    "# convert to double tensor i.e., \n",
    "g = torch.DoubletTensor(d.astype()Torch.Float64)+0.1) #  Why this error \n",
    "g.dtype\n",
    "\n",
    "g = d.type(torch.float64)\n",
    "g.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7bbbd0-a497-431b-b3ce-df7979e6481c",
   "metadata": {},
   "source": [
    "## <span style=\"color: red;\">Warning</span>\n",
    "We have to be careful, because if the tensor is on CPU not on GPU than both objects share the same memory location. If we change one we have to change other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e824150f-ac54-4323-b7e7-b4d9f5c1f99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(5)\n",
    "b = a.numpy()\n",
    "\n",
    "a.add_(1)\n",
    "print(a)\n",
    "print(b) # we can see it will also add 1 to b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5b7b43-4241-417c-b401-d71c1aa60a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy to torch: If we have numpy array in the begineeing\n",
    "a = np.ones(5)\n",
    "print(a)\n",
    "b = torch.from_numpy(a)\n",
    "print(b)\n",
    "\n",
    "a +=1\n",
    "print(a) \n",
    "print(b) # tensor is also modified automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee5fc33-6a62-490c-b4b7-5efa01d511cc",
   "metadata": {},
   "source": [
    "## Memory Usage in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fdb4be-1150-4011-a5d2-cbdf0d3fde11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the results of an operation to a previously allocated array with slice notation\n",
    "print('id(X):', id(X))        # id function can be used, whcih give a pointer and memory where tensor is stored\n",
    "X[:] = X + Y\n",
    "print('id(X):', id(X))\n",
    "\n",
    "print('id(X):', id(X))\n",
    "X += + Y\n",
    "print('id(X):', id(X))\n",
    "\n",
    "X[:] = X + Y\n",
    "X = X + Y\n",
    "print('id(X):', id(X)) # now we will have different memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfcc606-5747-4a09-80e1-3e2b2b256f79",
   "metadata": {},
   "source": [
    "## GPU usage in PyTorch\n",
    "Always carefule in transfering data in cpu and GPU. Is it necessary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611e6d3c-57bf-4efd-9257-b29624ef2c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = toech.device(\"cuda\")\n",
    "    x = torch.ones(5, device=device) # create a tensor and put it in GPU\n",
    "    y = torch.ones(5)\n",
    "    # move to device GPU\n",
    "    y = y.to(device)\n",
    "    # now do operation, whcih will be perform in GPU\n",
    "    z = x + y\n",
    "    # z.numpy() # this will retrun an error because numpy can only handle CPU tensors. So you cannot convert GPU bacjk to numpy\n",
    "    # We have to move back to CPU\n",
    "    z = z.to(\"cpu\")\n",
    "    z.numpy() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cf758d-f0fd-4122-8527-f0a9036efe1a",
   "metadata": {},
   "source": [
    "##  Gradient Calculation with Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efe54fd-a4ae-446e-aae1-b913c663485a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when we need to calculate the gradient, later for the optimization step.\n",
    "x = torch.ones(5, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7598d390-98da-482b-a21e-a11e364d89f0",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ **Thank You!** ðŸ™Œ  \n",
    "### ðŸš€ Happy Coding & Keep Learning! ðŸ’¡\n",
    "\n",
    "## <span style=\"color: yellow;\">We will see the Gradient concept in detial in next notebook</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afdb8d6-f160-4e90-ac3c-2da37b2dd6fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
